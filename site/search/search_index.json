{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Metamorphosis: AI-Powered Self-Review Processing System","text":""},{"location":"#overview","title":"Overview","text":"<p>Metamorphosis is a comprehensive AI-powered text processing system designed for employee self-review workflows. It leverages LangGraph orchestration, OpenAI's language models, and Model Context Protocol (MCP) integration to provide intelligent text processing capabilities including copy editing, summarization, achievement extraction, and quality evaluation.</p>"},{"location":"#project-architecture","title":"Project Architecture","text":"graph TB     subgraph \"User Interfaces\"         A[Streamlit UI]         B[FastAPI REST API]     end      subgraph \"Orchestration Layer\"         C[LangGraph Agent Workflows]         D[Self-Reviewer Agent]     end      subgraph \"Processing Layer\"         E[MCP Tools Server]         F[TextModifiers]     end      subgraph \"LLM Services\"         G[OpenAI GPT Models]         H[Model Registry]     end      subgraph \"Data Models\"         I[Pydantic Schemas]         J[Structured Outputs]     end      A --&gt; B     B --&gt; C     C --&gt; D     D --&gt; E     E --&gt; F     F --&gt; H     H --&gt; G     F --&gt; I     I --&gt; J"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#multi-stage-text-processing-pipeline","title":"\ud83d\udd04 Multi-Stage Text Processing Pipeline","text":"<ul> <li>Copy Editing: Grammar, spelling, and style improvements</li> <li>Summarization: Abstractive summaries with configurable length</li> <li>Achievement Extraction: Structured extraction of key accomplishments</li> <li>Quality Evaluation: Comprehensive writing quality assessment</li> <li>Visualization: Word cloud and radar chart generation</li> </ul>"},{"location":"#ai-agent-orchestration","title":"\ud83e\udd16 AI Agent Orchestration","text":"<ul> <li>LangGraph Workflows: State-based multi-agent processing</li> <li>Parallel Processing: Concurrent execution of independent tasks</li> <li>Real-time Streaming: Live updates via Server-Sent Events</li> <li>State Persistence: Thread-based conversation management</li> </ul>"},{"location":"#mcp-integration","title":"\ud83d\udee0\ufe0f MCP Integration","text":"<ul> <li>Standardized Tools: MCP-compliant text processing functions</li> <li>Type Safety: Pydantic models for structured outputs</li> <li>Error Handling: Comprehensive exception management</li> <li>Caching: LRU-cached model instances for performance</li> </ul>"},{"location":"#package-structure","title":"Package Structure","text":"<p>The project is organized into several specialized packages:</p>"},{"location":"#core-packages","title":"\ud83d\udce6 Core Packages","text":"Package Description Documentation <code>metamorphosis</code> Main package with configuration and utilities View \u2192 <code>metamorphosis.mcp</code> MCP tools and text processing utilities View \u2192 <code>metamorphosis.agents</code> LangGraph workflows and FastAPI service View \u2192 <code>metamorphosis.ui</code> Streamlit user interface View \u2192"},{"location":"#examples","title":"\ud83d\udcc1 Examples","text":"Script Purpose Documentation <code>summarizer_usage.py</code> Demonstrates text summarization View \u2192 <code>rationalize_usage.py</code> Shows copy editing capabilities View \u2192 <code>extract_achievements_usage.py</code> Achievement extraction demo View \u2192 <code>review_text_evaluator_usage.py</code> Quality evaluation example View \u2192 <code>visualize_evaluation_radar.py</code> Radar chart visualization View \u2192"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+</li> <li>OpenAI API key</li> <li>Required dependencies (see <code>pyproject.toml</code>)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li> <p>Environment Setup:    <pre><code># Clone the repository\ngit clone &lt;repository-url&gt;\ncd metamorphosis\n\n# Install dependencies\nuv sync\n\n# Set up environment variables\ncp .env.example .env\n# Edit .env with your OpenAI API key\n</code></pre></p> </li> <li> <p>Basic Usage:    <pre><code>from metamorphosis.mcp.text_modifiers import TextModifiers\n\n# Initialize the text processor\nmodifier = TextModifiers()\n\n# Process text\nresult = modifier.summarize(\n    text=\"Your review text here...\",\n    max_words=100\n)\nprint(result.summarized_text)\n</code></pre></p> </li> <li> <p>Run the Full Application:    <pre><code># Start the FastAPI service\npython -m metamorphosis.agents.agent_service\n\n# Launch the Streamlit UI (in another terminal)\nstreamlit run src/metamorphosis/ui/streamlit_ui.py\n</code></pre></p> </li> </ol>"},{"location":"#configuration","title":"Configuration","text":"<p>The system uses a hierarchical configuration approach:</p>"},{"location":"#environment-variables","title":"Environment Variables","text":"<pre><code># Required\nOPENAI_API_KEY=sk-...\n\n# Optional\nPROJECT_ROOT_DIR=/path/to/project\nMCP_SERVER_HOST=localhost\nMCP_SERVER_PORT=8000\nFASTAPI_HOST=0.0.0.0\nFASTAPI_PORT=8001\n</code></pre>"},{"location":"#configuration-file-configyaml","title":"Configuration File (<code>config.yaml</code>)","text":"<pre><code>models:\n  summarizer:\n    model: gpt-4o\n    temperature: 0.0\n    max_tokens: 2000\n    timeout: 120\n\n  copy_editor:\n    model: gpt-4o\n    temperature: 0.0\n    max_tokens: 4000\n    timeout: 180\n</code></pre>"},{"location":"#key-data-models","title":"Key Data Models","text":""},{"location":"#processing-results","title":"Processing Results","text":"<pre><code>from metamorphosis.datamodel import (\n    SummarizedText,\n    CopyEditedText,\n    AchievementsList,\n    ReviewScorecard\n)\n\n# Each model provides structured, validated outputs\n# with comprehensive metadata and error handling\n</code></pre>"},{"location":"#workflow-state","title":"Workflow State","text":"<pre><code>from metamorphosis.agents.self_reviewer import GraphState\n\n# LangGraph state management for multi-step processing\n# with type safety and serialization support\n</code></pre>"},{"location":"#development","title":"Development","text":""},{"location":"#code-quality-standards","title":"Code Quality Standards","text":"<p>The project follows strict coding standards:</p> <ul> <li>Complexity: Cyclomatic complexity \u2264 10</li> <li>Type Safety: Full type annotations with Python 3.12+ syntax</li> <li>Documentation: Google-style docstrings for all public APIs</li> <li>Formatting: Ruff with 100-character line limit</li> <li>Testing: Comprehensive test coverage with pytest</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<ol> <li>Follow the established code style and patterns</li> <li>Add comprehensive documentation for new features</li> <li>Include type annotations and docstrings</li> <li>Write tests for new functionality</li> <li>Update this documentation as needed</li> </ol>"},{"location":"#troubleshooting","title":"Troubleshooting","text":""},{"location":"#common-issues","title":"Common Issues","text":"<ul> <li>OpenAI API Errors: Check your API key and rate limits</li> <li>Import Errors: Ensure <code>PYTHONPATH</code> includes the <code>src</code> directory</li> <li>Configuration Issues: Verify <code>config.yaml</code> format and environment variables</li> <li>Performance: Monitor token usage and consider caching strategies</li> </ul>"},{"location":"#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre>"},{"location":"#license","title":"License","text":"<p>[Add license information here]</p>"},{"location":"#support","title":"Support","text":"<p>For issues and questions: - Check the documentation - Review the examples directory - File issues on the project repository</p> <p>This documentation is automatically generated and maintained in sync with the codebase.</p>"},{"location":"architecture/","title":"System Architecture","text":""},{"location":"architecture/#overview","title":"Overview","text":"<p>The Metamorphosis system implements a sophisticated AI-powered text processing architecture using modern Python frameworks and design patterns. This document provides a comprehensive view of the system's architecture, component relationships, and data flow.</p>"},{"location":"architecture/#high-level-architecture","title":"High-Level Architecture","text":"graph TB     subgraph \"Presentation Layer\"         UI[Streamlit UI]         API[FastAPI REST API]         CLI[Command Line Tools]     end      subgraph \"Orchestration Layer\"         LG[LangGraph Workflows]         AG[Agent Coordinator]         SM[State Manager]     end      subgraph \"Processing Layer\"         MCP[MCP Tools Server]         TM[TextModifiers Core]         WF[Workflow Nodes]     end      subgraph \"Model Layer\"         MR[Model Registry]         LLM1[Summarizer LLM]         LLM2[Copy Editor LLM]         LLM3[Achievements LLM]         LLM4[Evaluator LLM]     end      subgraph \"Data Layer\"         DM[Pydantic Models]         ST[State Storage]         FS[File System]     end      subgraph \"External Services\"         OPENAI[OpenAI API]         WC[Word Cloud Service]     end      UI --&gt; API     CLI --&gt; MCP     API --&gt; LG     LG --&gt; AG     AG --&gt; SM     SM --&gt; WF     WF --&gt; TM     TM --&gt; MCP     MCP --&gt; MR     MR --&gt; LLM1     MR --&gt; LLM2     MR --&gt; LLM3     MR --&gt; LLM4     LLM1 --&gt; OPENAI     LLM2 --&gt; OPENAI     LLM3 --&gt; OPENAI     LLM4 --&gt; OPENAI     WF --&gt; WC     TM --&gt; DM     SM --&gt; ST     ST --&gt; FS"},{"location":"architecture/#component-architecture","title":"Component Architecture","text":""},{"location":"architecture/#core-classes-and-relationships","title":"Core Classes and Relationships","text":"classDiagram     class TextModifiers {         -summarizer_llm: ChatOpenAI         -copy_editor_llm: ChatOpenAI         -key_achievements_llm: ChatOpenAI         -review_text_evaluator_llm: ChatOpenAI         +summarize(text, max_words) SummarizedText         +rationalize_text(text) CopyEditedText         +extract_achievements(text) AchievementsList         +evaluate_review_text(text) ReviewScorecard         +get_model_info(method) dict         -_log_model_details_table(method)     }      class ModelRegistry {         -_instance: ModelRegistry         -_initialized: bool         +summarizer_llm: ChatOpenAI         +copy_editor_llm: ChatOpenAI         +key_achievements_llm: ChatOpenAI         +review_text_evaluator_llm: ChatOpenAI         +__new__() ModelRegistry         -_build_chat_openai(config, api_key) ChatOpenAI     }      class GraphState {         +review_text: str         +copy_edited_text: str | None         +summary: str | None         +word_cloud_path: str | None     }      class FastAPIApp {         +invoke(request) InvokeResponse         +stream(request) StreamingResponse         +health() dict     }      class StreamlitUI {         +patch_state(dst, delta) dict         +stream_from_server(url, data) Iterator         +extract_values_from_event(event) dict     }      class SummarizedText {         +summarized_text: str         +size: int         +unit: str     }      class CopyEditedText {         +copy_edited_text: str         +size: int         +is_edited: bool     }      class AchievementsList {         +items: List[Achievement]         +size: int         +unit: str     }      class ReviewScorecard {         +metrics: List[MetricScore]         +overall: int         +verdict: Verdict         +notes: List[str]         +radar_labels: List[str]         +radar_values: List[int]     }      class Achievement {         +title: str         +outcome: str         +impact_area: ImpactArea         +metric_strings: List[str]         +timeframe: Optional[str]         +ownership_scope: Optional[OwnershipScope]         +collaborators: List[str]     }      class MetricScore {         +name: str         +score: int         +rationale: str         +suggestion: str     }      TextModifiers --&gt; ModelRegistry : uses     TextModifiers --&gt; SummarizedText : creates     TextModifiers --&gt; CopyEditedText : creates     TextModifiers --&gt; AchievementsList : creates     TextModifiers --&gt; ReviewScorecard : creates     AchievementsList --&gt; Achievement : contains     ReviewScorecard --&gt; MetricScore : contains     FastAPIApp --&gt; GraphState : manages     StreamlitUI --&gt; FastAPIApp : communicates     GraphState --&gt; TextModifiers : processes_with"},{"location":"architecture/#data-flow-architecture","title":"Data Flow Architecture","text":""},{"location":"architecture/#processing-pipeline","title":"Processing Pipeline","text":"sequenceDiagram     participant User     participant UI as Streamlit UI     participant API as FastAPI Service     participant LG as LangGraph     participant TM as TextModifiers     participant LLM as OpenAI API     participant FS as File System      User-&gt;&gt;UI: Enter review text     UI-&gt;&gt;API: POST /stream {text, thread_id}     API-&gt;&gt;API: Generate thread_id     API-&gt;&gt;LG: run_graph(text, thread_id)      LG-&gt;&gt;TM: rationalize_text(text)     TM-&gt;&gt;LLM: Copy editing request     LLM-&gt;&gt;TM: Edited text     TM-&gt;&gt;LG: CopyEditedText     LG-&gt;&gt;API: Stream event {copy_edited_text}     API-&gt;&gt;UI: SSE event      par Parallel Processing         LG-&gt;&gt;TM: summarize(edited_text)         TM-&gt;&gt;LLM: Summarization request         LLM-&gt;&gt;TM: Summary         TM-&gt;&gt;LG: SummarizedText         LG-&gt;&gt;API: Stream event {summary}         API-&gt;&gt;UI: SSE event     and         LG-&gt;&gt;FS: create_word_cloud(edited_text)         FS-&gt;&gt;LG: Word cloud path         LG-&gt;&gt;API: Stream event {word_cloud_path}         API-&gt;&gt;UI: SSE event     end      UI-&gt;&gt;User: Display results"},{"location":"architecture/#state-management-flow","title":"State Management Flow","text":"stateDiagram-v2     [*] --&gt; Initialized     Initialized --&gt; Processing: User submits text     Processing --&gt; CopyEditing: Start workflow     CopyEditing --&gt; ParallelProcessing: Text edited      state ParallelProcessing {         [*] --&gt; Summarizing         [*] --&gt; WordCloudGen         Summarizing --&gt; SummaryComplete         WordCloudGen --&gt; WordCloudComplete         SummaryComplete --&gt; [*]         WordCloudComplete --&gt; [*]     }      ParallelProcessing --&gt; Complete: All tasks done     Complete --&gt; [*]: Results delivered      Processing --&gt; Error: Exception occurred     Error --&gt; [*]: Error handled"},{"location":"architecture/#design-patterns","title":"Design Patterns","text":""},{"location":"architecture/#singleton-pattern-model-registry","title":"Singleton Pattern (Model Registry)","text":"<pre><code>class ModelRegistry:\n    _instance = None\n    _initialized = False\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n\n    def __init__(self):\n        if not self._initialized:\n            # Initialize LLM clients\n            self._initialize_llms()\n            self._initialized = True\n</code></pre>"},{"location":"architecture/#factory-pattern-configuration-loading","title":"Factory Pattern (Configuration Loading)","text":"<pre><code>def get_model_registry() -&gt; ModelRegistry:\n    \"\"\"Factory function for ModelRegistry singleton.\"\"\"\n    return ModelRegistry()\n\ndef _load_config() -&gt; dict[str, Any]:\n    \"\"\"Factory for configuration loading with fallbacks.\"\"\"\n    # Configuration loading logic\n    return config_data\n</code></pre>"},{"location":"architecture/#strategy-pattern-text-processing","title":"Strategy Pattern (Text Processing)","text":"<pre><code>class TextModifiers:\n    def __init__(self):\n        # Different strategies for different processing types\n        self.strategies = {\n            'summarize': self._create_summarizer_chain(),\n            'copy_edit': self._create_copy_editor_chain(),\n            'extract_achievements': self._create_achievements_chain(),\n            'evaluate': self._create_evaluator_chain()\n        }\n</code></pre>"},{"location":"architecture/#observer-pattern-streaming-events","title":"Observer Pattern (Streaming Events)","text":"<pre><code>async def _generate_stream_events(review_text: str, thread_id: str):\n    \"\"\"Observer pattern for streaming workflow events.\"\"\"\n    async for event in run_graph(graph, review_text, thread_id):\n        # Notify observers (UI clients) of state changes\n        yield f\"data: {json.dumps(event)}\\n\\n\".encode(\"utf-8\")\n</code></pre>"},{"location":"architecture/#decorator-pattern-validation","title":"Decorator Pattern (Validation)","text":"<pre><code>@validate_call\ndef summarize(\n    self,\n    *,\n    text: Annotated[str, Field(min_length=1)],\n    max_words: Annotated[int, Field(gt=0)] = 300,\n) -&gt; SummarizedText:\n    # Method implementation with automatic validation\n</code></pre>"},{"location":"architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"architecture/#api-security","title":"API Security","text":"graph LR     subgraph \"Security Layers\"         A[CORS Middleware]         B[Input Validation]         C[Rate Limiting]         D[Error Sanitization]     end      subgraph \"Data Protection\"         E[Environment Variables]         F[Secret Management]         G[Input Sanitization]     end      Request --&gt; A     A --&gt; B     B --&gt; C     C --&gt; D     D --&gt; Processing      Processing --&gt; E     E --&gt; F     F --&gt; G"},{"location":"architecture/#configuration-security","title":"Configuration Security","text":"<ul> <li>API Key Management: Environment variable isolation</li> <li>Input Validation: Pydantic model validation</li> <li>Error Handling: Sanitized error responses</li> <li>CORS Configuration: Restricted origins in production</li> </ul>"},{"location":"architecture/#performance-architecture","title":"Performance Architecture","text":""},{"location":"architecture/#caching-strategy","title":"Caching Strategy","text":"graph TB     subgraph \"Caching Layers\"         A[LLM Client CacheSingleton Registry]         B[Prompt Template CacheFile System]         C[Processing Results CacheLRU Cache]     end      subgraph \"Performance Optimizations\"         D[Parallel ProcessingAsyncIO]         E[Connection PoolingHTTP Clients]         F[Resource ManagementMemory Cleanup]     end      Request --&gt; A     A --&gt; B     B --&gt; C     C --&gt; D     D --&gt; E     E --&gt; F     F --&gt; Response"},{"location":"architecture/#scalability-considerations","title":"Scalability Considerations","text":"<ul> <li>Horizontal Scaling: Stateless service design</li> <li>Load Balancing: Multiple service instances</li> <li>Resource Optimization: Efficient memory usage</li> <li>Connection Management: HTTP connection pooling</li> </ul>"},{"location":"architecture/#error-handling-architecture","title":"Error Handling Architecture","text":""},{"location":"architecture/#exception-hierarchy","title":"Exception Hierarchy","text":"classDiagram     class Exception {         +message: str         +args: tuple     }      class ReviewError {         +message: str         +context: dict         +operation: str         +error_code: str     }      class ValidationError {         +validation_context: dict     }      class PostconditionError {         +postcondition_context: dict     }      class ConfigurationError {         +config_context: dict     }      class MCPToolError {         +tool_context: dict     }      class FileOperationError {         +file_context: dict     }      Exception &lt;|-- ReviewError     ReviewError &lt;|-- ValidationError     ReviewError &lt;|-- PostconditionError     ReviewError &lt;|-- ConfigurationError     ReviewError &lt;|-- MCPToolError     ReviewError &lt;|-- FileOperationError"},{"location":"architecture/#error-flow","title":"Error Flow","text":"sequenceDiagram     participant Client     participant API     participant Service     participant LLM      Client-&gt;&gt;API: Request     API-&gt;&gt;Service: Process     Service-&gt;&gt;LLM: LLM Call     LLM-&gt;&gt;Service: Error Response     Service-&gt;&gt;Service: Create PostconditionError     Service-&gt;&gt;API: Structured Error     API-&gt;&gt;API: Log Error Context     API-&gt;&gt;Client: JSON Error Response"},{"location":"architecture/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"architecture/#logging-architecture","title":"Logging Architecture","text":"graph TB     subgraph \"Logging Sources\"         A[TextModifiers]         B[Model Registry]         C[FastAPI Service]         D[LangGraph Workflows]     end      subgraph \"Log Processing\"         E[Loguru Logger]         F[Structured Logging]         G[Log Aggregation]     end      subgraph \"Output Destinations\"         H[Console Output]         I[File Logs]         J[Debug Logs]     end      A --&gt; E     B --&gt; E     C --&gt; E     D --&gt; E     E --&gt; F     F --&gt; G     G --&gt; H     G --&gt; I     G --&gt; J"},{"location":"architecture/#metrics-collection","title":"Metrics Collection","text":"<ul> <li>Processing Metrics: Token usage, processing time</li> <li>Error Metrics: Error rates, failure patterns  </li> <li>Performance Metrics: Response times, throughput</li> <li>Resource Metrics: Memory usage, CPU utilization</li> </ul>"},{"location":"architecture/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"architecture/#container-architecture","title":"Container Architecture","text":"graph TB     subgraph \"Application Container\"         A[Python 3.12 Runtime]         B[FastAPI Service]         C[Streamlit UI]         D[MCP Tools]     end      subgraph \"Configuration\"         E[Environment Variables]         F[Config Files]         G[Prompt Templates]     end      subgraph \"External Dependencies\"         H[OpenAI API]         I[File System Storage]     end      A --&gt; B     A --&gt; C     A --&gt; D     E --&gt; A     F --&gt; A     G --&gt; A     B --&gt; H     D --&gt; I"},{"location":"architecture/#service-dependencies","title":"Service Dependencies","text":"<ul> <li>Runtime: Python 3.12+</li> <li>Core Dependencies: FastAPI, LangChain, Pydantic, Streamlit</li> <li>AI Services: OpenAI API</li> <li>Utilities: Rich, Plotly, Loguru</li> <li>Development: Ruff, Pytest, Radon</li> </ul>"},{"location":"architecture/#future-architecture-considerations","title":"Future Architecture Considerations","text":""},{"location":"architecture/#extensibility-points","title":"Extensibility Points","text":"<ol> <li>New Processing Types: Additional text processing capabilities</li> <li>Multiple LLM Providers: Support for different AI services</li> <li>Enhanced Workflows: More complex multi-agent workflows</li> <li>Real-time Processing: WebSocket-based real-time updates</li> <li>Batch Processing: Large-scale document processing</li> </ol>"},{"location":"architecture/#scalability-enhancements","title":"Scalability Enhancements","text":"<ol> <li>Microservices: Split into smaller, focused services</li> <li>Message Queues: Asynchronous processing with queues</li> <li>Caching Layer: Redis or similar for distributed caching</li> <li>Database Integration: Persistent storage for results</li> <li>Load Balancing: Multiple service instances</li> </ol> <p>This architecture documentation is maintained in sync with the system implementation and updated as the system evolves.</p>"},{"location":"examples/","title":"Examples Directory","text":"<p>The <code>examples/</code> directory contains comprehensive usage examples and demonstrations of the Metamorphosis text processing capabilities. Each example is a standalone script that showcases specific features with detailed explanations and rich output formatting.</p>"},{"location":"examples/#overview","title":"Overview","text":"graph TB     subgraph \"Text Processing Examples\"         A[summarizer_usage.pyBasic Summarization]         B[rationalize_usage.pyCopy Editing Demo]     end      subgraph \"Advanced Processing Examples\"         C[extract_achievements_usage.pyAchievement Extraction]         D[review_text_evaluator_usage.pyQuality Evaluation]     end      subgraph \"Visualization Examples\"         E[visualize_evaluation_radar.pyRadar Chart Generation]     end      subgraph \"Input Data\"         F[sample_reviews/Example Review Files]     end      A --&gt; F     B --&gt; F     C --&gt; F     D --&gt; F     D --&gt; E"},{"location":"examples/#example-scripts","title":"Example Scripts","text":""},{"location":"examples/#basic-text-processing","title":"Basic Text Processing","text":"Script Purpose Features Output <code>summarizer_usage.py</code> Text summarization demo Configurable word limits, rich formatting Console display + JSONL <code>rationalize_usage.py</code> Copy editing showcase Before/after comparison, change tracking Rich table output"},{"location":"examples/#advanced-processing","title":"Advanced Processing","text":"Script Purpose Features Output <code>extract_achievements_usage.py</code> Achievement extraction Structured metadata, ranking Rich table + JSONL <code>review_text_evaluator_usage.py</code> Quality assessment 6-dimension scoring, suggestions Rich table + JSONL"},{"location":"examples/#visualization","title":"Visualization","text":"Script Purpose Features Output <code>visualize_evaluation_radar.py</code> Radar chart creation Interactive plots, goal references HTML file + console"},{"location":"examples/#common-features","title":"Common Features","text":""},{"location":"examples/#rich-console-output","title":"Rich Console Output","text":"<p>All examples use the <code>rich</code> library for beautiful console formatting:</p> <ul> <li>Color-coded output: Different colors for different types of information</li> <li>Progress indicators: Visual feedback during processing</li> <li>Structured tables: Organized display of results</li> <li>Panel formatting: Highlighted sections and tips</li> </ul>"},{"location":"examples/#error-handling","title":"Error Handling","text":"<p>Comprehensive error handling with user-friendly messages:</p> <pre><code>try:\n    # Processing logic\n    result = modifier.process_text(text=review_text)\nexcept ValidationError as e:\n    console.print(f\"\u274c Validation Error: {e}\", style=\"bold red\")\nexcept PostconditionError as e:\n    console.print(f\"\u274c Processing Error: {e}\", style=\"bold red\")\nexcept Exception as e:\n    console.print(f\"\u274c Unexpected Error: {e}\", style=\"bold red\")\n</code></pre>"},{"location":"examples/#data-persistence","title":"Data Persistence","text":"<p>Examples demonstrate both console output and data persistence:</p> <ul> <li>JSONL files: Structured data for further processing</li> <li>HTML files: Interactive visualizations</li> <li>Logging: Detailed operation tracking</li> </ul>"},{"location":"examples/#usage-patterns","title":"Usage Patterns","text":""},{"location":"examples/#running-examples","title":"Running Examples","text":"<p>All examples can be run from the project root using Python's module syntax:</p> <pre><code># Basic usage\npython -m src.examples.summarizer_usage\npython -m src.examples.rationalize_usage\n\n# Advanced features\npython -m src.examples.extract_achievements_usage\npython -m src.examples.review_text_evaluator_usage\n\n# Visualization\npython -m src.examples.visualize_evaluation_radar\n</code></pre>"},{"location":"examples/#using-with-uv","title":"Using with uv","text":"<p>For consistent dependency management:</p> <pre><code>uv run python -m src.examples.summarizer_usage\nuv run python -m src.examples.extract_achievements_usage\n</code></pre>"},{"location":"examples/#custom-input","title":"Custom Input","text":"<p>Examples can be modified to use custom input:</p> <pre><code># Modify the review_text variable in any example\nreview_text = \"\"\"\nYour custom employee review text here...\n\"\"\"\n</code></pre>"},{"location":"examples/#integration-examples","title":"Integration Examples","text":""},{"location":"examples/#basic-integration","title":"Basic Integration","text":"<pre><code>from metamorphosis.mcp.text_modifiers import TextModifiers\nfrom rich.console import Console\n\ndef simple_processing_example():\n    console = Console()\n    modifier = TextModifiers()\n\n    # Process text\n    result = modifier.summarize(\n        text=\"Your review text...\",\n        max_words=100\n    )\n\n    # Display result\n    console.print(f\"Summary: {result.summarized_text}\")\n</code></pre>"},{"location":"examples/#batch-processing","title":"Batch Processing","text":"<pre><code>def batch_processing_example():\n    modifier = TextModifiers()\n    reviews = load_multiple_reviews()\n\n    results = []\n    for review in reviews:\n        try:\n            result = modifier.extract_achievements(text=review)\n            results.append(result)\n        except Exception as e:\n            print(f\"Error processing review: {e}\")\n\n    return results\n</code></pre>"},{"location":"examples/#pipeline-integration","title":"Pipeline Integration","text":"<pre><code>def full_pipeline_example():\n    modifier = TextModifiers()\n    review_text = load_review()\n\n    # Step 1: Copy edit\n    edited = modifier.rationalize_text(text=review_text)\n\n    # Step 2: Extract achievements\n    achievements = modifier.extract_achievements(text=edited.copy_edited_text)\n\n    # Step 3: Evaluate quality\n    evaluation = modifier.evaluate_review_text(text=edited.copy_edited_text)\n\n    # Step 4: Summarize\n    summary = modifier.summarize(text=edited.copy_edited_text, max_words=100)\n\n    return {\n        \"edited\": edited,\n        \"achievements\": achievements,\n        \"evaluation\": evaluation,\n        \"summary\": summary\n    }\n</code></pre>"},{"location":"examples/#sample-data","title":"Sample Data","text":""},{"location":"examples/#review-files","title":"Review Files","text":"<p>The <code>sample_reviews/</code> directory contains example employee review files:</p> <ul> <li><code>raw_review.md</code>: Original, unedited review text</li> <li><code>copy_edited.md</code>: Professionally edited version</li> <li><code>data_engineer_review.md</code>: Technical role-specific example</li> </ul>"},{"location":"examples/#generated-outputs","title":"Generated Outputs","text":"<p>Examples generate various output files:</p> <ul> <li><code>key_achievements.jsonl</code>: Extracted achievements data</li> <li><code>text_evaluator_results.jsonl</code>: Quality evaluation results</li> <li><code>evaluation_radar_plot.html</code>: Interactive visualization</li> </ul>"},{"location":"examples/#customization-guide","title":"Customization Guide","text":""},{"location":"examples/#adding-new-examples","title":"Adding New Examples","text":"<p>To create a new example script:</p> <ol> <li> <p>Create the script file:    <pre><code># src/examples/my_custom_example.py\nfrom metamorphosis.mcp.text_modifiers import TextModifiers\nfrom rich.console import Console\n\ndef main():\n    console = Console()\n    modifier = TextModifiers()\n\n    # Your custom logic here\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p> </li> <li> <p>Add file header:    <pre><code># =============================================================================\n#  Filename: my_custom_example.py\n#\n#  Short Description: Custom example demonstrating specific feature\n#\n#  Creation date: YYYY-MM-DD\n#  Author: Your Name\n# =============================================================================\n</code></pre></p> </li> <li> <p>Follow established patterns:</p> </li> <li>Use rich for output formatting</li> <li>Include comprehensive error handling</li> <li>Provide clear documentation</li> <li>Save results to files when appropriate</li> </ol>"},{"location":"examples/#modifying-existing-examples","title":"Modifying Existing Examples","text":"<p>Common customizations:</p> <ol> <li> <p>Change input data:    <pre><code># Use different sample file\nreview_file = project_root / \"sample_reviews\" / \"your_review.md\"\n</code></pre></p> </li> <li> <p>Adjust processing parameters:    <pre><code># Different word limits for summarization\nresult = modifier.summarize(text=review_text, max_words=200)\n</code></pre></p> </li> <li> <p>Modify output format:    <pre><code># Add custom table columns\ntable.add_column(\"Custom Field\", style=\"cyan\")\n</code></pre></p> </li> </ol>"},{"location":"examples/#testing-examples","title":"Testing Examples","text":""},{"location":"examples/#automated-testing","title":"Automated Testing","text":"<p>Examples can be tested programmatically:</p> <pre><code>import subprocess\nimport sys\n\ndef test_example(example_name):\n    \"\"\"Test an example script.\"\"\"\n    try:\n        result = subprocess.run(\n            [sys.executable, \"-m\", f\"src.examples.{example_name}\"],\n            capture_output=True,\n            text=True,\n            timeout=300\n        )\n        return result.returncode == 0\n    except subprocess.TimeoutExpired:\n        return False\n\n# Test all examples\nexamples = [\n    \"summarizer_usage\",\n    \"rationalize_usage\",\n    \"extract_achievements_usage\",\n    \"review_text_evaluator_usage\",\n    \"visualize_evaluation_radar\"\n]\n\nfor example in examples:\n    success = test_example(example)\n    print(f\"{example}: {'\u2705 PASS' if success else '\u274c FAIL'}\")\n</code></pre>"},{"location":"examples/#manual-testing","title":"Manual Testing","text":"<ol> <li>Run each example individually</li> <li>Verify console output is formatted correctly</li> <li>Check generated files exist and contain valid data</li> <li>Test with different input data</li> </ol>"},{"location":"examples/#performance-considerations","title":"Performance Considerations","text":""},{"location":"examples/#resource-usage","title":"Resource Usage","text":"<p>Examples are designed to be educational rather than production-optimized:</p> <ul> <li>Token usage: Monitor OpenAI API costs when running multiple examples</li> <li>Memory usage: Large review texts may consume significant memory</li> <li>Processing time: Complex operations may take several seconds</li> </ul>"},{"location":"examples/#optimization-tips","title":"Optimization Tips","text":"<p>For production use:</p> <ol> <li>Implement caching: Reuse TextModifiers instances</li> <li>Batch processing: Process multiple reviews efficiently</li> <li>Async processing: Use async/await for concurrent operations</li> <li>Resource monitoring: Track API usage and costs</li> </ol>"},{"location":"examples/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Import errors:    <pre><code># Ensure PYTHONPATH is set correctly\nexport PYTHONPATH=\"${PYTHONPATH}:$(pwd)/src\"\n</code></pre></p> </li> <li> <p>API key issues:    <pre><code># Verify environment variables\necho $OPENAI_API_KEY\n</code></pre></p> </li> <li> <p>File not found errors:    <pre><code># Run from project root directory\ncd /path/to/metamorphosis\npython -m src.examples.example_name\n</code></pre></p> </li> </ol>"},{"location":"examples/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging for troubleshooting:</p> <pre><code>import logging\nfrom loguru import logger\n\n# Add debug logging\nlogger.add(\"debug.log\", level=\"DEBUG\")\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre>"},{"location":"examples/#contributing","title":"Contributing","text":""},{"location":"examples/#adding-new-examples_1","title":"Adding New Examples","text":"<p>When contributing new examples:</p> <ol> <li>Follow the established code style and patterns</li> <li>Include comprehensive documentation</li> <li>Add error handling and user feedback</li> <li>Test with various input scenarios</li> <li>Update this documentation</li> </ol>"},{"location":"examples/#improving-existing-examples","title":"Improving Existing Examples","text":"<p>Areas for improvement:</p> <ul> <li>Enhanced error messages</li> <li>Additional output formats</li> <li>Performance optimizations</li> <li>More comprehensive examples</li> <li>Better visualization options</li> </ul>"},{"location":"examples/#see-also","title":"See Also","text":"<ul> <li>MCP Package - Core text processing utilities</li> <li>Agents Package - Workflow orchestration</li> <li>Data Models - Pydantic schemas</li> <li>Configuration - Setup and configuration</li> </ul> <p>This documentation is automatically generated and maintained in sync with the example implementations.</p>"},{"location":"examples/extract_achievements_usage/","title":"Achievement Extraction Usage Example","text":"<p>Example usage of the TextModifiers.extract_achievements method.</p> <p>This script demonstrates how to: 1. Load a sample employee review from the sample_reviews directory 2. Extract key achievements using the TextModifiers class 3. Display the results in a beautiful rich table format 4. Save the structured achievements data to a JSONL file</p> Run this script from the project root <p>python -m src.examples.extract_achievements_usage</p>"},{"location":"examples/extract_achievements_usage/#examples.extract_achievements_usage.create_achievements_table","title":"<code>create_achievements_table(achievements_list)</code>","text":"<p>Create a rich table displaying the extracted achievements.</p> <p>Parameters:</p> Name Type Description Default <code>achievements_list</code> <code>AchievementsList</code> <p>The AchievementsList object containing extracted achievements.</p> required <p>Returns:</p> Type Description <code>Table</code> <p>A rich Table object formatted for display.</p> Source code in <code>src/examples/extract_achievements_usage.py</code> <pre><code>def create_achievements_table(achievements_list: AchievementsList) -&gt; Table:\n    \"\"\"Create a rich table displaying the extracted achievements.\n\n    Args:\n        achievements_list: The AchievementsList object containing extracted achievements.\n\n    Returns:\n        A rich Table object formatted for display.\n    \"\"\"\n    # Create the main table\n    table = Table(\n        title=(\n            f\"\ud83c\udfc6 Extracted Key Achievements \"\n            f\"({len(achievements_list.items)} items, ~{achievements_list.size} tokens)\"\n        ),\n        box=box.ROUNDED,\n        show_header=True,\n        header_style=\"bold magenta\",\n        title_style=\"bold blue\",\n        expand=True,\n    )\n\n    # Add columns\n    table.add_column(\"Title\", style=\"bold cyan\", width=25)\n    table.add_column(\"Outcome\", style=\"white\", width=40)\n    table.add_column(\"Impact Area\", style=\"bold green\", justify=\"center\", width=12)\n    table.add_column(\"Metrics\", style=\"bold yellow\", width=15)\n    table.add_column(\"Details\", style=\"dim white\", width=20)\n\n    # Add rows for each achievement\n    for i, achievement in enumerate(achievements_list.items, 1):\n        # Format metrics as a comma-separated string\n        metrics_text = \", \".join(achievement.metric_strings) if achievement.metric_strings else \"\u2014\"\n\n        # Format additional details (timeframe, scope, collaborators)\n        details_parts = []\n        if achievement.timeframe:\n            details_parts.append(f\"\u23f0 {achievement.timeframe}\")\n        if achievement.ownership_scope:\n            details_parts.append(f\"\ud83d\udc64 {achievement.ownership_scope}\")\n        if achievement.collaborators:\n            collabs = \", \".join(achievement.collaborators[:2])  # Show first 2 collaborators\n            if len(achievement.collaborators) &gt; 2:\n                collabs += f\" +{len(achievement.collaborators) - 2}\"\n            details_parts.append(f\"\ud83e\udd1d {collabs}\")\n\n        details_text = \"\\n\".join(details_parts) if details_parts else \"\u2014\"\n\n        # Color-code impact areas\n        impact_colors = {\n            \"reliability\": \"red\",\n            \"performance\": \"blue\",\n            \"security\": \"magenta\",\n            \"cost\": \"green\",\n            \"revenue\": \"bold green\",\n            \"customer\": \"cyan\",\n            \"delivery_speed\": \"yellow\",\n            \"quality\": \"white\",\n            \"compliance\": \"dim white\",\n            \"team\": \"bold blue\",\n        }\n        impact_color = impact_colors.get(achievement.impact_area, \"white\")\n        impact_text = Text(achievement.impact_area, style=impact_color)\n\n        # Add the row\n        table.add_row(\n            f\"{i}. {achievement.title}\",\n            achievement.outcome,\n            impact_text,\n            metrics_text,\n            details_text,\n        )\n\n    return table\n</code></pre>"},{"location":"examples/extract_achievements_usage/#examples.extract_achievements_usage.create_summary_panel","title":"<code>create_summary_panel(achievements_list)</code>","text":"<p>Create a summary panel with statistics about the achievements.</p> <p>Parameters:</p> Name Type Description Default <code>achievements_list</code> <code>AchievementsList</code> <p>The AchievementsList object containing extracted achievements.</p> required <p>Returns:</p> Type Description <code>Panel</code> <p>A rich Panel object with summary statistics.</p> Source code in <code>src/examples/extract_achievements_usage.py</code> <pre><code>def create_summary_panel(achievements_list: AchievementsList) -&gt; Panel:\n    \"\"\"Create a summary panel with statistics about the achievements.\n\n    Args:\n        achievements_list: The AchievementsList object containing extracted achievements.\n\n    Returns:\n        A rich Panel object with summary statistics.\n    \"\"\"\n    if not achievements_list.items:\n        return Panel(\n            \"No achievements were extracted from the review text.\",\n            title=\"\ud83d\udcca Summary\",\n            style=\"dim red\",\n        )\n\n    # Count achievements by impact area\n    impact_counts: dict[str, int] = {}\n    total_metrics = 0\n    achievements_with_timeframes = 0\n    achievements_with_collaborators = 0\n\n    for achievement in achievements_list.items:\n        impact_counts[achievement.impact_area] = impact_counts.get(achievement.impact_area, 0) + 1\n        total_metrics += len(achievement.metric_strings)\n        if achievement.timeframe:\n            achievements_with_timeframes += 1\n        if achievement.collaborators:\n            achievements_with_collaborators += 1\n\n    # Format the summary text\n    summary_lines = [\n        f\"\ud83d\udcc8 Total Achievements: {len(achievements_list.items)}\",\n        f\"\ud83d\udcca Total Metrics Found: {total_metrics}\",\n        f\"\u23f0 With Timeframes: {achievements_with_timeframes}\",\n        f\"\ud83e\udd1d With Collaborators: {achievements_with_collaborators}\",\n        f\"\ud83c\udfaf Token Estimate: {achievements_list.size}\",\n        \"\",\n        \"\ud83d\udccb Impact Areas:\",\n    ]\n\n    # Add impact area breakdown\n    for impact_area, count in sorted(impact_counts.items()):\n        summary_lines.append(f\"  \u2022 {impact_area}: {count}\")\n\n    return Panel(\n        \"\\n\".join(summary_lines), title=\"\ud83d\udcca Achievements Summary\", style=\"dim blue\", box=box.SIMPLE\n    )\n</code></pre>"},{"location":"examples/extract_achievements_usage/#examples.extract_achievements_usage.load_sample_review","title":"<code>load_sample_review()</code>","text":"<p>Load the sample copy-edited review from sample_reviews directory.</p> <p>Returns:</p> Type Description <code>str</code> <p>The content of the copy_edited.md file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the sample review file doesn't exist.</p> Source code in <code>src/examples/extract_achievements_usage.py</code> <pre><code>def load_sample_review() -&gt; str:\n    \"\"\"Load the sample copy-edited review from sample_reviews directory.\n\n    Returns:\n        The content of the copy_edited.md file.\n\n    Raises:\n        FileNotFoundError: If the sample review file doesn't exist.\n    \"\"\"\n    project_root = get_project_root()\n    sample_file = project_root / \"sample_reviews\" / \"copy_edited.md\"\n\n    if not sample_file.exists():\n        raise FileNotFoundError(f\"Sample review file not found: {sample_file}\")\n\n    return sample_file.read_text(encoding=\"utf-8\")\n</code></pre>"},{"location":"examples/extract_achievements_usage/#examples.extract_achievements_usage.main","title":"<code>main()</code>","text":"<p>Main function that demonstrates achievement extraction.</p> Source code in <code>src/examples/extract_achievements_usage.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main function that demonstrates achievement extraction.\"\"\"\n    console = Console()\n\n    try:\n        # Display header\n        console.print(\"\\n\")\n        console.print(\n            Panel.fit(\n                \"\ud83d\udd0d Achievement Extraction Demo\\n\\n\"\n                \"This example loads a sample employee review and extracts\\n\"\n                \"key achievements using the TextModifiers.extract_achievements method.\",\n                title=\"Metamorphosis Text Processing\",\n                style=\"bold blue\",\n            )\n        )\n\n        # Load the sample review\n        console.print(\"\\n\ud83d\udcd6 Loading sample review from sample_reviews/copy_edited.md...\")\n        review_text = load_sample_review()\n        console.print(f\"\u2705 Loaded review text ({len(review_text):,} characters)\")\n\n        # Initialize TextModifiers\n        console.print(\"\\n\ud83e\udd16 Initializing TextModifiers with LLM clients...\")\n        modifier = TextModifiers()\n        console.print(\"\u2705 TextModifiers initialized successfully\")\n\n        # Extract achievements\n        console.print(\"\\n\ud83d\udd0d Extracting key achievements from the review...\")\n        with console.status(\"[bold green]Processing with LLM...\"):\n            achievements = modifier.extract_achievements(text=review_text)\n\n        console.print(f\"\u2705 Extracted {len(achievements.items)} achievements\")\n\n        # Write achievements to JSONL file\n        console.print(\"\\n\ud83d\udcdd Writing achievements to JSONL file...\")\n        project_root = get_project_root()\n        jsonl_output_path = project_root / \"sample_reviews\" / \"key_achievements.jsonl\"\n        write_achievements_to_jsonl(achievements, jsonl_output_path)\n        console.print(f\"\u2705 Saved achievements to: {jsonl_output_path}\")\n\n        # Display results\n        console.print(\"\\n\")\n        console.print(create_summary_panel(achievements))\n        console.print(\"\\n\")\n        console.print(create_achievements_table(achievements))\n\n        # Display raw data option\n        console.print(\"\\n\")\n        console.print(\n            Panel(\n                \"\ud83d\udca1 Tip: The achievements are returned as structured Pydantic objects\\n\"\n                \"that can be easily serialized to JSON or integrated into other systems.\\n\\n\"\n                \"Each Achievement object contains: title, outcome, impact_area,\\n\"\n                \"metric_strings, timeframe, ownership_scope, and collaborators.\\n\\n\"\n                f\"\ud83d\udcc4 The complete data has been saved to: {jsonl_output_path.name}\",\n                title=\"\u2139\ufe0f  Integration Notes\",\n                style=\"dim cyan\",\n            )\n        )\n\n    except FileNotFoundError as e:\n        console.print(f\"\u274c Error: {e}\", style=\"bold red\")\n        sys.exit(1)\n    except Exception as e:\n        logger.exception(\"Unexpected error during achievement extraction\")\n        console.print(f\"\u274c Unexpected error: {e}\", style=\"bold red\")\n        sys.exit(1)\n</code></pre>"},{"location":"examples/extract_achievements_usage/#examples.extract_achievements_usage.write_achievements_to_jsonl","title":"<code>write_achievements_to_jsonl(achievements_list, output_path)</code>","text":"<p>Write the achievements to a JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>achievements_list</code> <code>AchievementsList</code> <p>The AchievementsList object containing extracted achievements.</p> required <code>output_path</code> <code>Path</code> <p>Path to the output JSONL file.</p> required Source code in <code>src/examples/extract_achievements_usage.py</code> <pre><code>def write_achievements_to_jsonl(achievements_list: AchievementsList, output_path: Path) -&gt; None:\n    \"\"\"Write the achievements to a JSONL file.\n\n    Args:\n        achievements_list: The AchievementsList object containing extracted achievements.\n        output_path: Path to the output JSONL file.\n    \"\"\"\n    # Create the directory if it doesn't exist\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Convert the AchievementsList to dict and write to JSONL\n    with output_path.open(\"w\", encoding=\"utf-8\") as f:\n        # Write the full AchievementsList object as one JSON line\n        achievements_dict = achievements_list.model_dump()\n        f.write(json.dumps(achievements_dict, ensure_ascii=False) + \"\\n\")\n\n    logger.debug(\"Wrote achievements to JSONL file: {}\", output_path)\n</code></pre>"},{"location":"examples/extract_achievements_usage/#overview","title":"Overview","text":"<p>The <code>extract_achievements_usage.py</code> script demonstrates the powerful achievement extraction capabilities of the Metamorphosis system. It showcases how to extract, rank, and display structured accomplishments from employee self-review text using advanced LLM processing.</p>"},{"location":"examples/extract_achievements_usage/#features","title":"Features","text":""},{"location":"examples/extract_achievements_usage/#core-functionality","title":"Core Functionality","text":"<ul> <li>Structured Achievement Extraction: Identifies up to 10 key accomplishments</li> <li>Rich Metadata: Impact areas, metrics, timeframes, and collaborators</li> <li>Ranking Algorithm: Achievements ranked by business impact and quality</li> <li>Professional Display: Beautiful rich table formatting with color coding</li> <li>Data Persistence: JSONL output for further processing and analysis</li> </ul>"},{"location":"examples/extract_achievements_usage/#visual-output","title":"Visual Output","text":"graph LR     A[Raw Review Text] --&gt; B[TextModifiers]     B --&gt; C[Achievement Extraction]     C --&gt; D[Rich Table Display]     C --&gt; E[JSONL File Output]     D --&gt; F[Console Output]     E --&gt; G[key_achievements.jsonl]"},{"location":"examples/extract_achievements_usage/#usage","title":"Usage","text":""},{"location":"examples/extract_achievements_usage/#basic-execution","title":"Basic Execution","text":"<p>Run from the project root directory:</p> <pre><code># Using Python directly\npython -m src.examples.extract_achievements_usage\n\n# Using uv (recommended)\nuv run python -m src.examples.extract_achievements_usage\n</code></pre>"},{"location":"examples/extract_achievements_usage/#expected-output","title":"Expected Output","text":"<p>The script produces rich console output with:</p> <ol> <li>Processing Status: Real-time feedback on extraction progress</li> <li>Achievement Table: Formatted display of extracted accomplishments</li> <li>Summary Panel: Overview of results and integration notes</li> <li>File Output: JSONL file creation confirmation</li> </ol>"},{"location":"examples/extract_achievements_usage/#code-structure","title":"Code Structure","text":""},{"location":"examples/extract_achievements_usage/#main-processing-flow","title":"Main Processing Flow","text":"<pre><code>def main() -&gt; None:\n    \"\"\"Main execution function with comprehensive error handling.\"\"\"\n    console = Console()\n\n    try:\n        # 1. Load sample review text\n        review_text = load_sample_review()\n\n        # 2. Initialize text processor\n        modifier = TextModifiers()\n\n        # 3. Extract achievements\n        achievements = modifier.extract_achievements(text=review_text)\n\n        # 4. Display results\n        display_results(console, achievements)\n\n        # 5. Save to JSONL\n        save_to_jsonl(achievements)\n\n    except Exception as e:\n        handle_error(console, e)\n</code></pre>"},{"location":"examples/extract_achievements_usage/#key-functions","title":"Key Functions","text":""},{"location":"examples/extract_achievements_usage/#achievement-table-creation","title":"Achievement Table Creation","text":"<pre><code>def create_achievements_table(achievements_list: AchievementsList) -&gt; Table:\n    \"\"\"Create a rich table displaying extracted achievements.\"\"\"\n    table = Table(\n        title=(\n            f\"\ud83c\udfc6 Extracted Key Achievements \"\n            f\"({len(achievements_list.items)} items, ~{achievements_list.size} tokens)\"\n        ),\n        box=box.ROUNDED,\n        show_header=True,\n        header_style=\"bold magenta\"\n    )\n\n    # Add columns with appropriate styling\n    table.add_column(\"Title\", style=\"bold cyan\", width=25)\n    table.add_column(\"Outcome\", style=\"white\", width=40)\n    table.add_column(\"Impact\", style=\"green\", width=15)\n    table.add_column(\"Metrics\", style=\"yellow\", width=20)\n    table.add_column(\"Timeline\", style=\"blue\", width=12)\n\n    # Populate table rows\n    for i, achievement in enumerate(achievements_list.items, 1):\n        table.add_row(\n            f\"{i}. {achievement.title}\",\n            achievement.outcome,\n            format_impact_area(achievement.impact_area),\n            format_metrics(achievement.metric_strings),\n            achievement.timeframe or \"Not specified\"\n        )\n\n    return table\n</code></pre>"},{"location":"examples/extract_achievements_usage/#jsonl-output","title":"JSONL Output","text":"<pre><code>def write_achievements_to_jsonl(\n    achievements_list: AchievementsList, \n    output_path: Path\n) -&gt; None:\n    \"\"\"Write achievements to JSONL format for further processing.\"\"\"\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with output_path.open(\"w\", encoding=\"utf-8\") as f:\n        achievements_dict = achievements_list.model_dump()\n        f.write(json.dumps(achievements_dict, ensure_ascii=False) + \"\\n\")\n\n    logger.debug(f\"Achievements written to {output_path}\")\n</code></pre>"},{"location":"examples/extract_achievements_usage/#sample-input","title":"Sample Input","text":"<p>The script uses a sample employee review from <code>sample_reviews/copy_edited.md</code>:</p> <pre><code>I had an eventful cycle this summer. Learnt agentic workflows and \nimplemented a self-reviewer agent process. It significantly improved \nemployee productivity for the organization.\n\nKey accomplishments include:\n- Reduced review processing time by 60%\n- Implemented automated quality checks\n- Collaborated with HR and Engineering teams\n- Delivered training to 50+ employees\n</code></pre>"},{"location":"examples/extract_achievements_usage/#sample-output","title":"Sample Output","text":""},{"location":"examples/extract_achievements_usage/#console-display","title":"Console Display","text":"<p>The script produces a beautifully formatted table:</p> <pre><code>\ud83c\udfc6 Extracted Key Achievements (3 items, ~145 tokens)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Title                       \u2502 Outcome                              \u2502 Impact          \u2502 Metrics              \u2502 Timeline     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1. Implemented self-reviewer\u2502 Delivered automated agent process   \u2502 \ud83d\ude80 Innovation   \u2502 60%                  \u2502 Summer 2025  \u2502\n\u2502    agent process           \u2502 that improved productivity           \u2502                 \u2502                      \u2502              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2. Reduced review processing\u2502 Achieved 60% reduction in processing\u2502 \u26a1 Performance  \u2502 60%, 50+             \u2502 Not specified\u2502\n\u2502    time                    \u2502 time through automation              \u2502                 \u2502                      \u2502              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 3. Delivered employee       \u2502 Trained 50+ employees on new        \u2502 \ud83d\udc65 Productivity \u2502 50+                  \u2502 Not specified\u2502\n\u2502    training program        \u2502 workflow processes                   \u2502                 \u2502                      \u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"examples/extract_achievements_usage/#jsonl-output_1","title":"JSONL Output","text":"<p>The generated <code>key_achievements.jsonl</code> file contains:</p> <pre><code>{\n  \"items\": [\n    {\n      \"title\": \"Implemented self-reviewer agent process\",\n      \"outcome\": \"Delivered automated agent process that improved productivity\",\n      \"impact_area\": \"innovation\",\n      \"metric_strings\": [\"60%\"],\n      \"timeframe\": \"Summer 2025\",\n      \"ownership_scope\": \"TechLead\",\n      \"collaborators\": [\"HR team\", \"Engineering teams\"]\n    }\n  ],\n  \"size\": 145,\n  \"unit\": \"tokens\"\n}\n</code></pre>"},{"location":"examples/extract_achievements_usage/#advanced-features","title":"Advanced Features","text":""},{"location":"examples/extract_achievements_usage/#impact-area-classification","title":"Impact Area Classification","text":"<p>Achievements are automatically classified into impact areas:</p> <ul> <li>\ud83d\udd27 Reliability: System stability and uptime improvements</li> <li>\u26a1 Performance: Speed and efficiency optimizations</li> <li>\ud83d\udd12 Security: Security enhancements and compliance</li> <li>\ud83d\udcb0 Cost Efficiency: Resource optimization and savings</li> <li>\ud83d\udc64 User Experience: User-facing improvements</li> <li>\ud83d\udc65 Productivity: Developer and team productivity gains</li> <li>\ud83d\udcc8 Scalability: System scaling and capacity improvements</li> <li>\ud83d\ude80 Innovation: New technologies and methodologies</li> <li>\ud83d\udccb Compliance: Regulatory and policy adherence</li> <li>\u2699\ufe0f Operations: Operational excellence improvements</li> <li>\ud83d\udcca Business Growth: Revenue and business impact</li> <li>\ud83d\udd28 Technical Debt: Code quality and maintenance</li> </ul>"},{"location":"examples/extract_achievements_usage/#metric-extraction","title":"Metric Extraction","text":"<p>The system extracts quantitative metrics directly from the text:</p> <pre><code># Examples of extracted metrics\nachievement.metric_strings = [\n    \"60%\",           # Percentage improvements\n    \"480ms\",         # Latency measurements  \n    \"50+\",           # Quantity indicators\n    \"$100K\",         # Cost savings\n    \"99.9%\",         # Availability metrics\n    \"10x\"            # Multiplier improvements\n]\n</code></pre>"},{"location":"examples/extract_achievements_usage/#collaboration-tracking","title":"Collaboration Tracking","text":"<p>Identifies and tracks collaboration patterns:</p> <pre><code>achievement.collaborators = [\n    \"HR team\",\n    \"Engineering teams\", \n    \"Product Management\",\n    \"Data Science team\"\n]\n</code></pre>"},{"location":"examples/extract_achievements_usage/#timeframe-detection","title":"Timeframe Detection","text":"<p>Automatically detects and extracts timeframes:</p> <pre><code>achievement.timeframe = \"H1 2025\"  # Half-year periods\nachievement.timeframe = \"Q3\"       # Quarter periods  \nachievement.timeframe = \"Summer\"   # Seasonal periods\nachievement.timeframe = \"2024\"     # Annual periods\n</code></pre>"},{"location":"examples/extract_achievements_usage/#customization","title":"Customization","text":""},{"location":"examples/extract_achievements_usage/#input-modification","title":"Input Modification","text":"<p>To use different input text:</p> <pre><code># Option 1: Modify the review file\n# Edit sample_reviews/copy_edited.md\n\n# Option 2: Use different file\nreview_file = project_root / \"sample_reviews\" / \"your_review.md\"\n\n# Option 3: Use inline text\nreview_text = \"\"\"\nYour custom employee review text here...\n\"\"\"\n</code></pre>"},{"location":"examples/extract_achievements_usage/#output-customization","title":"Output Customization","text":"<p>Modify the table display:</p> <pre><code># Add custom columns\ntable.add_column(\"Custom Field\", style=\"cyan\")\n\n# Change color scheme\ntable = Table(\n    title=\"Custom Title\",\n    box=box.SIMPLE,  # Different box style\n    header_style=\"bold green\"  # Different header color\n)\n\n# Custom row formatting\nfor achievement in achievements.items:\n    custom_value = process_achievement(achievement)\n    table.add_row(\n        achievement.title,\n        achievement.outcome,\n        custom_value  # Your custom processing\n    )\n</code></pre>"},{"location":"examples/extract_achievements_usage/#processing-parameters","title":"Processing Parameters","text":"<p>The extraction can be customized through prompt modification:</p> <pre><code># Modify prompts/key_achievements_system_prompt.md to:\n# - Change ranking criteria\n# - Adjust output format\n# - Modify quality thresholds\n# - Add custom instructions\n</code></pre>"},{"location":"examples/extract_achievements_usage/#error-handling","title":"Error Handling","text":""},{"location":"examples/extract_achievements_usage/#comprehensive-error-management","title":"Comprehensive Error Management","text":"<pre><code>try:\n    achievements = modifier.extract_achievements(text=review_text)\nexcept ValidationError as e:\n    console.print(f\"\u274c Input validation failed: {e}\", style=\"bold red\")\nexcept PostconditionError as e:\n    console.print(f\"\u274c Processing failed: {e}\", style=\"bold red\")\nexcept FileNotFoundError as e:\n    console.print(f\"\u274c Sample file not found: {e}\", style=\"bold red\")\nexcept Exception as e:\n    console.print(f\"\u274c Unexpected error: {e}\", style=\"bold red\")\n    logger.exception(\"Unexpected error in main execution\")\n</code></pre>"},{"location":"examples/extract_achievements_usage/#graceful-degradation","title":"Graceful Degradation","text":"<p>The script handles various edge cases:</p> <ul> <li>Empty input text: Provides helpful error message</li> <li>No achievements found: Displays appropriate message</li> <li>File access issues: Suggests alternative approaches</li> <li>API failures: Provides debugging information</li> </ul>"},{"location":"examples/extract_achievements_usage/#performance-considerations","title":"Performance Considerations","text":""},{"location":"examples/extract_achievements_usage/#token-usage","title":"Token Usage","text":"<ul> <li>Typical usage: 100-500 tokens per review</li> <li>Large reviews: May use 1000+ tokens</li> <li>Cost estimation: Monitor OpenAI API usage</li> </ul>"},{"location":"examples/extract_achievements_usage/#processing-time","title":"Processing Time","text":"<ul> <li>Simple reviews: 2-5 seconds</li> <li>Complex reviews: 5-15 seconds</li> <li>Network dependent: API latency affects total time</li> </ul>"},{"location":"examples/extract_achievements_usage/#memory-usage","title":"Memory Usage","text":"<ul> <li>Lightweight: Minimal memory footprint</li> <li>Scalable: Can process multiple reviews sequentially</li> <li>Efficient: Proper cleanup of temporary objects</li> </ul>"},{"location":"examples/extract_achievements_usage/#integration-examples","title":"Integration Examples","text":""},{"location":"examples/extract_achievements_usage/#batch-processing","title":"Batch Processing","text":"<pre><code>def process_multiple_reviews():\n    modifier = TextModifiers()\n    results = []\n\n    for review_file in review_files:\n        try:\n            text = load_review(review_file)\n            achievements = modifier.extract_achievements(text=text)\n            results.append({\n                'file': review_file,\n                'achievements': achievements\n            })\n        except Exception as e:\n            logger.error(f\"Failed to process {review_file}: {e}\")\n\n    return results\n</code></pre>"},{"location":"examples/extract_achievements_usage/#api-integration","title":"API Integration","text":"<pre><code>from fastapi import FastAPI\nfrom metamorphosis.mcp.text_modifiers import TextModifiers\n\napp = FastAPI()\nmodifier = TextModifiers()\n\n@app.post(\"/extract-achievements\")\nasync def extract_achievements(request: dict):\n    try:\n        achievements = modifier.extract_achievements(\n            text=request[\"review_text\"]\n        )\n        return achievements.model_dump()\n    except Exception as e:\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"examples/extract_achievements_usage/#data-pipeline-integration","title":"Data Pipeline Integration","text":"<pre><code>def create_achievement_pipeline():\n    \"\"\"Create a data processing pipeline.\"\"\"\n    modifier = TextModifiers()\n\n    def process_review(review_text: str) -&gt; dict:\n        achievements = modifier.extract_achievements(text=review_text)\n        return {\n            'achievements_count': len(achievements.items),\n            'total_tokens': achievements.size,\n            'top_impact_area': get_top_impact_area(achievements),\n            'data': achievements.model_dump()\n        }\n\n    return process_review\n</code></pre>"},{"location":"examples/extract_achievements_usage/#testing","title":"Testing","text":""},{"location":"examples/extract_achievements_usage/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom examples.extract_achievements_usage import (\n    create_achievements_table,\n    write_achievements_to_jsonl\n)\n\ndef test_table_creation():\n    # Create mock achievements\n    achievements = create_mock_achievements()\n\n    # Test table creation\n    table = create_achievements_table(achievements)\n    assert table.title\n    assert len(table.columns) == 5\n\ndef test_jsonl_output(tmp_path):\n    achievements = create_mock_achievements()\n    output_file = tmp_path / \"test_output.jsonl\"\n\n    write_achievements_to_jsonl(achievements, output_file)\n\n    assert output_file.exists()\n    # Verify content is valid JSON\n    import json\n    with output_file.open() as f:\n        data = json.load(f)\n        assert \"items\" in data\n</code></pre>"},{"location":"examples/extract_achievements_usage/#integration-tests","title":"Integration Tests","text":"<pre><code>def test_full_extraction_pipeline():\n    \"\"\"Test the complete extraction process.\"\"\"\n    from examples.extract_achievements_usage import main\n\n    # Mock the input and run the main function\n    # Verify output files are created\n    # Check console output formatting\n</code></pre>"},{"location":"examples/extract_achievements_usage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/extract_achievements_usage/#common-issues","title":"Common Issues","text":"<ol> <li>No achievements extracted:</li> <li>Review text may be too vague or activity-focused</li> <li>Try text with more concrete outcomes and metrics</li> <li> <p>Check the prompt templates for guidance</p> </li> <li> <p>Low-quality extractions:</p> </li> <li>Input text lacks specific details</li> <li>Missing quantitative information</li> <li> <p>No clear impact statements</p> </li> <li> <p>API errors:</p> </li> <li>Verify OpenAI API key is valid</li> <li>Check rate limits and quotas</li> <li>Monitor network connectivity</li> </ol>"},{"location":"examples/extract_achievements_usage/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging:</p> <pre><code>import logging\nfrom loguru import logger\n\n# Add debug logging\nlogger.add(\"achievement_debug.log\", level=\"DEBUG\")\nlogging.basicConfig(level=logging.DEBUG)\n\n# Run the script - all operations will be logged\n</code></pre>"},{"location":"examples/extract_achievements_usage/#see-also","title":"See Also","text":"<ul> <li>TextModifiers Class - Core achievement extraction implementation</li> <li>Data Models - Achievement and AchievementsList schemas</li> <li>Other Examples - Additional usage examples</li> <li>MCP Package - Text processing architecture</li> </ul> <p>This documentation is automatically generated from the source code and maintained in sync with the implementation.</p>"},{"location":"metamorphosis/","title":"Metamorphosis Package","text":"<p>The <code>metamorphosis</code> package is the core module that provides AI-powered text processing capabilities for employee self-review workflows. It serves as the central orchestration point for LLM-based text utilities, agent workflows, and user interfaces.</p>"},{"location":"metamorphosis/#package-architecture","title":"Package Architecture","text":"graph TB     subgraph \"metamorphosis Package\"         A[__init__.pyConfiguration &amp; Registry]         B[datamodel.pyPydantic Models]         C[exceptions.pyError Handling]         D[utilities.pyCommon Utilities]         E[model_registry.pyLLM Management]     end      subgraph \"Sub-packages\"         F[mcp/MCP Tools]         G[agents/LangGraph Workflows]         H[ui/User Interfaces]     end      A --&gt; E     A --&gt; B     A --&gt; C     B --&gt; F     E --&gt; F     F --&gt; G     G --&gt; H      subgraph \"External Dependencies\"         I[OpenAI API]         J[LangChain]         K[Pydantic]     end      E --&gt; I     F --&gt; J     B --&gt; K"},{"location":"metamorphosis/#core-components","title":"Core Components","text":""},{"location":"metamorphosis/#configuration-management","title":"Configuration Management","text":"<p>The package provides centralized configuration management through:</p> <ul> <li>Environment Variable Loading: Automatic <code>.env</code> file processing</li> <li>YAML Configuration: Structured configuration via <code>config.yaml</code></li> <li>Model Registry: Centralized LLM client management</li> <li>Project Root Resolution: Dynamic project path detection</li> </ul>"},{"location":"metamorphosis/#data-models","title":"Data Models","text":"<p>Comprehensive Pydantic models for type-safe data handling:</p> <ul> <li>Processing Results: Structured outputs for all text processing operations</li> <li>Request/Response Models: API contract definitions</li> <li>Workflow State: LangGraph state management schemas</li> <li>Configuration Models: Type-safe configuration validation</li> </ul>"},{"location":"metamorphosis/#error-handling","title":"Error Handling","text":"<p>Robust exception hierarchy with:</p> <ul> <li>Specific Error Types: Granular error classification</li> <li>Context Preservation: Rich error context for debugging</li> <li>Operation Tracking: Error source identification</li> <li>Graceful Degradation: Fallback mechanisms</li> </ul>"},{"location":"metamorphosis/#module-documentation","title":"Module Documentation","text":""},{"location":"metamorphosis/#core-modules","title":"Core Modules","text":"Module Description Key Components <code>__init__.py</code> Package initialization and configuration <code>_load_config()</code>, <code>get_model_registry()</code> <code>datamodel.py</code> Pydantic data models <code>SummarizedText</code>, <code>CopyEditedText</code>, <code>AchievementsList</code> <code>exceptions.py</code> Exception hierarchy <code>PostconditionError</code>, <code>ValidationError</code>, <code>MCPToolError</code> <code>utilities.py</code> Common utility functions <code>get_project_root()</code>, <code>read_text_file()</code> <code>model_registry.py</code> LLM client management <code>ModelRegistry</code>, <code>_LLMSettings</code>"},{"location":"metamorphosis/#sub-packages","title":"Sub-packages","text":"Package Purpose Documentation <code>mcp/</code> Model Context Protocol integration View Details \u2192 <code>agents/</code> LangGraph agent workflows View Details \u2192 <code>ui/</code> Streamlit user interfaces View Details \u2192"},{"location":"metamorphosis/#usage-patterns","title":"Usage Patterns","text":""},{"location":"metamorphosis/#basic-initialization","title":"Basic Initialization","text":"<pre><code>from metamorphosis import get_model_registry\nfrom metamorphosis.mcp.text_modifiers import TextModifiers\n\n# Get configured LLM clients\nregistry = get_model_registry()\n\n# Initialize text processing utilities\nmodifiers = TextModifiers()\n</code></pre>"},{"location":"metamorphosis/#configuration-access","title":"Configuration Access","text":"<pre><code>from metamorphosis import config\n\n# Access configuration values\nopenai_key = config.get('openai_api_key')\nmodel_settings = config.get('models', {})\n</code></pre>"},{"location":"metamorphosis/#error-handling_1","title":"Error Handling","text":"<pre><code>from metamorphosis.exceptions import PostconditionError, ValidationError\n\ntry:\n    result = modifiers.summarize(text=\"...\")\nexcept ValidationError as e:\n    print(f\"Validation failed: {e}\")\nexcept PostconditionError as e:\n    print(f\"Processing failed: {e}\")\n</code></pre>"},{"location":"metamorphosis/#design-patterns","title":"Design Patterns","text":""},{"location":"metamorphosis/#singleton-registry","title":"Singleton Registry","text":"<p>The <code>ModelRegistry</code> implements a singleton pattern to ensure efficient resource usage:</p> <pre><code>class ModelRegistry:\n    _instance = None\n    _initialized = False\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n</code></pre>"},{"location":"metamorphosis/#factory-pattern","title":"Factory Pattern","text":"<p>Configuration loading uses a factory pattern with lazy initialization:</p> <pre><code>def get_model_registry() -&gt; ModelRegistry:\n    \"\"\"Factory function for ModelRegistry singleton.\"\"\"\n    return ModelRegistry()\n</code></pre>"},{"location":"metamorphosis/#validation-decorators","title":"Validation Decorators","text":"<p>Consistent validation using Pydantic decorators:</p> <pre><code>@validate_call\ndef process_text(\n    text: Annotated[str, Field(min_length=1)],\n    max_words: Annotated[int, Field(gt=0)] = 300\n) -&gt; SummarizedText:\n    # Implementation with automatic validation\n</code></pre>"},{"location":"metamorphosis/#configuration-schema","title":"Configuration Schema","text":""},{"location":"metamorphosis/#environment-variables","title":"Environment Variables","text":"<pre><code># Required\nOPENAI_API_KEY: str  # OpenAI API key\nPROJECT_ROOT_DIR: str  # Project root directory\n\n# Optional\nMCP_SERVER_HOST: str = \"localhost\"\nMCP_SERVER_PORT: int = 8000\nFASTAPI_HOST: str = \"0.0.0.0\"\nFASTAPI_PORT: int = 8001\n</code></pre>"},{"location":"metamorphosis/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>models:\n  summarizer:\n    model: str  # Model name (e.g., \"gpt-4o\")\n    temperature: float  # Sampling temperature\n    max_tokens: int  # Maximum output tokens\n    timeout: int  # Request timeout in seconds\n\n  copy_editor:\n    model: str\n    temperature: float\n    max_tokens: int\n    timeout: int\n\n  key_achievements:\n    model: str\n    temperature: float\n    max_tokens: int\n    timeout: int\n\n  review_text_evaluator:\n    model: str\n    temperature: float\n    max_tokens: int\n    timeout: int\n</code></pre>"},{"location":"metamorphosis/#performance-considerations","title":"Performance Considerations","text":""},{"location":"metamorphosis/#resource-management","title":"Resource Management","text":"<ul> <li>LLM Client Caching: Singleton registry prevents multiple client instantiation</li> <li>Connection Pooling: Efficient HTTP connection reuse</li> <li>Memory Management: Proper cleanup of large text processing results</li> </ul>"},{"location":"metamorphosis/#optimization-strategies","title":"Optimization Strategies","text":"<ul> <li>Lazy Loading: Configuration and models loaded on-demand</li> <li>Caching: LRU cache for expensive operations</li> <li>Parallel Processing: Concurrent execution where possible</li> </ul>"},{"location":"metamorphosis/#testing","title":"Testing","text":""},{"location":"metamorphosis/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom metamorphosis import get_model_registry\nfrom metamorphosis.exceptions import ConfigurationError\n\ndef test_model_registry_singleton():\n    registry1 = get_model_registry()\n    registry2 = get_model_registry()\n    assert registry1 is registry2\n\ndef test_configuration_validation():\n    with pytest.raises(ConfigurationError):\n        # Test invalid configuration handling\n        pass\n</code></pre>"},{"location":"metamorphosis/#integration-tests","title":"Integration Tests","text":"<pre><code>def test_end_to_end_processing():\n    modifiers = TextModifiers()\n    result = modifiers.summarize(\n        text=\"Sample employee review text...\",\n        max_words=50\n    )\n    assert isinstance(result, SummarizedText)\n    assert len(result.summarized_text) &gt; 0\n</code></pre>"},{"location":"metamorphosis/#migration-guide","title":"Migration Guide","text":""},{"location":"metamorphosis/#from-v0x-to-v10","title":"From v0.x to v1.0","text":"<ul> <li>Update import statements to use absolute imports</li> <li>Replace manual configuration with <code>get_model_registry()</code></li> <li>Update exception handling to use new hierarchy</li> <li>Migrate to new Pydantic v2 models</li> </ul>"},{"location":"metamorphosis/#troubleshooting","title":"Troubleshooting","text":""},{"location":"metamorphosis/#common-issues","title":"Common Issues","text":"<ol> <li>Configuration Errors:</li> <li>Verify <code>.env</code> file exists and contains required variables</li> <li>Check <code>config.yaml</code> syntax and structure</li> <li> <p>Ensure OpenAI API key is valid</p> </li> <li> <p>Import Errors:</p> </li> <li>Verify <code>PYTHONPATH</code> includes <code>src</code> directory</li> <li>Check for circular import issues</li> <li> <p>Ensure all dependencies are installed</p> </li> <li> <p>Model Loading Issues:</p> </li> <li>Verify OpenAI API connectivity</li> <li>Check rate limits and quotas</li> <li>Monitor token usage</li> </ol>"},{"location":"metamorphosis/#debug-mode","title":"Debug Mode","text":"<p>Enable comprehensive logging:</p> <pre><code>import logging\nfrom loguru import logger\n\n# Enable debug logging\nlogger.add(\"debug.log\", level=\"DEBUG\")\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre>"},{"location":"metamorphosis/#see-also","title":"See Also","text":"<ul> <li>MCP Package Documentation - Text processing tools</li> <li>Agents Package Documentation - Workflow orchestration</li> <li>UI Package Documentation - User interfaces</li> <li>Examples - Usage examples and tutorials</li> </ul> <p>This documentation is automatically generated from the source code and kept in sync with the latest implementation.</p>"},{"location":"metamorphosis/datamodel/","title":"Data Models","text":""},{"location":"metamorphosis/datamodel/#metamorphosis.datamodel.CopyEditedText","title":"<code>CopyEditedText</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Rationalized text with typos and grammar errors corrected.</p> Source code in <code>src/metamorphosis/datamodel.py</code> <pre><code>class CopyEditedText(BaseModel):\n    \"\"\"Rationalized text with typos and grammar errors corrected.\"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    copy_edited_text: str = Field(\n        ..., description=\"The lightly normalized and corrected text\", min_length=1\n    )\n    size: int = Field(..., description=\"The size of the copy-edited text in tokens\")\n    is_edited: bool = Field(..., description=\"Whether the text was edited\")\n</code></pre>"},{"location":"metamorphosis/datamodel/#metamorphosis.datamodel.InvokeRequest","title":"<code>InvokeRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request model for synchronous self-review processing (/invoke).</p> Source code in <code>src/metamorphosis/datamodel.py</code> <pre><code>class InvokeRequest(BaseModel):\n    \"\"\"Request model for synchronous self-review processing (/invoke).\"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    review_text: str = Field(\n        ...,\n        min_length=1,\n        description=\"The review text to process through the LangGraph\",\n        example=(\n            \"I had an eventful cycle this summer. Learnt agentic workflows and implemented a \"\n            \"self-reviewer agent for the periodic employee self-review process. It significantly \"\n            \"improved employee productivity for the organization.\"\n        ),\n    )\n    thread_id: str | None = Field(\n        None,\n        description=(\n            \"Optional thread ID for state persistence. If not provided, a new UUID will be generated.\"\n        ),\n        example=\"thread_123\",\n    )\n</code></pre>"},{"location":"metamorphosis/datamodel/#metamorphosis.datamodel.InvokeResponse","title":"<code>InvokeResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response model for synchronous self-review processing results.</p> Source code in <code>src/metamorphosis/datamodel.py</code> <pre><code>class InvokeResponse(BaseModel):\n    \"\"\"Response model for synchronous self-review processing results.\"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    original_text: str = Field(..., description=\"The original review text\")\n    copy_edited_text: str | None = Field(None, description=\"The copy-edited text\")\n    summary: str | None = Field(None, description=\"The summary of the copy-edited text\")\n    word_cloud_path: str | None = Field(None, description=\"The path to the word cloud image\")\n</code></pre>"},{"location":"metamorphosis/datamodel/#metamorphosis.datamodel.StreamRequest","title":"<code>StreamRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request model for streaming self-review processing via SSE (/stream).</p> Source code in <code>src/metamorphosis/datamodel.py</code> <pre><code>class StreamRequest(BaseModel):\n    \"\"\"Request model for streaming self-review processing via SSE (/stream).\"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    review_text: str = Field(\n        ..., min_length=1, description=\"The review text to process through the LangGraph\"\n    )\n    thread_id: str = Field(\n        ..., min_length=1, description=\"Unique identifier for the conversation thread\"\n    )\n    mode: str = Field(\n        \"values\",\n        pattern=r\"^(values|updates)$\",\n        description=(\n            \"Streaming mode - 'updates' for state changes only, 'values' for full state each step\"\n        ),\n    )\n</code></pre>"},{"location":"metamorphosis/datamodel/#metamorphosis.datamodel.SummarizedText","title":"<code>SummarizedText</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Structured summary output returned by summarization routines.</p> Source code in <code>src/metamorphosis/datamodel.py</code> <pre><code>class SummarizedText(BaseModel):\n    \"\"\"Structured summary output returned by summarization routines.\"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    summarized_text: str = Field(\n        ..., description=\"The generated abstractive summary text\", min_length=1\n    )\n    size: int = Field(..., description=\"The size of the summary in tokens\")\n</code></pre>"},{"location":"metamorphosis/datamodel/#overview","title":"Overview","text":"<p>The <code>metamorphosis.datamodel</code> module provides comprehensive Pydantic data models for type-safe handling of text processing inputs, outputs, and workflow state. All models implement strict validation, serialization, and provide rich metadata for debugging and monitoring.</p>"},{"location":"metamorphosis/datamodel/#core-processing-models","title":"Core Processing Models","text":""},{"location":"metamorphosis/datamodel/#text-processing-results","title":"Text Processing Results","text":"classDiagram     class SummarizedText {         +summarized_text: str         +size: int         +unit: str         +model_config: ConfigDict     }      class CopyEditedText {         +copy_edited_text: str         +size: int         +is_edited: bool         +model_config: ConfigDict     }      class Achievement {         +title: str         +outcome: str         +impact_area: ImpactArea         +metric_strings: List[str]         +timeframe: Optional[str]         +ownership_scope: Optional[OwnershipScope]         +collaborators: List[str]     }      class AchievementsList {         +items: List[Achievement]         +size: int         +unit: str     }      class MetricScore {         +name: str         +score: int         +rationale: str         +suggestion: str     }      class ReviewScorecard {         +metrics: List[MetricScore]         +overall: int         +verdict: Verdict         +notes: List[str]         +radar_labels: List[str]         +radar_values: List[int]     }      AchievementsList --&gt; Achievement : contains     ReviewScorecard --&gt; MetricScore : contains"},{"location":"metamorphosis/datamodel/#enumeration-types","title":"Enumeration Types","text":"classDiagram     class ImpactArea {         &lt;&gt;         reliability         performance         security         cost_efficiency         user_experience         developer_productivity         scalability         innovation         compliance         operational_excellence         business_growth         technical_debt         other     }      class OwnershipScope {         &lt;&gt;         IC         TechLead         Manager         Director         VP         Org_wide         Cross_functional         other     }      class Verdict {         &lt;&gt;         excellent         strong         mixed         weak     }      Achievement --&gt; ImpactArea     Achievement --&gt; OwnershipScope     ReviewScorecard --&gt; Verdict"},{"location":"metamorphosis/datamodel/#api-models","title":"API Models","text":""},{"location":"metamorphosis/datamodel/#request-models","title":"Request Models","text":""},{"location":"metamorphosis/datamodel/#invokerequest","title":"InvokeRequest","text":"<p>Used for synchronous workflow execution via the <code>/invoke</code> endpoint.</p>"},{"location":"metamorphosis/datamodel/#streamrequest","title":"StreamRequest","text":"<p>Used for asynchronous streaming workflow execution via the <code>/stream</code> endpoint.</p>"},{"location":"metamorphosis/datamodel/#response-models","title":"Response Models","text":""},{"location":"metamorphosis/datamodel/#invokeresponse","title":"InvokeResponse","text":"<p>Structured response for synchronous processing results.</p>"},{"location":"metamorphosis/datamodel/#workflowstate","title":"WorkflowState","text":"<p>Internal state management for LangGraph workflows.</p>"},{"location":"metamorphosis/datamodel/#usage-examples","title":"Usage Examples","text":""},{"location":"metamorphosis/datamodel/#processing-results","title":"Processing Results","text":"<pre><code>from metamorphosis.datamodel import SummarizedText, CopyEditedText\n\n# Create a summary result\nsummary = SummarizedText(\n    summarized_text=\"Concise summary of the review...\",\n    size=150,\n    unit=\"tokens\"\n)\n\n# Create a copy editing result\nedited = CopyEditedText(\n    copy_edited_text=\"Professionally edited text...\",\n    size=500,\n    is_edited=True\n)\n\n# Access validated data\nprint(f\"Summary: {summary.summarized_text}\")\nprint(f\"Tokens: {summary.size} {summary.unit}\")\nprint(f\"Was edited: {edited.is_edited}\")\n</code></pre>"},{"location":"metamorphosis/datamodel/#achievement-extraction","title":"Achievement Extraction","text":"<pre><code>from metamorphosis.datamodel import Achievement, AchievementsList, ImpactArea\n\n# Create an achievement\nachievement = Achievement(\n    title=\"Reduced checkout latency\",\n    outcome=\"Improved p95 latency from 480ms to 190ms through cache optimization\",\n    impact_area=ImpactArea.performance,\n    metric_strings=[\"480ms\", \"190ms\"],\n    timeframe=\"H1 2025\",\n    ownership_scope=OwnershipScope.TechLead,\n    collaborators=[\"Payments team\", \"SRE team\"]\n)\n\n# Create achievements list\nachievements = AchievementsList(\n    items=[achievement],\n    size=45,\n    unit=\"tokens\"\n)\n</code></pre>"},{"location":"metamorphosis/datamodel/#quality-evaluation","title":"Quality Evaluation","text":"<pre><code>from metamorphosis.datamodel import MetricScore, ReviewScorecard, Verdict\n\n# Create metric scores\nmetrics = [\n    MetricScore(\n        name=\"OutcomeOverActivity\",\n        score=85,\n        rationale=\"Strong focus on concrete outcomes\",\n        suggestion=\"Continue emphasizing results\"\n    ),\n    MetricScore(\n        name=\"QuantitativeSpecificity\",\n        score=90,\n        rationale=\"Excellent use of specific metrics\",\n        suggestion=\"Maintain this level of detail\"\n    )\n]\n\n# Create scorecard\nscorecard = ReviewScorecard(\n    metrics=metrics,\n    overall=87,\n    verdict=Verdict.excellent,\n    notes=[\"strong_metrics\", \"clear_outcomes\"],\n    radar_labels=[\"OutcomeOverActivity\", \"QuantitativeSpecificity\"],\n    radar_values=[85, 90]\n)\n</code></pre>"},{"location":"metamorphosis/datamodel/#validation-features","title":"Validation Features","text":""},{"location":"metamorphosis/datamodel/#automatic-validation","title":"Automatic Validation","text":"<p>All models use Pydantic's automatic validation:</p> <pre><code>from pydantic import ValidationError\n\ntry:\n    # This will fail validation\n    summary = SummarizedText(\n        summarized_text=\"\",  # Empty string not allowed\n        size=-1,  # Negative size not allowed\n        unit=\"\"  # Empty unit not allowed\n    )\nexcept ValidationError as e:\n    print(f\"Validation errors: {e}\")\n</code></pre>"},{"location":"metamorphosis/datamodel/#field-constraints","title":"Field Constraints","text":"<p>Models implement comprehensive field constraints:</p> <pre><code>class Achievement(BaseModel):\n    title: str = Field(..., min_length=1, max_length=120)\n    outcome: str = Field(..., min_length=1, max_length=400)\n    metric_strings: List[str] = Field(default_factory=list)\n    # ... additional constraints\n</code></pre>"},{"location":"metamorphosis/datamodel/#custom-validators","title":"Custom Validators","text":"<p>Some models include custom validation logic:</p> <pre><code>@field_validator(\"overall\")\n@classmethod\ndef validate_overall_score(cls, v: int) -&gt; int:\n    if not (0 &lt;= v &lt;= 100):\n        raise ValueError(\"Overall score must be between 0 and 100\")\n    return v\n</code></pre>"},{"location":"metamorphosis/datamodel/#serialization","title":"Serialization","text":""},{"location":"metamorphosis/datamodel/#json-serialization","title":"JSON Serialization","text":"<p>All models support JSON serialization:</p> <pre><code># Serialize to JSON\njson_data = summary.model_dump()\njson_string = summary.model_dump_json()\n\n# Deserialize from JSON\nsummary_restored = SummarizedText.model_validate(json_data)\nsummary_from_string = SummarizedText.model_validate_json(json_string)\n</code></pre>"},{"location":"metamorphosis/datamodel/#jsonl-output","title":"JSONL Output","text":"<p>Models work seamlessly with JSONL format:</p> <pre><code>import json\n\n# Write to JSONL\nwith open(\"results.jsonl\", \"w\") as f:\n    for result in processing_results:\n        f.write(result.model_dump_json() + \"\\n\")\n\n# Read from JSONL\nresults = []\nwith open(\"results.jsonl\", \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        result = SummarizedText.model_validate(data)\n        results.append(result)\n</code></pre>"},{"location":"metamorphosis/datamodel/#configuration","title":"Configuration","text":""},{"location":"metamorphosis/datamodel/#model-configuration","title":"Model Configuration","text":"<p>All models use strict configuration:</p> <pre><code>model_config = ConfigDict(\n    str_strip_whitespace=True,  # Auto-strip whitespace\n    validate_assignment=True,   # Validate on assignment\n    extra=\"forbid\",            # Forbid extra fields\n    frozen=False               # Allow field modification\n)\n</code></pre>"},{"location":"metamorphosis/datamodel/#field-documentation","title":"Field Documentation","text":"<p>Models include comprehensive field documentation:</p> <pre><code>class SummarizedText(BaseModel):\n    summarized_text: str = Field(\n        ...,\n        description=\"The generated summary text\",\n        min_length=1\n    )\n    size: int = Field(\n        ...,\n        description=\"Estimated token count of the summary\",\n        ge=0\n    )\n</code></pre>"},{"location":"metamorphosis/datamodel/#testing","title":"Testing","text":""},{"location":"metamorphosis/datamodel/#model-validation-tests","title":"Model Validation Tests","text":"<pre><code>import pytest\nfrom metamorphosis.datamodel import SummarizedText\n\ndef test_summarized_text_validation():\n    # Valid data\n    summary = SummarizedText(\n        summarized_text=\"Valid summary\",\n        size=50,\n        unit=\"tokens\"\n    )\n    assert summary.summarized_text == \"Valid summary\"\n\n    # Invalid data\n    with pytest.raises(ValidationError):\n        SummarizedText(\n            summarized_text=\"\",  # Empty string\n            size=50,\n            unit=\"tokens\"\n        )\n</code></pre>"},{"location":"metamorphosis/datamodel/#serialization-tests","title":"Serialization Tests","text":"<pre><code>def test_json_serialization():\n    original = SummarizedText(\n        summarized_text=\"Test summary\",\n        size=25,\n        unit=\"tokens\"\n    )\n\n    # Round-trip serialization\n    json_data = original.model_dump()\n    restored = SummarizedText.model_validate(json_data)\n\n    assert original == restored\n</code></pre>"},{"location":"metamorphosis/datamodel/#migration-guide","title":"Migration Guide","text":""},{"location":"metamorphosis/datamodel/#from-v0x-to-v10","title":"From v0.x to v1.0","text":"<ol> <li> <p>Update import statements:    <pre><code># Old\nfrom metamorphosis.models import Summary, EditResult\n\n# New\nfrom metamorphosis.datamodel import SummarizedText, CopyEditedText\n</code></pre></p> </li> <li> <p>Update field names:    <pre><code># Old\nresult.summary_text\nresult.token_count\n\n# New\nresult.summarized_text\nresult.size\n</code></pre></p> </li> <li> <p>Handle new validation:    <pre><code># Add proper error handling for validation\ntry:\n    model = SummarizedText(**data)\nexcept ValidationError as e:\n    # Handle validation errors\n    pass\n</code></pre></p> </li> </ol>"},{"location":"metamorphosis/datamodel/#see-also","title":"See Also","text":"<ul> <li>MCP Package - Text processing utilities that use these models</li> <li>Agents Package - Workflow orchestration with state management</li> <li>Examples - Usage examples with real data</li> <li>Pydantic Documentation - Underlying validation framework</li> </ul> <p>This documentation is automatically generated from the source code and maintained in sync with the model definitions.</p>"},{"location":"metamorphosis/agents/","title":"Agents Package","text":"<p>The <code>metamorphosis.agents</code> package implements LangGraph-based agent workflows for orchestrating multi-stage text processing operations. It provides both synchronous and asynchronous processing capabilities through a FastAPI service with real-time streaming support.</p>"},{"location":"metamorphosis/agents/#package-architecture","title":"Package Architecture","text":"graph TB     subgraph \"FastAPI Service Layer\"         A[agent_service.pyREST API Endpoints]         B[Request/Response ModelsPydantic Schemas]     end      subgraph \"LangGraph Workflow Layer\"         C[self_reviewer.pyWorkflow Definition]         D[GraphStateState Management]         E[Workflow NodesProcessing Steps]     end      subgraph \"Processing Integration\"         F[MCP ToolsText Processing]         G[External ServicesWord Cloud Generation]     end      subgraph \"State Management\"         H[Thread-based StorageConversation Persistence]         I[Streaming EventsReal-time Updates]     end      A --&gt; C     A --&gt; B     C --&gt; D     C --&gt; E     E --&gt; F     E --&gt; G     D --&gt; H     A --&gt; I"},{"location":"metamorphosis/agents/#core-components","title":"Core Components","text":""},{"location":"metamorphosis/agents/#1-fastapi-service-agent_servicepy","title":"1. FastAPI Service (<code>agent_service.py</code>)","text":"<p>The main service provides REST API endpoints for text processing workflows:</p>"},{"location":"metamorphosis/agents/#endpoints","title":"Endpoints","text":"<ul> <li><code>POST /invoke</code>: Synchronous workflow execution</li> <li><code>POST /stream</code>: Asynchronous streaming workflow execution</li> <li><code>GET /docs</code>: Interactive API documentation</li> <li><code>GET /health</code>: Service health check</li> </ul>"},{"location":"metamorphosis/agents/#features","title":"Features","text":"<ul> <li>CORS Support: Configurable cross-origin resource sharing</li> <li>Error Handling: Comprehensive exception management</li> <li>Request Validation: Pydantic model validation</li> <li>Streaming Support: Server-Sent Events (SSE) for real-time updates</li> <li>Thread Management: Unique thread IDs for conversation persistence</li> </ul>"},{"location":"metamorphosis/agents/#2-langgraph-workflow-self_reviewerpy","title":"2. LangGraph Workflow (<code>self_reviewer.py</code>)","text":"<p>Implements a sophisticated multi-stage text processing workflow:</p>"},{"location":"metamorphosis/agents/#workflow-stages","title":"Workflow Stages","text":"graph LR     A[START] --&gt; B[Copy Editor Node]     B --&gt; C[Summarizer Node]     B --&gt; D[Word Cloud Node]     C --&gt; E[END]     D --&gt; E"},{"location":"metamorphosis/agents/#node-implementations","title":"Node Implementations","text":"<ol> <li>Copy Editor Node: Text rationalization and improvement</li> <li>Summarizer Node: Abstractive summary generation (parallel)</li> <li>Word Cloud Node: Visual representation generation (parallel)</li> </ol>"},{"location":"metamorphosis/agents/#3-state-management-graphstate","title":"3. State Management (<code>GraphState</code>)","text":"<p>Type-safe state management using TypedDict:</p> <pre><code>class GraphState(TypedDict):\n    review_text: str  # Original input text\n    copy_edited_text: str | None  # Processed text\n    summary: str | None  # Generated summary\n    word_cloud_path: str | None  # Generated visualization path\n</code></pre>"},{"location":"metamorphosis/agents/#module-documentation","title":"Module Documentation","text":""},{"location":"metamorphosis/agents/#core-modules","title":"Core Modules","text":"Module Description Key Components <code>agent_service.py</code> FastAPI REST API service Endpoints, middleware, error handling <code>self_reviewer.py</code> LangGraph workflow implementation Workflow nodes, state management <code>__init__.py</code> Package initialization Exports and configuration"},{"location":"metamorphosis/agents/#key-classes-and-functions","title":"Key Classes and Functions","text":"Component Purpose Documentation <code>FastAPI App</code> Main service application View Details \u2192 <code>GraphState</code> Workflow state definition View Details \u2192 <code>build_graph</code> Workflow construction View Details \u2192 <code>run_graph</code> Workflow execution View Details \u2192"},{"location":"metamorphosis/agents/#workflow-architecture","title":"Workflow Architecture","text":""},{"location":"metamorphosis/agents/#processing-flow","title":"Processing Flow","text":"sequenceDiagram     participant Client     participant FastAPI     participant LangGraph     participant CopyEditor     participant Summarizer     participant WordCloud      Client-&gt;&gt;FastAPI: POST /stream     FastAPI-&gt;&gt;FastAPI: Generate thread_id     FastAPI-&gt;&gt;LangGraph: run_graph(text, thread_id)      LangGraph-&gt;&gt;CopyEditor: Process text     CopyEditor-&gt;&gt;LangGraph: Edited text      par Parallel Processing         LangGraph-&gt;&gt;Summarizer: Generate summary         Summarizer-&gt;&gt;LangGraph: Summary result     and         LangGraph-&gt;&gt;WordCloud: Create visualization         WordCloud-&gt;&gt;LangGraph: Image path     end      LangGraph-&gt;&gt;FastAPI: Stream events     FastAPI-&gt;&gt;Client: SSE events"},{"location":"metamorphosis/agents/#state-transitions","title":"State Transitions","text":"stateDiagram-v2     [*] --&gt; Initial     Initial --&gt; CopyEditing: Start workflow     CopyEditing --&gt; Processing: Text edited     Processing --&gt; Summarizing: Parallel branch 1     Processing --&gt; WordCloudGen: Parallel branch 2     Summarizing --&gt; Complete: Summary done     WordCloudGen --&gt; Complete: Word cloud done     Complete --&gt; [*]: All tasks finished"},{"location":"metamorphosis/agents/#api-reference","title":"API Reference","text":""},{"location":"metamorphosis/agents/#request-models","title":"Request Models","text":""},{"location":"metamorphosis/agents/#invokerequest","title":"InvokeRequest","text":"<pre><code>class InvokeRequest(BaseModel):\n    review_text: str = Field(..., min_length=1)\n    thread_id: Optional[str] = Field(None, description=\"Optional thread ID\")\n</code></pre>"},{"location":"metamorphosis/agents/#streamrequest","title":"StreamRequest","text":"<pre><code>class StreamRequest(BaseModel):\n    review_text: str = Field(..., min_length=1)\n    thread_id: Optional[str] = Field(None, description=\"Optional thread ID\")\n    mode: str = Field(\"values\", description=\"Streaming mode\")\n</code></pre>"},{"location":"metamorphosis/agents/#response-models","title":"Response Models","text":""},{"location":"metamorphosis/agents/#invokeresponse","title":"InvokeResponse","text":"<pre><code>class InvokeResponse(BaseModel):\n    thread_id: str\n    copy_edited_text: str | None\n    summary: str | None\n    word_cloud_path: str | None\n</code></pre>"},{"location":"metamorphosis/agents/#usage-examples","title":"Usage Examples","text":""},{"location":"metamorphosis/agents/#synchronous-processing","title":"Synchronous Processing","text":"<pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:8001/invoke\",\n    json={\n        \"review_text\": \"I worked on improving system performance this quarter...\",\n        \"thread_id\": \"user-123\"\n    }\n)\n\nresult = response.json()\nprint(f\"Edited text: {result['copy_edited_text']}\")\nprint(f\"Summary: {result['summary']}\")\nprint(f\"Word cloud: {result['word_cloud_path']}\")\n</code></pre>"},{"location":"metamorphosis/agents/#streaming-processing","title":"Streaming Processing","text":"<pre><code>import requests\nimport json\n\ndef stream_processing(review_text: str):\n    response = requests.post(\n        \"http://localhost:8001/stream\",\n        json={\n            \"review_text\": review_text,\n            \"mode\": \"values\"\n        },\n        stream=True\n    )\n\n    for line in response.iter_lines():\n        if line.startswith(b\"data:\"):\n            data = json.loads(line[5:])  # Remove \"data:\" prefix\n            print(f\"Event: {data}\")\n\n            # Process real-time updates\n            if \"copy_edited_text\" in data:\n                print(f\"Copy editing complete: {data['copy_edited_text'][:100]}...\")\n            if \"summary\" in data:\n                print(f\"Summary ready: {data['summary']}\")\n</code></pre>"},{"location":"metamorphosis/agents/#python-client-integration","title":"Python Client Integration","text":"<pre><code>from metamorphosis.agents.self_reviewer import run_graph, build_graph\n\nasync def process_review(text: str, thread_id: str = \"default\"):\n    graph = await build_graph()\n\n    async for event in run_graph(\n        graph=graph,\n        review_text=text,\n        thread_id=thread_id\n    ):\n        print(f\"Processing event: {event}\")\n\n        # Handle different event types\n        if \"copy_edited_text\" in event:\n            # Copy editing completed\n            pass\n        elif \"summary\" in event:\n            # Summarization completed\n            pass\n        elif \"word_cloud_path\" in event:\n            # Visualization completed\n            pass\n</code></pre>"},{"location":"metamorphosis/agents/#configuration","title":"Configuration","text":""},{"location":"metamorphosis/agents/#fastapi-service-configuration","title":"FastAPI Service Configuration","text":"<pre><code>app = FastAPI(\n    title=\"LangGraph for FastAPI Service\",\n    description=\"A FastAPI service for processing self-review texts\",\n    version=\"1.0.0\",\n    docs_url=\"/docs\",\n    redoc_url=\"/redoc\",\n)\n</code></pre>"},{"location":"metamorphosis/agents/#cors-configuration","title":"CORS Configuration","text":"<pre><code>app.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Configure for production\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n</code></pre>"},{"location":"metamorphosis/agents/#environment-variables","title":"Environment Variables","text":"<pre><code># Service configuration\nFASTAPI_HOST=0.0.0.0\nFASTAPI_PORT=8001\n\n# OpenAI configuration\nOPENAI_API_KEY=sk-...\n\n# Project configuration\nPROJECT_ROOT_DIR=/path/to/project\n</code></pre>"},{"location":"metamorphosis/agents/#advanced-features","title":"Advanced Features","text":""},{"location":"metamorphosis/agents/#thread-based-state-persistence","title":"Thread-based State Persistence","text":"<p>The service maintains conversation state using unique thread IDs:</p> <pre><code>def _generate_thread_id(provided_thread_id: str | None) -&gt; str:\n    if provided_thread_id and provided_thread_id.strip():\n        return provided_thread_id.strip()\n    return str(uuid.uuid4())\n</code></pre>"},{"location":"metamorphosis/agents/#error-handling-strategy","title":"Error Handling Strategy","text":"<p>Comprehensive error handling with structured responses:</p> <pre><code>def _create_error_response(error_message: str, status_code: int = 500) -&gt; JSONResponse:\n    return JSONResponse(\n        status_code=status_code,\n        content={\n            \"error\": error_message,\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"status\": \"error\"\n        }\n    )\n</code></pre>"},{"location":"metamorphosis/agents/#streaming-event-generation","title":"Streaming Event Generation","text":"<p>Real-time event streaming using Server-Sent Events:</p> <pre><code>async def _generate_stream_events(\n    review_text: str, \n    thread_id: str, \n    mode: str, \n    request: Request\n) -&gt; AsyncIterator[bytes]:\n    try:\n        graph = await build_graph()\n        async for event in run_graph(\n            graph=graph,\n            review_text=review_text,\n            thread_id=thread_id\n        ):\n            # Format and yield SSE event\n            yield f\"data: {json.dumps(event, default=str)}\\n\\n\".encode(\"utf-8\")\n    except Exception as e:\n        error_data = {\"error\": str(e), \"status\": \"error\"}\n        yield f\"data: {json.dumps(error_data)}\\n\\n\".encode(\"utf-8\")\n</code></pre>"},{"location":"metamorphosis/agents/#performance-optimization","title":"Performance Optimization","text":""},{"location":"metamorphosis/agents/#parallel-processing","title":"Parallel Processing","text":"<p>The workflow implements parallel execution for independent tasks:</p> <pre><code># Copy editing happens first (required for other tasks)\ncopy_edited_text = await copy_editor_node(state)\n\n# Summarization and word cloud generation run in parallel\nawait asyncio.gather(\n    summarizer_node(state),\n    wordcloud_node(state)\n)\n</code></pre>"},{"location":"metamorphosis/agents/#resource-management","title":"Resource Management","text":"<ul> <li>Connection Pooling: Efficient HTTP connection reuse</li> <li>Memory Management: Proper cleanup of large processing results</li> <li>Token Optimization: Efficient prompt design and caching</li> </ul>"},{"location":"metamorphosis/agents/#monitoring-and-observability","title":"Monitoring and Observability","text":"<ul> <li>Structured Logging: Comprehensive operation tracking</li> <li>Error Context: Rich error information for debugging</li> <li>Performance Metrics: Processing time and resource usage tracking</li> </ul>"},{"location":"metamorphosis/agents/#testing","title":"Testing","text":""},{"location":"metamorphosis/agents/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom metamorphosis.agents.self_reviewer import build_graph, run_graph\n\n@pytest.mark.asyncio\nasync def test_workflow_construction():\n    graph = await build_graph()\n    assert graph is not None\n    assert hasattr(graph, 'nodes')\n\n@pytest.mark.asyncio\nasync def test_workflow_execution():\n    graph = await build_graph()\n    events = []\n\n    async for event in run_graph(\n        graph=graph,\n        review_text=\"Test review text\",\n        thread_id=\"test-123\"\n    ):\n        events.append(event)\n\n    assert len(events) &gt; 0\n    assert any(\"copy_edited_text\" in event for event in events)\n</code></pre>"},{"location":"metamorphosis/agents/#integration-tests","title":"Integration Tests","text":"<pre><code>import requests\nimport pytest\n\ndef test_invoke_endpoint():\n    response = requests.post(\n        \"http://localhost:8001/invoke\",\n        json={\n            \"review_text\": \"Test review for processing\",\n            \"thread_id\": \"test-thread\"\n        }\n    )\n\n    assert response.status_code == 200\n    data = response.json()\n    assert \"copy_edited_text\" in data\n    assert \"summary\" in data\n    assert \"word_cloud_path\" in data\n\ndef test_streaming_endpoint():\n    response = requests.post(\n        \"http://localhost:8001/stream\",\n        json={\n            \"review_text\": \"Test streaming review\",\n            \"mode\": \"values\"\n        },\n        stream=True\n    )\n\n    assert response.status_code == 200\n    assert response.headers[\"content-type\"] == \"text/plain\"\n</code></pre>"},{"location":"metamorphosis/agents/#deployment","title":"Deployment","text":""},{"location":"metamorphosis/agents/#docker-deployment","title":"Docker Deployment","text":"<pre><code>FROM python:3.12-slim\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY src/ ./src/\nCOPY config.yaml .\nCOPY prompts/ ./prompts/\n\nCMD [\"python\", \"-m\", \"metamorphosis.agents.agent_service\"]\n</code></pre>"},{"location":"metamorphosis/agents/#production-configuration","title":"Production Configuration","text":"<pre><code># Use production-ready settings\napp = FastAPI(\n    title=\"Metamorphosis Agent Service\",\n    docs_url=None,  # Disable in production\n    redoc_url=None,  # Disable in production\n)\n\n# Configure CORS for specific origins\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"https://your-domain.com\"],\n    allow_credentials=True,\n    allow_methods=[\"GET\", \"POST\"],\n    allow_headers=[\"*\"],\n)\n</code></pre>"},{"location":"metamorphosis/agents/#health-monitoring","title":"Health Monitoring","text":"<pre><code>@app.get(\"/health\")\nasync def health_check():\n    return {\n        \"status\": \"healthy\",\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"version\": \"1.0.0\"\n    }\n</code></pre>"},{"location":"metamorphosis/agents/#troubleshooting","title":"Troubleshooting","text":""},{"location":"metamorphosis/agents/#common-issues","title":"Common Issues","text":"<ol> <li>Service Startup Errors:</li> <li>Check OpenAI API key configuration</li> <li>Verify all dependencies are installed</li> <li> <p>Ensure port availability</p> </li> <li> <p>Workflow Execution Failures:</p> </li> <li>Monitor LLM API rate limits</li> <li>Check prompt template availability</li> <li> <p>Verify MCP tools server connectivity</p> </li> <li> <p>Streaming Issues:</p> </li> <li>Ensure proper SSE client implementation</li> <li>Check network connectivity and timeouts</li> <li>Monitor memory usage for large texts</li> </ol>"},{"location":"metamorphosis/agents/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Run service with debug mode\nif __name__ == \"__main__\":\n    uvicorn.run(\n        \"metamorphosis.agents.agent_service:app\",\n        host=\"0.0.0.0\",\n        port=8001,\n        log_level=\"debug\"\n    )\n</code></pre>"},{"location":"metamorphosis/agents/#see-also","title":"See Also","text":"<ul> <li>MCP Package - Text processing tools integration</li> <li>UI Package - Streamlit user interface</li> <li>Data Models - Request/response schemas</li> <li>Examples - Usage examples and tutorials</li> </ul> <p>This documentation is automatically generated from the source code and maintained in sync with the codebase.</p>"},{"location":"metamorphosis/mcp/","title":"MCP (Model Context Protocol) Package","text":"<p>The <code>metamorphosis.mcp</code> package provides Model Context Protocol integration for text processing capabilities. It implements a standardized interface for LLM-backed text utilities with type-safe structured outputs and comprehensive error handling.</p>"},{"location":"metamorphosis/mcp/#package-overview","title":"Package Overview","text":"classDiagram     class TextModifiers {         +summarizer_llm: ChatOpenAI         +copy_editor_llm: ChatOpenAI         +key_achievements_llm: ChatOpenAI         +review_text_evaluator_llm: ChatOpenAI         +__init__() None         +summarize(text: str, max_words: int) SummarizedText         +rationalize_text(text: str) CopyEditedText         +extract_achievements(text: str) AchievementsList         +evaluate_review_text(text: str) ReviewScorecard         +get_model_info(method: str) dict         +_log_model_details_table(method: str) None     }      class ToolsServer {         +copy_edit(text: str) CopyEditedText         +create_word_cloud(text: str) str         +abstractive_summarize(text: str, max_words: int) SummarizedText         +_get_modifiers() TextModifiers     }      class SummarizedText {         +summarized_text: str         +size: int         +unit: str     }      class CopyEditedText {         +copy_edited_text: str         +size: int         +is_edited: bool     }      class AchievementsList {         +items: List[Achievement]         +size: int         +unit: str     }      class ReviewScorecard {         +metrics: List[MetricScore]         +overall: int         +verdict: Verdict         +notes: List[str]         +radar_labels: List[str]         +radar_values: List[int]     }      TextModifiers --&gt; SummarizedText : creates     TextModifiers --&gt; CopyEditedText : creates     TextModifiers --&gt; AchievementsList : creates     TextModifiers --&gt; ReviewScorecard : creates     ToolsServer --&gt; TextModifiers : uses     ToolsServer ..&gt; SummarizedText : returns     ToolsServer ..&gt; CopyEditedText : returns"},{"location":"metamorphosis/mcp/#core-components","title":"Core Components","text":""},{"location":"metamorphosis/mcp/#textmodifiers-class","title":"TextModifiers Class","text":"<p>The <code>TextModifiers</code> class is the heart of the MCP package, providing four main text processing capabilities:</p>"},{"location":"metamorphosis/mcp/#1-text-summarization","title":"1. Text Summarization","text":"<ul> <li>Purpose: Generate abstractive summaries of employee review text</li> <li>Model: Configurable LLM (default: gpt-4o)</li> <li>Features: Configurable word limits, structured output, token estimation</li> </ul>"},{"location":"metamorphosis/mcp/#2-text-rationalization-copy-editing","title":"2. Text Rationalization (Copy Editing)","text":"<ul> <li>Purpose: Improve grammar, spelling, and professional tone</li> <li>Features: Preserves meaning and structure, tracks modifications</li> <li>Guarantees: No content addition/removal, maintains author voice</li> </ul>"},{"location":"metamorphosis/mcp/#3-achievement-extraction","title":"3. Achievement Extraction","text":"<ul> <li>Purpose: Extract and structure key accomplishments from reviews</li> <li>Output: Up to 10 ranked achievements with metadata</li> <li>Metadata: Impact areas, metrics, timeframes, collaborators</li> </ul>"},{"location":"metamorphosis/mcp/#4-review-quality-evaluation","title":"4. Review Quality Evaluation","text":"<ul> <li>Purpose: Assess writing quality across six dimensions</li> <li>Scoring: 0-100 scale with weighted overall score</li> <li>Output: Detailed metrics, suggestions, radar chart data</li> </ul>"},{"location":"metamorphosis/mcp/#mcp-tools-server","title":"MCP Tools Server","text":"<p>The <code>tools_server.py</code> module implements MCP-compliant tool functions that can be called by external agents and workflows:</p> <ul> <li>Standardized Interface: MCP protocol compliance</li> <li>Error Handling: Comprehensive exception management</li> <li>Caching: LRU-cached TextModifiers instances</li> <li>Logging: Detailed operation tracking</li> </ul>"},{"location":"metamorphosis/mcp/#module-documentation","title":"Module Documentation","text":""},{"location":"metamorphosis/mcp/#core-modules","title":"Core Modules","text":"Module Description Key Components <code>text_modifiers.py</code> Main text processing class <code>TextModifiers</code> <code>tools_server.py</code> MCP server implementation Tool functions <code>__init__.py</code> Package initialization Exports"},{"location":"metamorphosis/mcp/#key-classes","title":"Key Classes","text":"Class Purpose Documentation <code>TextModifiers</code> Core text processing utilities View Details \u2192"},{"location":"metamorphosis/mcp/#tool-functions","title":"Tool Functions","text":"Function Purpose Documentation <code>copy_edit</code> MCP tool for text rationalization View Details \u2192 <code>abstractive_summarize</code> MCP tool for summarization View Details \u2192 <code>create_word_cloud</code> MCP tool for visualization View Details \u2192"},{"location":"metamorphosis/mcp/#processing-pipeline","title":"Processing Pipeline","text":""},{"location":"metamorphosis/mcp/#text-processing-flow","title":"Text Processing Flow","text":"sequenceDiagram     participant Client     participant ToolsServer     participant TextModifiers     participant LLM     participant ModelRegistry      Client-&gt;&gt;ToolsServer: copy_edit(text)     ToolsServer-&gt;&gt;ToolsServer: _get_modifiers()     ToolsServer-&gt;&gt;TextModifiers: rationalize_text(text)     TextModifiers-&gt;&gt;ModelRegistry: get copy_editor_llm     ModelRegistry-&gt;&gt;TextModifiers: ChatOpenAI instance     TextModifiers-&gt;&gt;TextModifiers: _log_model_details_table()     TextModifiers-&gt;&gt;LLM: invoke with prompt     LLM-&gt;&gt;TextModifiers: structured response     TextModifiers-&gt;&gt;TextModifiers: validate output     TextModifiers-&gt;&gt;ToolsServer: CopyEditedText     ToolsServer-&gt;&gt;Client: validated result"},{"location":"metamorphosis/mcp/#error-handling-flow","title":"Error Handling Flow","text":"sequenceDiagram     participant Client     participant ToolsServer     participant TextModifiers     participant Exception      Client-&gt;&gt;ToolsServer: tool_function(invalid_input)     ToolsServer-&gt;&gt;TextModifiers: process(invalid_input)     TextModifiers-&gt;&gt;Exception: ValidationError     Exception-&gt;&gt;TextModifiers: error context     TextModifiers-&gt;&gt;ToolsServer: PostconditionError     ToolsServer-&gt;&gt;ToolsServer: log error     ToolsServer-&gt;&gt;Client: structured error response"},{"location":"metamorphosis/mcp/#configuration","title":"Configuration","text":""},{"location":"metamorphosis/mcp/#model-configuration","title":"Model Configuration","text":"<p>Each processing method uses a dedicated LLM configuration:</p> <pre><code>models:\n  summarizer:\n    model: \"gpt-4o\"\n    temperature: 0.0\n    max_tokens: 2000\n    timeout: 120\n\n  copy_editor:\n    model: \"gpt-4o\"\n    temperature: 0.0\n    max_tokens: 4000\n    timeout: 180\n\n  key_achievements:\n    model: \"gpt-4o\"  # Non-reasoning model for structured output\n    temperature: 0.0\n    max_tokens: 4000\n    timeout: 300\n\n  review_text_evaluator:\n    model: \"gpt-5\"  # Reasoning model for complex evaluation\n    temperature: 0.0\n    max_tokens: 20000\n    timeout: 300\n</code></pre>"},{"location":"metamorphosis/mcp/#prompt-templates","title":"Prompt Templates","text":"<p>The system uses external prompt templates for maintainability:</p> <pre><code>prompts/\n\u251c\u2500\u2500 summarizer_system_prompt.md\n\u251c\u2500\u2500 summarizer_user_prompt.md\n\u251c\u2500\u2500 text_rationalization_system_prompt.md\n\u251c\u2500\u2500 text_rationalization_user_prompt.md\n\u251c\u2500\u2500 key_achievements_system_prompt.md\n\u251c\u2500\u2500 key_achievements_user_prompt.md\n\u251c\u2500\u2500 text_evaluator_system_prompt.md\n\u2514\u2500\u2500 text_evaluator_user_prompt.md\n</code></pre>"},{"location":"metamorphosis/mcp/#usage-examples","title":"Usage Examples","text":""},{"location":"metamorphosis/mcp/#basic-text-processing","title":"Basic Text Processing","text":"<pre><code>from metamorphosis.mcp.text_modifiers import TextModifiers\n\n# Initialize the processor\nmodifier = TextModifiers()\n\n# Summarize text\nsummary = modifier.summarize(\n    text=\"Long employee review text...\",\n    max_words=100\n)\nprint(f\"Summary: {summary.summarized_text}\")\nprint(f\"Size: {summary.size} {summary.unit}\")\n\n# Copy edit text\nedited = modifier.rationalize_text(\n    text=\"Text with grammar errors and typos...\"\n)\nprint(f\"Edited: {edited.copy_edited_text}\")\nprint(f\"Was modified: {edited.is_edited}\")\n</code></pre>"},{"location":"metamorphosis/mcp/#achievement-extraction","title":"Achievement Extraction","text":"<pre><code># Extract achievements\nachievements = modifier.extract_achievements(\n    text=\"I led the migration project that reduced latency by 50%...\"\n)\n\nfor achievement in achievements.items:\n    print(f\"Title: {achievement.title}\")\n    print(f\"Impact: {achievement.impact_area}\")\n    print(f\"Metrics: {achievement.metric_strings}\")\n    print(f\"Timeframe: {achievement.timeframe}\")\n    print(\"---\")\n</code></pre>"},{"location":"metamorphosis/mcp/#quality-evaluation","title":"Quality Evaluation","text":"<pre><code># Evaluate review quality\nscorecard = modifier.evaluate_review_text(\n    text=\"Employee review text to evaluate...\"\n)\n\nprint(f\"Overall Score: {scorecard.overall}/100\")\nprint(f\"Verdict: {scorecard.verdict}\")\n\nfor metric in scorecard.metrics:\n    print(f\"{metric.name}: {metric.score}/100\")\n    print(f\"Rationale: {metric.rationale}\")\n    print(f\"Suggestion: {metric.suggestion}\")\n    print(\"---\")\n</code></pre>"},{"location":"metamorphosis/mcp/#mcp-tool-usage","title":"MCP Tool Usage","text":"<pre><code>from metamorphosis.mcp.tools_server import copy_edit, abstractive_summarize\n\n# Use MCP tools directly\nedited_result = copy_edit(\"Text to edit...\")\nsummary_result = abstractive_summarize(\"Text to summarize...\", max_words=50)\n</code></pre>"},{"location":"metamorphosis/mcp/#advanced-features","title":"Advanced Features","text":""},{"location":"metamorphosis/mcp/#model-information-introspection","title":"Model Information Introspection","text":"<pre><code># Get model configuration for debugging\nmodel_info = modifier.get_model_info(\"summarize\")\nprint(f\"Model: {model_info['model']}\")\nprint(f\"Temperature: {model_info['temperature']}\")\nprint(f\"Max Tokens: {model_info['max_tokens']}\")\n</code></pre>"},{"location":"metamorphosis/mcp/#custom-prompt-templates","title":"Custom Prompt Templates","text":"<pre><code># The system automatically loads prompts from the prompts/ directory\n# Modify the .md files to customize behavior\n</code></pre>"},{"location":"metamorphosis/mcp/#logging-and-debugging","title":"Logging and Debugging","text":"<pre><code>import logging\nfrom loguru import logger\n\n# Enable detailed logging\nlogger.add(\"mcp_debug.log\", level=\"DEBUG\")\n\n# The system will log:\n# - Model configurations\n# - Processing steps\n# - Token usage\n# - Error details\n</code></pre>"},{"location":"metamorphosis/mcp/#performance-optimization","title":"Performance Optimization","text":""},{"location":"metamorphosis/mcp/#caching-strategy","title":"Caching Strategy","text":"<ul> <li>LLM Client Caching: Singleton model registry prevents duplicate initialization</li> <li>Prompt Template Caching: Templates loaded once at startup</li> <li>LRU Cache: Tools server uses <code>@lru_cache</code> for TextModifiers instances</li> </ul>"},{"location":"metamorphosis/mcp/#token-management","title":"Token Management","text":"<ul> <li>Efficient Prompting: Optimized prompt templates minimize token usage</li> <li>Structured Output: Direct JSON generation reduces parsing overhead</li> <li>Batch Processing: Process multiple items efficiently</li> </ul>"},{"location":"metamorphosis/mcp/#error-recovery","title":"Error Recovery","text":"<ul> <li>Graceful Degradation: Fallback strategies for API failures</li> <li>Retry Logic: Automatic retry for transient errors</li> <li>Circuit Breaker: Prevent cascade failures</li> </ul>"},{"location":"metamorphosis/mcp/#testing","title":"Testing","text":""},{"location":"metamorphosis/mcp/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom metamorphosis.mcp.text_modifiers import TextModifiers\nfrom metamorphosis.datamodel import SummarizedText\n\ndef test_summarization():\n    modifier = TextModifiers()\n    result = modifier.summarize(\n        text=\"Long text to summarize...\",\n        max_words=50\n    )\n    assert isinstance(result, SummarizedText)\n    assert len(result.summarized_text) &gt; 0\n    assert result.size &gt; 0\n\ndef test_copy_editing():\n    modifier = TextModifiers()\n    result = modifier.rationalize_text(\n        text=\"Text with errers and typos.\"\n    )\n    assert isinstance(result, CopyEditedText)\n    assert \"errors\" in result.copy_edited_text.lower()\n</code></pre>"},{"location":"metamorphosis/mcp/#integration-tests","title":"Integration Tests","text":"<pre><code>def test_mcp_tools_integration():\n    from metamorphosis.mcp.tools_server import copy_edit\n\n    result = copy_edit(\"Test text with errors.\")\n    assert result.copy_edited_text\n    assert isinstance(result.is_edited, bool)\n</code></pre>"},{"location":"metamorphosis/mcp/#troubleshooting","title":"Troubleshooting","text":""},{"location":"metamorphosis/mcp/#common-issues","title":"Common Issues","text":"<ol> <li>LLM API Errors:</li> <li>Check OpenAI API key validity</li> <li>Monitor rate limits and quotas</li> <li> <p>Verify network connectivity</p> </li> <li> <p>Prompt Template Issues:</p> </li> <li>Ensure all prompt files exist in <code>prompts/</code> directory</li> <li>Check file encoding (UTF-8)</li> <li> <p>Validate Jinja2 template syntax</p> </li> <li> <p>Memory Issues:</p> </li> <li>Monitor token usage with large texts</li> <li>Implement text chunking for very long inputs</li> <li>Use streaming for real-time processing</li> </ol>"},{"location":"metamorphosis/mcp/#debug-mode","title":"Debug Mode","text":"<pre><code># Enable comprehensive debugging\nfrom metamorphosis.mcp.text_modifiers import TextModifiers\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\nmodifier = TextModifiers()\n\n# All operations will now log detailed information\n</code></pre>"},{"location":"metamorphosis/mcp/#migration-guide","title":"Migration Guide","text":""},{"location":"metamorphosis/mcp/#from-legacy-versions","title":"From Legacy Versions","text":"<ul> <li>Update import statements to use new module structure</li> <li>Replace manual LLM initialization with ModelRegistry</li> <li>Update exception handling for new hierarchy</li> <li>Migrate to Pydantic v2 models</li> </ul>"},{"location":"metamorphosis/mcp/#see-also","title":"See Also","text":"<ul> <li>Agents Package - LangGraph workflow integration</li> <li>Data Models - Pydantic schema definitions</li> <li>Examples - Usage examples and tutorials</li> <li>Model Registry - LLM client management</li> </ul> <p>This documentation is automatically generated from the source code and maintained in sync with the codebase.</p>"},{"location":"metamorphosis/mcp/TextModifiers/","title":"TextModifiers Class","text":"<p>LLM-backed text processing utilities with structured outputs.</p> <p>This class provides text processing capabilities including summarization and copy editing using OpenAI's language models. All methods return structured Pydantic models for type safety and validation.</p> <p>The class uses prompt templates loaded from external files and leverages LangChain's structured output capabilities for reliable parsing.</p> Source code in <code>src/metamorphosis/mcp/text_modifiers.py</code> <pre><code>class TextModifiers:\n    \"\"\"LLM-backed text processing utilities with structured outputs.\n\n    This class provides text processing capabilities including summarization\n    and copy editing using OpenAI's language models. All methods return\n    structured Pydantic models for type safety and validation.\n\n    The class uses prompt templates loaded from external files and leverages\n    LangChain's structured output capabilities for reliable parsing.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the TextModifiers with LLM chains and prompt templates.\n\n        Raises:\n            ConfigurationError: If required environment variables are missing.\n            FileOperationError: If prompt files cannot be loaded.\n        \"\"\"\n        logger.debug(\"Initializing TextModifiers\")\n\n        # Acquire LLM clients from the central registry\n        registry = get_model_registry()\n        self.summarizer_llm = registry.summarizer_llm\n        self.copy_editor_llm = registry.copy_editor_llm\n        self.key_achievements_llm = registry.key_achievements_llm\n        self.review_text_evaluator_llm = registry.review_text_evaluator_llm\n\n        # Load prompt templates from files using utility functions\n        project_root = get_project_root()\n        prompts_dir = project_root / \"prompts\"\n        logger.debug(\"Using prompts directory: {}\", prompts_dir)\n\n        # Load as-is; no VOICE placeholder expected in the template\n        self.summarizer_system_prompt = read_text_file(prompts_dir / \"summarizer_system_prompt.md\")\n        self.summarizer_user_prompt = read_text_file(prompts_dir / \"summarizer_user_prompt.md\")\n\n        self.key_achievements_system_prompt = read_text_file(\n            prompts_dir / \"key_achievements_system_prompt.md\"\n        )\n        self.key_achievements_user_prompt = read_text_file(\n            prompts_dir / \"key_achievements_user_prompt.md\"\n        )\n\n        self.text_rationalization_system_prompt = read_text_file(\n            prompts_dir / \"text_rationalization_system_prompt.md\"\n        )\n        self.text_rationalization_user_prompt = read_text_file(\n            prompts_dir / \"text_rationalization_user_prompt.md\"\n        )\n\n        self.text_evaluator_system_prompt = read_text_file(\n            prompts_dir / \"text_evaluator_system_prompt.md\"\n        )\n        self.text_evaluator_user_prompt = read_text_file(\n            prompts_dir / \"text_evaluator_user_prompt.md\"\n        )\n\n        # Compose prompts with input placeholders for clarity.\n\n        logger.debug(\"TextModifiers initialized successfully\")\n\n    @validate_call\n    def summarize(\n        self,\n        *,\n        text: Annotated[str, Field(min_length=1)],\n        max_words: Annotated[int, Field(gt=0)] = 300,\n    ) -&gt; SummarizedText:\n        \"\"\"Summarize text using the configured LLM chain.\n\n        Args:\n            text: The input text to summarize.\n            max_words: Maximum number of words in the summary.\n\n        Returns:\n            SummarizedText: Structured summary with the summarized text.\n\n        Raises:\n            PostconditionError: If the output validation fails.\n        \"\"\"\n        logger.debug(\"summarize: processing text (length={}, max_words={})\", len(text), max_words)\n        # Log the model details as a simple table for traceability and debugging.\n        self._log_model_details_table(\"summarize\")\n\n        messages = [\n            (\"system\", self.summarizer_system_prompt),\n            (\"user\", self.summarizer_user_prompt.format(review=text)),\n        ]\n        prompt = ChatPromptTemplate.from_messages(messages)\n        logger.debug(\"summarizer_llm: {}\", self.summarizer_llm)\n        summarizer = prompt | self.summarizer_llm.with_structured_output(SummarizedText)\n\n        try:\n            result = summarizer.invoke({})\n        except Exception as e:\n            logger.error(\"summarize: LLM invocation failed - {}\", str(e))\n            raise PostconditionError(\n                \"Summarization LLM invocation failed\", operation=\"summarize_llm_invocation\"\n            ) from e\n\n        # Postcondition (O(1)): ensure structured output is valid\n        if not isinstance(result, SummarizedText) or not result.summarized_text:\n            raise_postcondition_error(\n                \"Summarization output validation failed\",\n                context={\n                    \"result_type\": type(result).__name__,\n                    \"has_text\": bool(getattr(result, \"summarized_text\", None)),\n                },\n                operation=\"summarize_validation\",\n            )\n\n        logger.debug(\n            \"summarize: completed successfully (output_length={})\", len(result.summarized_text)\n        )\n        return result\n\n    @validate_call\n    def rationalize_text(self, *, text: Annotated[str, Field(min_length=1)]) -&gt; CopyEditedText:\n        \"\"\"Rationalize text by correcting grammar, spelling, and formatting errors.\n\n        This method performs text rationalization using an LLM-based copy editor that makes\n        minor, localized corrections to improve the professional quality of text while\n        preserving the original meaning, structure, and content. The rationalization process\n        focuses on fixing:\n\n        - Spelling errors and typos (e.g., \"teh\" \u2192 \"the\", \"recieved\" \u2192 \"received\")\n        - Grammar issues (subject-verb agreement, tense consistency, articles)\n        - Punctuation normalization (quotes, commas, dashes, parentheses)\n        - Capitalization consistency (proper nouns, product names, teams)\n        - Whitespace and formatting standardization\n        - Casual shorthand replacement with formal equivalents (e.g., \"w/\" \u2192 \"with\")\n\n        The method is specifically designed for employee self-reviews and similar business\n        documents that may contain informal language, typos, or inconsistent formatting\n        from being pasted from drafts or messaging platforms.\n\n        **Preservation Guarantees:**\n        - Original paragraph structure and ordering are maintained exactly\n        - Numerical values and units remain identical (only formatting may be normalized)\n        - Bullet points, headings, and section order are preserved\n        - Author's voice and intent (first/third person) are retained\n        - No content is added, removed, or significantly rewritten\n\n        **Style Transformations:**\n        - Converts informal shorthand to professional language when unambiguous\n        - Normalizes product/tool names for consistency\n        - Standardizes punctuation and spacing around units and symbols\n        - Removes casual interjections while preserving meaning\n        - Corrects double negatives in reduction statements (e.g., \"reduced by -38%\" \u2192 \"reduced by 38%\")  # noqa: E501\n\n        Args:\n            text: The input text to rationalize. Must be non-empty string containing\n                the content to be copy-edited. Typically employee self-review text\n                or similar business documents that need professional polish.\n\n        Returns:\n            CopyEditedText: A structured response containing:\n                - copy_edited_text: The rationalized text with corrections applied\n                - size: Estimated token count of the rationalized text\n                - is_edited: Boolean indicating whether any changes were made\n\n        Raises:\n            ValidationError: If the input text is empty or invalid.\n            PostconditionError: If the LLM output validation fails or the structured\n                response cannot be parsed correctly.\n            ConfigurationError: If the copy editor LLM is not properly configured.\n\n        Example:\n            &gt;&gt;&gt; modifier = TextModifiers()\n            &gt;&gt;&gt; result = modifier.rationalize_text(\n            ...     text=\"I migrated teh system w/ better performance. \"\n            ...          \"Latency dropped by -38% after optimizations.\"\n            ... )\n            &gt;&gt;&gt; print(result.copy_edited_text)\n            \"I migrated the system with better performance. \"\n            \"Latency dropped by 38% after optimizations.\"\n            &gt;&gt;&gt; print(result.is_edited)\n            True\n\n        Note:\n            This method uses the text_rationalization_system_prompt.md and\n            text_rationalization_user_prompt.md templates to guide the LLM's\n            behavior. The rationalization is performed by the copy_editor_llm\n            configured in the model registry.\n        \"\"\"\n        logger.debug(\"rationalize_text: processing text (length={})\", len(text))\n        # Log the model details as a simple table for traceability and debugging.\n        self._log_model_details_table(\"rationalize_text\")\n\n        # Construct the prompt using the text rationalization templates\n        messages = [\n            (\"system\", self.text_rationalization_system_prompt),\n            (\"user\", self.text_rationalization_user_prompt.format(EMPLOYEE_REVIEW_TEXT=text)),\n        ]\n        prompt = ChatPromptTemplate.from_messages(messages)\n\n        # Create the rationalization chain with structured output\n        rationalizer = prompt | self.copy_editor_llm.with_structured_output(CopyEditedText)\n\n        # Invoke the chain with exception handling\n        try:\n            result = rationalizer.invoke({})\n        except Exception as e:\n            logger.error(\"rationalize_text: LLM invocation failed - {}\", str(e))\n            raise PostconditionError(\n                \"Text rationalization LLM invocation failed\",\n                operation=\"rationalize_text_llm_invocation\",\n            ) from e\n\n        # Postcondition (O(1)): ensure structured output is valid\n        if not isinstance(result, CopyEditedText) or not result.copy_edited_text:\n            raise_postcondition_error(\n                \"Text rationalization output validation failed\",\n                context={\n                    \"result_type\": type(result).__name__,\n                    \"has_text\": bool(getattr(result, \"copy_edited_text\", None)),\n                },\n                operation=\"rationalize_text_validation\",\n            )\n\n        logger.debug(\n            \"rationalize_text: completed successfully (output_length={}, edited={})\",\n            len(result.copy_edited_text),\n            result.is_edited,\n        )\n        return result\n\n    # =========================================================================\n\n    @validate_call\n    def extract_achievements(\n        self, *, text: Annotated[str, Field(min_length=1)]\n    ) -&gt; AchievementsList:\n        \"\"\"Extract key achievements from employee self-review text.\n\n        This method analyzes employee self-review text and extracts up to 10 key achievements\n        with structured metadata including impact areas, metrics, timeframes, and\n        collaboration details. The extraction focuses on identifying concrete outcomes\n        and business impact rather than activities or tasks.\n\n        The method uses an LLM-based achievement extractor that follows strict guidelines\n        to identify and rank achievements by:\n        - Business/customer impact (revenue, cost, risk, user outcomes)\n        - Reliability/quality/security improvements (SLO/MTTR/incidents/defects)\n        - Breadth of ownership (Cross-team/Org-wide &gt; TechLead &gt; IC)\n        - Adoption/usage and external validation\n        - Recency (tie-breaker)\n\n        **Achievement Structure:**\n        Each extracted achievement includes:\n        - **title**: Concise, outcome-oriented label (\u226412 words)\n        - **outcome**: Impact/result description (\u226440 words)\n        - **impact_area**: Categorized impact type (reliability, performance, security, etc.)\n        - **metric_strings**: Verbatim numbers/units from the review text\n        - **timeframe**: Explicit time period if stated (e.g., \"Q2 2025\", \"H1\")\n        - **ownership_scope**: Leadership level if explicit (IC, TechLead, Manager, etc.)\n        - **collaborators**: Named people/teams if mentioned\n\n        **Quality Guarantees:**\n        - No invented metrics, dates, or collaborators - only explicit information\n        - Numbers and units copied exactly as they appear in the review\n        - Achievements are deduplicated and ranked by impact\n        - Maximum of 10 achievements returned, fewer if insufficient quality achievements exist\n\n        Args:\n            text: The employee self-review text to analyze. Must be non-empty string\n                containing the review content from which to extract achievements.\n                Typically contains mixed content (tasks, outcomes, anecdotes) that\n                needs to be parsed for concrete accomplishments.\n\n        Returns:\n            AchievementsList: A structured response containing:\n                - items: List of up to 10 Achievement objects ranked by impact\n                - size: Token estimate of concatenated titles and outcomes\n                - unit: Always \"tokens\"\n\n        Raises:\n            ValidationError: If the input text is empty or invalid.\n            PostconditionError: If the LLM output validation fails or the structured\n                response cannot be parsed correctly.\n            ConfigurationError: If the key achievements LLM is not properly configured.\n\n        Example:\n            &gt;&gt;&gt; modifier = TextModifiers()\n            &gt;&gt;&gt; result = modifier.extract_achievements(\n            ...     text=\"I reduced checkout p95 latency from 480ms to 190ms in H1 2025 \"\n            ...          \"by redesigning the caching layer with the Payments and SRE teams. \"\n            ...          \"This improved conversion rates during peak traffic periods.\"\n            ... )\n            &gt;&gt;&gt; print(result.items[0].title)\n            \"Cut checkout p95 latency\"\n            &gt;&gt;&gt; print(result.items[0].impact_area)\n            \"performance\"\n            &gt;&gt;&gt; print(result.items[0].metric_strings)\n            [\"480ms\", \"190ms\"]\n\n        Note:\n            This method uses the key_achievements_system_prompt.md and\n            key_achievements_user_prompt.md templates to guide the LLM's behavior.\n            The extraction is performed by the key_achievements_llm configured\n            in the model registry.\n        \"\"\"\n        logger.debug(\"extract_achievements: processing text (length={})\", len(text))\n        # Log the model details as a simple table for traceability and debugging.\n        self._log_model_details_table(\"extract_achievements\")\n\n        # Construct the prompt using the key achievements templates\n        messages = [\n            (\"system\", self.key_achievements_system_prompt),\n            (\"user\", self.key_achievements_user_prompt.format(EMPLOYEE_REVIEW_TEXT=text)),\n        ]\n        prompt = ChatPromptTemplate.from_messages(messages)\n\n        # Create the achievement extraction chain with structured output\n        extractor = prompt | self.key_achievements_llm.with_structured_output(AchievementsList)\n\n        # Invoke the chain with exception handling\n        try:\n            result = extractor.invoke({})\n        except Exception as e:\n            logger.error(\"extract_achievements: LLM invocation failed - {}\", str(e))\n            raise PostconditionError(\n                \"Achievement extraction LLM invocation failed\",\n                operation=\"extract_achievements_llm_invocation\",\n            ) from e\n\n        # Postcondition (O(1)): ensure structured output is valid\n        if not isinstance(result, AchievementsList):\n            raise_postcondition_error(\n                \"Achievement extraction output validation failed\",\n                context={\n                    \"result_type\": type(result).__name__,\n                    \"has_items\": bool(getattr(result, \"items\", None)),\n                },\n                operation=\"extract_achievements_validation\",\n            )\n\n        logger.debug(\n            \"extract_achievements: completed successfully (items_count={}, total_size={})\",\n            len(result.items),\n            result.size,\n        )\n        return result\n\n    # =========================================================================\n\n    @validate_call\n    def evaluate_review_text(self, *, text: Annotated[str, Field(min_length=1)]) -&gt; ReviewScorecard:\n        \"\"\"Evaluate the writing quality of employee self-review text.\n\n        This method analyzes employee self-review text and provides a comprehensive\n        assessment of writing quality across six key dimensions. The evaluation\n        focuses on the quality of the writing itself, not job performance, to help\n        HR partners and engineering leaders quickly assess review quality.\n\n        The method uses an LLM-based evaluator that scores the review on:\n        - **OutcomeOverActivity** (25%): Emphasis on concrete outcomes vs. task lists\n        - **QuantitativeSpecificity** (25%): Use of metrics, numbers, and baselines\n        - **ClarityCoherence** (15%): Logical flow and readability\n        - **Conciseness** (15%): Efficient, non-redundant expression\n        - **OwnershipLeadership** (10%): Clear ownership and leadership signals\n        - **Collaboration** (10%): Evidence of cross-team work and partnerships\n\n        **Scoring System:**\n        Each dimension is scored 0-100 using anchor levels (20/40/60/80/95) with\n        specific rubrics. The overall score is a weighted average that determines\n        the verdict: excellent (\u226585), strong (70-84), mixed (50-69), weak (&lt;50).\n\n        **Output Structure:**\n        - Individual scores and rationales for each dimension\n        - Specific, actionable improvement suggestions\n        - Weighted overall score and verdict classification\n        - Optional flags for common issues (e.g., no_numbers_detected, short_review)\n        - Radar chart data for visualization\n\n        Args:\n            text: The employee self-review text to evaluate. Must be non-empty string\n                containing the review content to be assessed for writing quality.\n                Typically contains mixed content about achievements, projects, and\n                activities that needs quality assessment.\n\n        Returns:\n            ReviewScorecard: A structured assessment containing:\n                - metrics: List of 6 MetricScore objects with scores, rationales, suggestions\n                - overall: Weighted average score (0-100)\n                - verdict: Quality classification (excellent/strong/mixed/weak)\n                - notes: Optional flags for specific issues detected\n                - radar_labels: Metric names for chart visualization\n                - radar_values: Corresponding scores for chart visualization\n\n        Raises:\n            ValidationError: If the input text is empty or invalid.\n            PostconditionError: If the LLM output validation fails or the structured\n                response cannot be parsed correctly.\n            ConfigurationError: If the review text evaluator LLM is not properly configured.\n\n        Example:\n            &gt;&gt;&gt; modifier = TextModifiers()\n            &gt;&gt;&gt; result = modifier.evaluate_review_text(\n            ...     text=\"I reduced latency from 480ms to 190ms by optimizing the cache. \"\n            ...          \"This improved user experience and reduced server costs by 15%.\"\n            ... )\n            &gt;&gt;&gt; print(result.overall)\n            75\n            &gt;&gt;&gt; print(result.verdict)\n            \"strong\"\n            &gt;&gt;&gt; print(result.metrics[0].name)\n            \"OutcomeOverActivity\"\n\n        Note:\n            This method uses the text_evaluator_system_prompt.md and\n            text_evaluator_user_prompt.md templates to guide the LLM's behavior.\n            The evaluation is performed by the review_text_evaluator_llm configured\n            in the model registry.\n        \"\"\"\n        logger.debug(\"evaluate_review_text: processing text (length={})\", len(text))\n        # Log the model details as a simple table for traceability and debugging.\n        self._log_model_details_table(\"evaluate_review_text\")\n\n        # Construct the prompt using the text evaluator templates\n        messages = [\n            (\"system\", self.text_evaluator_system_prompt),\n            (\"user\", self.text_evaluator_user_prompt.format(EMPLOYEE_REVIEW_TEXT=text)),\n        ]\n        prompt = ChatPromptTemplate.from_messages(messages)\n\n        # Create the evaluation chain with structured output\n        evaluator = prompt | self.review_text_evaluator_llm.with_structured_output(ReviewScorecard)\n\n        # Invoke the chain with exception handling\n        try:\n            result = evaluator.invoke({})\n        except Exception as e:\n            logger.error(\"evaluate_review_text: LLM invocation failed - {}\", str(e))\n            raise PostconditionError(\n                \"Review text evaluation LLM invocation failed\",\n                operation=\"evaluate_review_text_llm_invocation\",\n            ) from e\n\n        # Postcondition (O(1)): ensure structured output is valid\n        if (\n            not isinstance(result, ReviewScorecard)\n            or not result.metrics\n            or len(result.metrics) != 6\n        ):\n            raise_postcondition_error(\n                \"Review text evaluation output validation failed\",\n                context={\n                    \"result_type\": type(result).__name__,\n                    \"has_metrics\": bool(getattr(result, \"metrics\", None)),\n                    \"metrics_count\": len(getattr(result, \"metrics\", [])),\n                },\n                operation=\"evaluate_review_text_validation\",\n            )\n\n        logger.debug(\n            \"evaluate_review_text: completed successfully (overall_score={}, verdict={})\",\n            result.overall,\n            result.verdict,\n        )\n        return result\n\n    # =========================================================================\n\n    def get_model_info(self, method: str) -&gt; dict[str, Any] | None:\n        \"\"\"Get model configuration information for a specific method.\n\n        Args:\n            method: The method name (e.g., \"summarize\", \"rationalize_text\", \n                \"extract_achievements\", \"evaluate_review_text\").\n\n        Returns:\n            Dictionary containing model configuration or None if not found.\n        \"\"\"\n        method_to_llm = {\n            \"summarize\": self.summarizer_llm,\n            \"rationalize_text\": self.copy_editor_llm,\n            \"extract_achievements\": self.key_achievements_llm,\n            \"evaluate_review_text\": self.review_text_evaluator_llm,\n        }\n\n        llm = method_to_llm.get(method)\n        if not llm:\n            return None\n\n        # Extract configuration from the LLM instance\n        model_info = {\n            \"model\": getattr(llm, \"model_name\", \"N/A\"),\n            \"temperature\": getattr(llm, \"temperature\", \"N/A\"),\n            \"max_tokens\": getattr(llm, \"max_tokens\", \"N/A\"),\n            \"timeout\": getattr(llm, \"request_timeout\", \"N/A\"),\n        }\n\n        # Add optional parameters if they exist\n        for attr in (\"top_p\", \"frequency_penalty\", \"presence_penalty\"):\n            value = getattr(llm, attr, None)\n            if value is not None:\n                model_info[attr] = value\n\n        return model_info\n\n    def _log_model_details_table(self, method: str) -&gt; None:\n        \"\"\"Log the LLM model details as a table for the given TextModifiers method.\n\n        Args:\n            method: The name of the method (e.g., \"summarize\", \"rationalize_text\").\n        \"\"\"\n        # Defensive: ensure the model config is present and has expected keys\n        model_info = self.get_model_info(method)\n        if not model_info:\n            logger.warning(\"No model info found for method '{}'\", method)\n            return\n\n        # Prepare table rows\n        rows = [\n            (\"Model\", model_info.get(\"model\", \"N/A\")),\n            (\"Temperature\", model_info.get(\"temperature\", \"N/A\")),\n            (\"Max Tokens\", model_info.get(\"max_tokens\", \"N/A\")),\n            (\"Timeout\", model_info.get(\"timeout\", \"N/A\")),\n        ]\n        # Optional fields\n        for key in (\"top_p\", \"frequency_penalty\", \"presence_penalty\"):\n            if key in model_info:\n                rows.append((key.replace(\"_\", \" \").title(), model_info[key]))\n\n        # Format as table\n        col_width = max(len(str(k)) for k, _ in rows) + 2\n        table_lines = [\n            f\"{'Parameter'.ljust(col_width)}| Value\",\n            f\"{'-' * (col_width)}|{'-' * 20}\",\n        ]\n        for k, v in rows:\n            table_lines.append(f\"{str(k).ljust(col_width)}| {v}\")\n\n        logger.info(\"LLM Model Details for '{}':\\n{}\", method, \"\\n\".join(table_lines))\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#metamorphosis.mcp.text_modifiers.TextModifiers.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the TextModifiers with LLM chains and prompt templates.</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If required environment variables are missing.</p> <code>FileOperationError</code> <p>If prompt files cannot be loaded.</p> Source code in <code>src/metamorphosis/mcp/text_modifiers.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the TextModifiers with LLM chains and prompt templates.\n\n    Raises:\n        ConfigurationError: If required environment variables are missing.\n        FileOperationError: If prompt files cannot be loaded.\n    \"\"\"\n    logger.debug(\"Initializing TextModifiers\")\n\n    # Acquire LLM clients from the central registry\n    registry = get_model_registry()\n    self.summarizer_llm = registry.summarizer_llm\n    self.copy_editor_llm = registry.copy_editor_llm\n    self.key_achievements_llm = registry.key_achievements_llm\n    self.review_text_evaluator_llm = registry.review_text_evaluator_llm\n\n    # Load prompt templates from files using utility functions\n    project_root = get_project_root()\n    prompts_dir = project_root / \"prompts\"\n    logger.debug(\"Using prompts directory: {}\", prompts_dir)\n\n    # Load as-is; no VOICE placeholder expected in the template\n    self.summarizer_system_prompt = read_text_file(prompts_dir / \"summarizer_system_prompt.md\")\n    self.summarizer_user_prompt = read_text_file(prompts_dir / \"summarizer_user_prompt.md\")\n\n    self.key_achievements_system_prompt = read_text_file(\n        prompts_dir / \"key_achievements_system_prompt.md\"\n    )\n    self.key_achievements_user_prompt = read_text_file(\n        prompts_dir / \"key_achievements_user_prompt.md\"\n    )\n\n    self.text_rationalization_system_prompt = read_text_file(\n        prompts_dir / \"text_rationalization_system_prompt.md\"\n    )\n    self.text_rationalization_user_prompt = read_text_file(\n        prompts_dir / \"text_rationalization_user_prompt.md\"\n    )\n\n    self.text_evaluator_system_prompt = read_text_file(\n        prompts_dir / \"text_evaluator_system_prompt.md\"\n    )\n    self.text_evaluator_user_prompt = read_text_file(\n        prompts_dir / \"text_evaluator_user_prompt.md\"\n    )\n\n    # Compose prompts with input placeholders for clarity.\n\n    logger.debug(\"TextModifiers initialized successfully\")\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#metamorphosis.mcp.text_modifiers.TextModifiers.evaluate_review_text","title":"<code>evaluate_review_text(*, text)</code>","text":"<p>Evaluate the writing quality of employee self-review text.</p> <p>This method analyzes employee self-review text and provides a comprehensive assessment of writing quality across six key dimensions. The evaluation focuses on the quality of the writing itself, not job performance, to help HR partners and engineering leaders quickly assess review quality.</p> <p>The method uses an LLM-based evaluator that scores the review on: - OutcomeOverActivity (25%): Emphasis on concrete outcomes vs. task lists - QuantitativeSpecificity (25%): Use of metrics, numbers, and baselines - ClarityCoherence (15%): Logical flow and readability - Conciseness (15%): Efficient, non-redundant expression - OwnershipLeadership (10%): Clear ownership and leadership signals - Collaboration (10%): Evidence of cross-team work and partnerships</p> <p>Scoring System: Each dimension is scored 0-100 using anchor levels (20/40/60/80/95) with specific rubrics. The overall score is a weighted average that determines the verdict: excellent (\u226585), strong (70-84), mixed (50-69), weak (&lt;50).</p> <p>Output Structure: - Individual scores and rationales for each dimension - Specific, actionable improvement suggestions - Weighted overall score and verdict classification - Optional flags for common issues (e.g., no_numbers_detected, short_review) - Radar chart data for visualization</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Annotated[str, Field(min_length=1)]</code> <p>The employee self-review text to evaluate. Must be non-empty string containing the review content to be assessed for writing quality. Typically contains mixed content about achievements, projects, and activities that needs quality assessment.</p> required <p>Returns:</p> Name Type Description <code>ReviewScorecard</code> <code>ReviewScorecard</code> <p>A structured assessment containing: - metrics: List of 6 MetricScore objects with scores, rationales, suggestions - overall: Weighted average score (0-100) - verdict: Quality classification (excellent/strong/mixed/weak) - notes: Optional flags for specific issues detected - radar_labels: Metric names for chart visualization - radar_values: Corresponding scores for chart visualization</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the input text is empty or invalid.</p> <code>PostconditionError</code> <p>If the LLM output validation fails or the structured response cannot be parsed correctly.</p> <code>ConfigurationError</code> <p>If the review text evaluator LLM is not properly configured.</p> Example <p>modifier = TextModifiers() result = modifier.evaluate_review_text( ...     text=\"I reduced latency from 480ms to 190ms by optimizing the cache. \" ...          \"This improved user experience and reduced server costs by 15%.\" ... ) print(result.overall) 75 print(result.verdict) \"strong\" print(result.metrics[0].name) \"OutcomeOverActivity\"</p> Note <p>This method uses the text_evaluator_system_prompt.md and text_evaluator_user_prompt.md templates to guide the LLM's behavior. The evaluation is performed by the review_text_evaluator_llm configured in the model registry.</p> Source code in <code>src/metamorphosis/mcp/text_modifiers.py</code> <pre><code>@validate_call\ndef evaluate_review_text(self, *, text: Annotated[str, Field(min_length=1)]) -&gt; ReviewScorecard:\n    \"\"\"Evaluate the writing quality of employee self-review text.\n\n    This method analyzes employee self-review text and provides a comprehensive\n    assessment of writing quality across six key dimensions. The evaluation\n    focuses on the quality of the writing itself, not job performance, to help\n    HR partners and engineering leaders quickly assess review quality.\n\n    The method uses an LLM-based evaluator that scores the review on:\n    - **OutcomeOverActivity** (25%): Emphasis on concrete outcomes vs. task lists\n    - **QuantitativeSpecificity** (25%): Use of metrics, numbers, and baselines\n    - **ClarityCoherence** (15%): Logical flow and readability\n    - **Conciseness** (15%): Efficient, non-redundant expression\n    - **OwnershipLeadership** (10%): Clear ownership and leadership signals\n    - **Collaboration** (10%): Evidence of cross-team work and partnerships\n\n    **Scoring System:**\n    Each dimension is scored 0-100 using anchor levels (20/40/60/80/95) with\n    specific rubrics. The overall score is a weighted average that determines\n    the verdict: excellent (\u226585), strong (70-84), mixed (50-69), weak (&lt;50).\n\n    **Output Structure:**\n    - Individual scores and rationales for each dimension\n    - Specific, actionable improvement suggestions\n    - Weighted overall score and verdict classification\n    - Optional flags for common issues (e.g., no_numbers_detected, short_review)\n    - Radar chart data for visualization\n\n    Args:\n        text: The employee self-review text to evaluate. Must be non-empty string\n            containing the review content to be assessed for writing quality.\n            Typically contains mixed content about achievements, projects, and\n            activities that needs quality assessment.\n\n    Returns:\n        ReviewScorecard: A structured assessment containing:\n            - metrics: List of 6 MetricScore objects with scores, rationales, suggestions\n            - overall: Weighted average score (0-100)\n            - verdict: Quality classification (excellent/strong/mixed/weak)\n            - notes: Optional flags for specific issues detected\n            - radar_labels: Metric names for chart visualization\n            - radar_values: Corresponding scores for chart visualization\n\n    Raises:\n        ValidationError: If the input text is empty or invalid.\n        PostconditionError: If the LLM output validation fails or the structured\n            response cannot be parsed correctly.\n        ConfigurationError: If the review text evaluator LLM is not properly configured.\n\n    Example:\n        &gt;&gt;&gt; modifier = TextModifiers()\n        &gt;&gt;&gt; result = modifier.evaluate_review_text(\n        ...     text=\"I reduced latency from 480ms to 190ms by optimizing the cache. \"\n        ...          \"This improved user experience and reduced server costs by 15%.\"\n        ... )\n        &gt;&gt;&gt; print(result.overall)\n        75\n        &gt;&gt;&gt; print(result.verdict)\n        \"strong\"\n        &gt;&gt;&gt; print(result.metrics[0].name)\n        \"OutcomeOverActivity\"\n\n    Note:\n        This method uses the text_evaluator_system_prompt.md and\n        text_evaluator_user_prompt.md templates to guide the LLM's behavior.\n        The evaluation is performed by the review_text_evaluator_llm configured\n        in the model registry.\n    \"\"\"\n    logger.debug(\"evaluate_review_text: processing text (length={})\", len(text))\n    # Log the model details as a simple table for traceability and debugging.\n    self._log_model_details_table(\"evaluate_review_text\")\n\n    # Construct the prompt using the text evaluator templates\n    messages = [\n        (\"system\", self.text_evaluator_system_prompt),\n        (\"user\", self.text_evaluator_user_prompt.format(EMPLOYEE_REVIEW_TEXT=text)),\n    ]\n    prompt = ChatPromptTemplate.from_messages(messages)\n\n    # Create the evaluation chain with structured output\n    evaluator = prompt | self.review_text_evaluator_llm.with_structured_output(ReviewScorecard)\n\n    # Invoke the chain with exception handling\n    try:\n        result = evaluator.invoke({})\n    except Exception as e:\n        logger.error(\"evaluate_review_text: LLM invocation failed - {}\", str(e))\n        raise PostconditionError(\n            \"Review text evaluation LLM invocation failed\",\n            operation=\"evaluate_review_text_llm_invocation\",\n        ) from e\n\n    # Postcondition (O(1)): ensure structured output is valid\n    if (\n        not isinstance(result, ReviewScorecard)\n        or not result.metrics\n        or len(result.metrics) != 6\n    ):\n        raise_postcondition_error(\n            \"Review text evaluation output validation failed\",\n            context={\n                \"result_type\": type(result).__name__,\n                \"has_metrics\": bool(getattr(result, \"metrics\", None)),\n                \"metrics_count\": len(getattr(result, \"metrics\", [])),\n            },\n            operation=\"evaluate_review_text_validation\",\n        )\n\n    logger.debug(\n        \"evaluate_review_text: completed successfully (overall_score={}, verdict={})\",\n        result.overall,\n        result.verdict,\n    )\n    return result\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#metamorphosis.mcp.text_modifiers.TextModifiers.extract_achievements","title":"<code>extract_achievements(*, text)</code>","text":"<p>Extract key achievements from employee self-review text.</p> <p>This method analyzes employee self-review text and extracts up to 10 key achievements with structured metadata including impact areas, metrics, timeframes, and collaboration details. The extraction focuses on identifying concrete outcomes and business impact rather than activities or tasks.</p> <p>The method uses an LLM-based achievement extractor that follows strict guidelines to identify and rank achievements by: - Business/customer impact (revenue, cost, risk, user outcomes) - Reliability/quality/security improvements (SLO/MTTR/incidents/defects) - Breadth of ownership (Cross-team/Org-wide &gt; TechLead &gt; IC) - Adoption/usage and external validation - Recency (tie-breaker)</p> <p>Achievement Structure: Each extracted achievement includes: - title: Concise, outcome-oriented label (\u226412 words) - outcome: Impact/result description (\u226440 words) - impact_area: Categorized impact type (reliability, performance, security, etc.) - metric_strings: Verbatim numbers/units from the review text - timeframe: Explicit time period if stated (e.g., \"Q2 2025\", \"H1\") - ownership_scope: Leadership level if explicit (IC, TechLead, Manager, etc.) - collaborators: Named people/teams if mentioned</p> <p>Quality Guarantees: - No invented metrics, dates, or collaborators - only explicit information - Numbers and units copied exactly as they appear in the review - Achievements are deduplicated and ranked by impact - Maximum of 10 achievements returned, fewer if insufficient quality achievements exist</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Annotated[str, Field(min_length=1)]</code> <p>The employee self-review text to analyze. Must be non-empty string containing the review content from which to extract achievements. Typically contains mixed content (tasks, outcomes, anecdotes) that needs to be parsed for concrete accomplishments.</p> required <p>Returns:</p> Name Type Description <code>AchievementsList</code> <code>AchievementsList</code> <p>A structured response containing: - items: List of up to 10 Achievement objects ranked by impact - size: Token estimate of concatenated titles and outcomes - unit: Always \"tokens\"</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the input text is empty or invalid.</p> <code>PostconditionError</code> <p>If the LLM output validation fails or the structured response cannot be parsed correctly.</p> <code>ConfigurationError</code> <p>If the key achievements LLM is not properly configured.</p> Example <p>modifier = TextModifiers() result = modifier.extract_achievements( ...     text=\"I reduced checkout p95 latency from 480ms to 190ms in H1 2025 \" ...          \"by redesigning the caching layer with the Payments and SRE teams. \" ...          \"This improved conversion rates during peak traffic periods.\" ... ) print(result.items[0].title) \"Cut checkout p95 latency\" print(result.items[0].impact_area) \"performance\" print(result.items[0].metric_strings) [\"480ms\", \"190ms\"]</p> Note <p>This method uses the key_achievements_system_prompt.md and key_achievements_user_prompt.md templates to guide the LLM's behavior. The extraction is performed by the key_achievements_llm configured in the model registry.</p> Source code in <code>src/metamorphosis/mcp/text_modifiers.py</code> <pre><code>@validate_call\ndef extract_achievements(\n    self, *, text: Annotated[str, Field(min_length=1)]\n) -&gt; AchievementsList:\n    \"\"\"Extract key achievements from employee self-review text.\n\n    This method analyzes employee self-review text and extracts up to 10 key achievements\n    with structured metadata including impact areas, metrics, timeframes, and\n    collaboration details. The extraction focuses on identifying concrete outcomes\n    and business impact rather than activities or tasks.\n\n    The method uses an LLM-based achievement extractor that follows strict guidelines\n    to identify and rank achievements by:\n    - Business/customer impact (revenue, cost, risk, user outcomes)\n    - Reliability/quality/security improvements (SLO/MTTR/incidents/defects)\n    - Breadth of ownership (Cross-team/Org-wide &gt; TechLead &gt; IC)\n    - Adoption/usage and external validation\n    - Recency (tie-breaker)\n\n    **Achievement Structure:**\n    Each extracted achievement includes:\n    - **title**: Concise, outcome-oriented label (\u226412 words)\n    - **outcome**: Impact/result description (\u226440 words)\n    - **impact_area**: Categorized impact type (reliability, performance, security, etc.)\n    - **metric_strings**: Verbatim numbers/units from the review text\n    - **timeframe**: Explicit time period if stated (e.g., \"Q2 2025\", \"H1\")\n    - **ownership_scope**: Leadership level if explicit (IC, TechLead, Manager, etc.)\n    - **collaborators**: Named people/teams if mentioned\n\n    **Quality Guarantees:**\n    - No invented metrics, dates, or collaborators - only explicit information\n    - Numbers and units copied exactly as they appear in the review\n    - Achievements are deduplicated and ranked by impact\n    - Maximum of 10 achievements returned, fewer if insufficient quality achievements exist\n\n    Args:\n        text: The employee self-review text to analyze. Must be non-empty string\n            containing the review content from which to extract achievements.\n            Typically contains mixed content (tasks, outcomes, anecdotes) that\n            needs to be parsed for concrete accomplishments.\n\n    Returns:\n        AchievementsList: A structured response containing:\n            - items: List of up to 10 Achievement objects ranked by impact\n            - size: Token estimate of concatenated titles and outcomes\n            - unit: Always \"tokens\"\n\n    Raises:\n        ValidationError: If the input text is empty or invalid.\n        PostconditionError: If the LLM output validation fails or the structured\n            response cannot be parsed correctly.\n        ConfigurationError: If the key achievements LLM is not properly configured.\n\n    Example:\n        &gt;&gt;&gt; modifier = TextModifiers()\n        &gt;&gt;&gt; result = modifier.extract_achievements(\n        ...     text=\"I reduced checkout p95 latency from 480ms to 190ms in H1 2025 \"\n        ...          \"by redesigning the caching layer with the Payments and SRE teams. \"\n        ...          \"This improved conversion rates during peak traffic periods.\"\n        ... )\n        &gt;&gt;&gt; print(result.items[0].title)\n        \"Cut checkout p95 latency\"\n        &gt;&gt;&gt; print(result.items[0].impact_area)\n        \"performance\"\n        &gt;&gt;&gt; print(result.items[0].metric_strings)\n        [\"480ms\", \"190ms\"]\n\n    Note:\n        This method uses the key_achievements_system_prompt.md and\n        key_achievements_user_prompt.md templates to guide the LLM's behavior.\n        The extraction is performed by the key_achievements_llm configured\n        in the model registry.\n    \"\"\"\n    logger.debug(\"extract_achievements: processing text (length={})\", len(text))\n    # Log the model details as a simple table for traceability and debugging.\n    self._log_model_details_table(\"extract_achievements\")\n\n    # Construct the prompt using the key achievements templates\n    messages = [\n        (\"system\", self.key_achievements_system_prompt),\n        (\"user\", self.key_achievements_user_prompt.format(EMPLOYEE_REVIEW_TEXT=text)),\n    ]\n    prompt = ChatPromptTemplate.from_messages(messages)\n\n    # Create the achievement extraction chain with structured output\n    extractor = prompt | self.key_achievements_llm.with_structured_output(AchievementsList)\n\n    # Invoke the chain with exception handling\n    try:\n        result = extractor.invoke({})\n    except Exception as e:\n        logger.error(\"extract_achievements: LLM invocation failed - {}\", str(e))\n        raise PostconditionError(\n            \"Achievement extraction LLM invocation failed\",\n            operation=\"extract_achievements_llm_invocation\",\n        ) from e\n\n    # Postcondition (O(1)): ensure structured output is valid\n    if not isinstance(result, AchievementsList):\n        raise_postcondition_error(\n            \"Achievement extraction output validation failed\",\n            context={\n                \"result_type\": type(result).__name__,\n                \"has_items\": bool(getattr(result, \"items\", None)),\n            },\n            operation=\"extract_achievements_validation\",\n        )\n\n    logger.debug(\n        \"extract_achievements: completed successfully (items_count={}, total_size={})\",\n        len(result.items),\n        result.size,\n    )\n    return result\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#metamorphosis.mcp.text_modifiers.TextModifiers.get_model_info","title":"<code>get_model_info(method)</code>","text":"<p>Get model configuration information for a specific method.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method name (e.g., \"summarize\", \"rationalize_text\",  \"extract_achievements\", \"evaluate_review_text\").</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>Dictionary containing model configuration or None if not found.</p> Source code in <code>src/metamorphosis/mcp/text_modifiers.py</code> <pre><code>def get_model_info(self, method: str) -&gt; dict[str, Any] | None:\n    \"\"\"Get model configuration information for a specific method.\n\n    Args:\n        method: The method name (e.g., \"summarize\", \"rationalize_text\", \n            \"extract_achievements\", \"evaluate_review_text\").\n\n    Returns:\n        Dictionary containing model configuration or None if not found.\n    \"\"\"\n    method_to_llm = {\n        \"summarize\": self.summarizer_llm,\n        \"rationalize_text\": self.copy_editor_llm,\n        \"extract_achievements\": self.key_achievements_llm,\n        \"evaluate_review_text\": self.review_text_evaluator_llm,\n    }\n\n    llm = method_to_llm.get(method)\n    if not llm:\n        return None\n\n    # Extract configuration from the LLM instance\n    model_info = {\n        \"model\": getattr(llm, \"model_name\", \"N/A\"),\n        \"temperature\": getattr(llm, \"temperature\", \"N/A\"),\n        \"max_tokens\": getattr(llm, \"max_tokens\", \"N/A\"),\n        \"timeout\": getattr(llm, \"request_timeout\", \"N/A\"),\n    }\n\n    # Add optional parameters if they exist\n    for attr in (\"top_p\", \"frequency_penalty\", \"presence_penalty\"):\n        value = getattr(llm, attr, None)\n        if value is not None:\n            model_info[attr] = value\n\n    return model_info\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#metamorphosis.mcp.text_modifiers.TextModifiers.rationalize_text","title":"<code>rationalize_text(*, text)</code>","text":"<p>Rationalize text by correcting grammar, spelling, and formatting errors.</p> <p>This method performs text rationalization using an LLM-based copy editor that makes minor, localized corrections to improve the professional quality of text while preserving the original meaning, structure, and content. The rationalization process focuses on fixing:</p> <ul> <li>Spelling errors and typos (e.g., \"teh\" \u2192 \"the\", \"recieved\" \u2192 \"received\")</li> <li>Grammar issues (subject-verb agreement, tense consistency, articles)</li> <li>Punctuation normalization (quotes, commas, dashes, parentheses)</li> <li>Capitalization consistency (proper nouns, product names, teams)</li> <li>Whitespace and formatting standardization</li> <li>Casual shorthand replacement with formal equivalents (e.g., \"w/\" \u2192 \"with\")</li> </ul> <p>The method is specifically designed for employee self-reviews and similar business documents that may contain informal language, typos, or inconsistent formatting from being pasted from drafts or messaging platforms.</p> <p>Preservation Guarantees: - Original paragraph structure and ordering are maintained exactly - Numerical values and units remain identical (only formatting may be normalized) - Bullet points, headings, and section order are preserved - Author's voice and intent (first/third person) are retained - No content is added, removed, or significantly rewritten</p> <p>Style Transformations: - Converts informal shorthand to professional language when unambiguous - Normalizes product/tool names for consistency - Standardizes punctuation and spacing around units and symbols - Removes casual interjections while preserving meaning - Corrects double negatives in reduction statements (e.g., \"reduced by -38%\" \u2192 \"reduced by 38%\")  # noqa: E501</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Annotated[str, Field(min_length=1)]</code> <p>The input text to rationalize. Must be non-empty string containing the content to be copy-edited. Typically employee self-review text or similar business documents that need professional polish.</p> required <p>Returns:</p> Name Type Description <code>CopyEditedText</code> <code>CopyEditedText</code> <p>A structured response containing: - copy_edited_text: The rationalized text with corrections applied - size: Estimated token count of the rationalized text - is_edited: Boolean indicating whether any changes were made</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the input text is empty or invalid.</p> <code>PostconditionError</code> <p>If the LLM output validation fails or the structured response cannot be parsed correctly.</p> <code>ConfigurationError</code> <p>If the copy editor LLM is not properly configured.</p> Example <p>modifier = TextModifiers() result = modifier.rationalize_text( ...     text=\"I migrated teh system w/ better performance. \" ...          \"Latency dropped by -38% after optimizations.\" ... ) print(result.copy_edited_text) \"I migrated the system with better performance. \" \"Latency dropped by 38% after optimizations.\" print(result.is_edited) True</p> Note <p>This method uses the text_rationalization_system_prompt.md and text_rationalization_user_prompt.md templates to guide the LLM's behavior. The rationalization is performed by the copy_editor_llm configured in the model registry.</p> Source code in <code>src/metamorphosis/mcp/text_modifiers.py</code> <pre><code>@validate_call\ndef rationalize_text(self, *, text: Annotated[str, Field(min_length=1)]) -&gt; CopyEditedText:\n    \"\"\"Rationalize text by correcting grammar, spelling, and formatting errors.\n\n    This method performs text rationalization using an LLM-based copy editor that makes\n    minor, localized corrections to improve the professional quality of text while\n    preserving the original meaning, structure, and content. The rationalization process\n    focuses on fixing:\n\n    - Spelling errors and typos (e.g., \"teh\" \u2192 \"the\", \"recieved\" \u2192 \"received\")\n    - Grammar issues (subject-verb agreement, tense consistency, articles)\n    - Punctuation normalization (quotes, commas, dashes, parentheses)\n    - Capitalization consistency (proper nouns, product names, teams)\n    - Whitespace and formatting standardization\n    - Casual shorthand replacement with formal equivalents (e.g., \"w/\" \u2192 \"with\")\n\n    The method is specifically designed for employee self-reviews and similar business\n    documents that may contain informal language, typos, or inconsistent formatting\n    from being pasted from drafts or messaging platforms.\n\n    **Preservation Guarantees:**\n    - Original paragraph structure and ordering are maintained exactly\n    - Numerical values and units remain identical (only formatting may be normalized)\n    - Bullet points, headings, and section order are preserved\n    - Author's voice and intent (first/third person) are retained\n    - No content is added, removed, or significantly rewritten\n\n    **Style Transformations:**\n    - Converts informal shorthand to professional language when unambiguous\n    - Normalizes product/tool names for consistency\n    - Standardizes punctuation and spacing around units and symbols\n    - Removes casual interjections while preserving meaning\n    - Corrects double negatives in reduction statements (e.g., \"reduced by -38%\" \u2192 \"reduced by 38%\")  # noqa: E501\n\n    Args:\n        text: The input text to rationalize. Must be non-empty string containing\n            the content to be copy-edited. Typically employee self-review text\n            or similar business documents that need professional polish.\n\n    Returns:\n        CopyEditedText: A structured response containing:\n            - copy_edited_text: The rationalized text with corrections applied\n            - size: Estimated token count of the rationalized text\n            - is_edited: Boolean indicating whether any changes were made\n\n    Raises:\n        ValidationError: If the input text is empty or invalid.\n        PostconditionError: If the LLM output validation fails or the structured\n            response cannot be parsed correctly.\n        ConfigurationError: If the copy editor LLM is not properly configured.\n\n    Example:\n        &gt;&gt;&gt; modifier = TextModifiers()\n        &gt;&gt;&gt; result = modifier.rationalize_text(\n        ...     text=\"I migrated teh system w/ better performance. \"\n        ...          \"Latency dropped by -38% after optimizations.\"\n        ... )\n        &gt;&gt;&gt; print(result.copy_edited_text)\n        \"I migrated the system with better performance. \"\n        \"Latency dropped by 38% after optimizations.\"\n        &gt;&gt;&gt; print(result.is_edited)\n        True\n\n    Note:\n        This method uses the text_rationalization_system_prompt.md and\n        text_rationalization_user_prompt.md templates to guide the LLM's\n        behavior. The rationalization is performed by the copy_editor_llm\n        configured in the model registry.\n    \"\"\"\n    logger.debug(\"rationalize_text: processing text (length={})\", len(text))\n    # Log the model details as a simple table for traceability and debugging.\n    self._log_model_details_table(\"rationalize_text\")\n\n    # Construct the prompt using the text rationalization templates\n    messages = [\n        (\"system\", self.text_rationalization_system_prompt),\n        (\"user\", self.text_rationalization_user_prompt.format(EMPLOYEE_REVIEW_TEXT=text)),\n    ]\n    prompt = ChatPromptTemplate.from_messages(messages)\n\n    # Create the rationalization chain with structured output\n    rationalizer = prompt | self.copy_editor_llm.with_structured_output(CopyEditedText)\n\n    # Invoke the chain with exception handling\n    try:\n        result = rationalizer.invoke({})\n    except Exception as e:\n        logger.error(\"rationalize_text: LLM invocation failed - {}\", str(e))\n        raise PostconditionError(\n            \"Text rationalization LLM invocation failed\",\n            operation=\"rationalize_text_llm_invocation\",\n        ) from e\n\n    # Postcondition (O(1)): ensure structured output is valid\n    if not isinstance(result, CopyEditedText) or not result.copy_edited_text:\n        raise_postcondition_error(\n            \"Text rationalization output validation failed\",\n            context={\n                \"result_type\": type(result).__name__,\n                \"has_text\": bool(getattr(result, \"copy_edited_text\", None)),\n            },\n            operation=\"rationalize_text_validation\",\n        )\n\n    logger.debug(\n        \"rationalize_text: completed successfully (output_length={}, edited={})\",\n        len(result.copy_edited_text),\n        result.is_edited,\n    )\n    return result\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#metamorphosis.mcp.text_modifiers.TextModifiers.summarize","title":"<code>summarize(*, text, max_words=300)</code>","text":"<p>Summarize text using the configured LLM chain.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Annotated[str, Field(min_length=1)]</code> <p>The input text to summarize.</p> required <code>max_words</code> <code>Annotated[int, Field(gt=0)]</code> <p>Maximum number of words in the summary.</p> <code>300</code> <p>Returns:</p> Name Type Description <code>SummarizedText</code> <code>SummarizedText</code> <p>Structured summary with the summarized text.</p> <p>Raises:</p> Type Description <code>PostconditionError</code> <p>If the output validation fails.</p> Source code in <code>src/metamorphosis/mcp/text_modifiers.py</code> <pre><code>@validate_call\ndef summarize(\n    self,\n    *,\n    text: Annotated[str, Field(min_length=1)],\n    max_words: Annotated[int, Field(gt=0)] = 300,\n) -&gt; SummarizedText:\n    \"\"\"Summarize text using the configured LLM chain.\n\n    Args:\n        text: The input text to summarize.\n        max_words: Maximum number of words in the summary.\n\n    Returns:\n        SummarizedText: Structured summary with the summarized text.\n\n    Raises:\n        PostconditionError: If the output validation fails.\n    \"\"\"\n    logger.debug(\"summarize: processing text (length={}, max_words={})\", len(text), max_words)\n    # Log the model details as a simple table for traceability and debugging.\n    self._log_model_details_table(\"summarize\")\n\n    messages = [\n        (\"system\", self.summarizer_system_prompt),\n        (\"user\", self.summarizer_user_prompt.format(review=text)),\n    ]\n    prompt = ChatPromptTemplate.from_messages(messages)\n    logger.debug(\"summarizer_llm: {}\", self.summarizer_llm)\n    summarizer = prompt | self.summarizer_llm.with_structured_output(SummarizedText)\n\n    try:\n        result = summarizer.invoke({})\n    except Exception as e:\n        logger.error(\"summarize: LLM invocation failed - {}\", str(e))\n        raise PostconditionError(\n            \"Summarization LLM invocation failed\", operation=\"summarize_llm_invocation\"\n        ) from e\n\n    # Postcondition (O(1)): ensure structured output is valid\n    if not isinstance(result, SummarizedText) or not result.summarized_text:\n        raise_postcondition_error(\n            \"Summarization output validation failed\",\n            context={\n                \"result_type\": type(result).__name__,\n                \"has_text\": bool(getattr(result, \"summarized_text\", None)),\n            },\n            operation=\"summarize_validation\",\n        )\n\n    logger.debug(\n        \"summarize: completed successfully (output_length={})\", len(result.summarized_text)\n    )\n    return result\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#overview","title":"Overview","text":"<p>The <code>TextModifiers</code> class is the core component of the MCP package, providing comprehensive LLM-backed text processing utilities. It implements four main capabilities: summarization, copy editing (text rationalization), achievement extraction, and review quality evaluation.</p>"},{"location":"metamorphosis/mcp/TextModifiers/#class-architecture","title":"Class Architecture","text":"classDiagram     class TextModifiers {         -summarizer_llm: ChatOpenAI         -copy_editor_llm: ChatOpenAI         -key_achievements_llm: ChatOpenAI         -review_text_evaluator_llm: ChatOpenAI         -summarizer_system_prompt: str         -summarizer_user_prompt: str         -text_rationalization_system_prompt: str         -text_rationalization_user_prompt: str         -key_achievements_system_prompt: str         -key_achievements_user_prompt: str         -text_evaluator_system_prompt: str         -text_evaluator_user_prompt: str         +__init__()         +summarize(text, max_words) SummarizedText         +rationalize_text(text) CopyEditedText         +extract_achievements(text) AchievementsList         +evaluate_review_text(text) ReviewScorecard         +get_model_info(method) dict         -_log_model_details_table(method)     }      class ModelRegistry {         +summarizer_llm: ChatOpenAI         +copy_editor_llm: ChatOpenAI         +key_achievements_llm: ChatOpenAI         +review_text_evaluator_llm: ChatOpenAI     }      class SummarizedText {         +summarized_text: str         +size: int         +unit: str     }      class CopyEditedText {         +copy_edited_text: str         +size: int         +is_edited: bool     }      class AchievementsList {         +items: List[Achievement]         +size: int         +unit: str     }      class ReviewScorecard {         +metrics: List[MetricScore]         +overall: int         +verdict: Verdict         +notes: List[str]         +radar_labels: List[str]         +radar_values: List[int]     }      TextModifiers --&gt; ModelRegistry : uses     TextModifiers --&gt; SummarizedText : creates     TextModifiers --&gt; CopyEditedText : creates     TextModifiers --&gt; AchievementsList : creates     TextModifiers --&gt; ReviewScorecard : creates"},{"location":"metamorphosis/mcp/TextModifiers/#initialization","title":"Initialization","text":""},{"location":"metamorphosis/mcp/TextModifiers/#constructor","title":"Constructor","text":"<pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the TextModifiers with LLM chains and prompt templates.\"\"\"\n</code></pre> <p>The constructor performs several key initialization steps:</p> <ol> <li>LLM Client Acquisition: Retrieves configured LLM clients from the ModelRegistry</li> <li>Prompt Template Loading: Loads external prompt templates from the <code>prompts/</code> directory</li> <li>Validation: Ensures all required components are properly initialized</li> </ol>"},{"location":"metamorphosis/mcp/TextModifiers/#llm-clients","title":"LLM Clients","text":"<p>The class initializes four specialized LLM clients:</p> <ul> <li><code>summarizer_llm</code>: For text summarization (default: gpt-4o)</li> <li><code>copy_editor_llm</code>: For text rationalization (default: gpt-4o)</li> <li><code>key_achievements_llm</code>: For achievement extraction (default: gpt-4o)</li> <li><code>review_text_evaluator_llm</code>: For quality evaluation (default: gpt-5)</li> </ul>"},{"location":"metamorphosis/mcp/TextModifiers/#prompt-templates","title":"Prompt Templates","text":"<p>Prompt templates are loaded from external Markdown files for maintainability:</p> <pre><code># System and user prompts for each capability\nself.summarizer_system_prompt = read_text_file(prompts_dir / \"summarizer_system_prompt.md\")\nself.summarizer_user_prompt = read_text_file(prompts_dir / \"summarizer_user_prompt.md\")\n# ... (similar for other capabilities)\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#core-methods","title":"Core Methods","text":""},{"location":"metamorphosis/mcp/TextModifiers/#1-text-summarization","title":"1. Text Summarization","text":"<pre><code>@validate_call\ndef summarize(\n    self,\n    *,\n    text: Annotated[str, Field(min_length=1)],\n    max_words: Annotated[int, Field(gt=0)] = 300,\n) -&gt; SummarizedText:\n</code></pre> <p>Purpose: Generate abstractive summaries of employee review text.</p> <p>Features: - Configurable word limits (default: 300 words) - Preserves key information and context - Structured output with metadata - Token estimation for billing/monitoring</p> <p>Example Usage: <pre><code>modifier = TextModifiers()\nresult = modifier.summarize(\n    text=\"Long employee review discussing multiple projects and achievements...\",\n    max_words=100\n)\nprint(f\"Summary: {result.summarized_text}\")\nprint(f\"Token estimate: {result.size} {result.unit}\")\n</code></pre></p> <p>Processing Flow: 1. Input validation using Pydantic decorators 2. Model details logging for traceability 3. LLM invocation with structured output parsing 4. Post-condition validation 5. Result packaging in <code>SummarizedText</code> model</p>"},{"location":"metamorphosis/mcp/TextModifiers/#2-text-rationalization-copy-editing","title":"2. Text Rationalization (Copy Editing)","text":"<pre><code>@validate_call\ndef rationalize_text(self, *, text: Annotated[str, Field(min_length=1)]) -&gt; CopyEditedText:\n</code></pre> <p>Purpose: Improve grammar, spelling, and professional tone while preserving meaning.</p> <p>Key Features: - Grammar &amp; Spelling: Fixes errors and typos - Professional Tone: Converts informal language to professional style - Structure Preservation: Maintains original paragraph structure - Content Integrity: No addition or removal of factual content - Voice Preservation: Retains author's perspective (first/third person)</p> <p>Transformation Examples: - <code>\"teh system\"</code> \u2192 <code>\"the system\"</code> - <code>\"w/ better performance\"</code> \u2192 <code>\"with better performance\"</code> - <code>\"reduced by -38%\"</code> \u2192 <code>\"reduced by 38%\"</code></p> <p>Example Usage: <pre><code>result = modifier.rationalize_text(\n    text=\"I migrated teh system w/ better performance. Latency dropped by -38%.\"\n)\nprint(f\"Edited: {result.copy_edited_text}\")\nprint(f\"Was modified: {result.is_edited}\")\n</code></pre></p>"},{"location":"metamorphosis/mcp/TextModifiers/#3-achievement-extraction","title":"3. Achievement Extraction","text":"<pre><code>@validate_call\ndef extract_achievements(\n    self, *, text: Annotated[str, Field(min_length=1)]\n) -&gt; AchievementsList:\n</code></pre> <p>Purpose: Extract and structure key accomplishments from employee reviews.</p> <p>Key Features: - Ranking: Achievements ranked by business impact - Structured Metadata: Impact areas, metrics, timeframes, collaborators - Quality Filtering: Only defensible achievements included - Limit: Up to 10 achievements (configurable in prompt)</p> <p>Achievement Structure: <pre><code>class Achievement:\n    title: str  # Concise outcome-oriented label (\u226412 words)\n    outcome: str  # Impact/result description (\u226440 words)\n    impact_area: ImpactArea  # Categorized impact type\n    metric_strings: List[str]  # Verbatim numbers from text\n    timeframe: Optional[str]  # Explicit time period if stated\n    ownership_scope: Optional[OwnershipScope]  # Leadership level\n    collaborators: List[str]  # Named people/teams\n</code></pre></p> <p>Example Usage: <pre><code>result = modifier.extract_achievements(\n    text=\"I reduced checkout p95 latency from 480ms to 190ms in H1 2025 by redesigning the caching layer with the Payments and SRE teams.\"\n)\n\nfor achievement in result.items:\n    print(f\"Title: {achievement.title}\")\n    print(f\"Impact Area: {achievement.impact_area}\")\n    print(f\"Metrics: {achievement.metric_strings}\")\n    print(f\"Timeframe: {achievement.timeframe}\")\n    print(f\"Collaborators: {achievement.collaborators}\")\n</code></pre></p>"},{"location":"metamorphosis/mcp/TextModifiers/#4-review-quality-evaluation","title":"4. Review Quality Evaluation","text":"<pre><code>@validate_call\ndef evaluate_review_text(\n    self, *, text: Annotated[str, Field(min_length=1)]\n) -&gt; ReviewScorecard:\n</code></pre> <p>Purpose: Assess writing quality across six key dimensions.</p> <p>Evaluation Dimensions (with weights): 1. OutcomeOverActivity (25%): Emphasis on concrete outcomes vs. task lists 2. QuantitativeSpecificity (25%): Use of metrics, numbers, and baselines 3. ClarityCoherence (15%): Logical flow and readability 4. Conciseness (15%): Efficient, non-redundant expression 5. OwnershipLeadership (10%): Clear ownership and leadership signals 6. Collaboration (10%): Evidence of cross-team work</p> <p>Scoring System: - Each dimension scored 0-100 using anchor levels (20/40/60/80/95) - Overall score is weighted average - Verdict classification: excellent (\u226585), strong (70-84), mixed (50-69), weak (&lt;50)</p> <p>Example Usage: <pre><code>result = modifier.evaluate_review_text(\n    text=\"I reduced latency from 480ms to 190ms by optimizing the cache. This improved user experience and reduced server costs by 15%.\"\n)\n\nprint(f\"Overall Score: {result.overall}/100\")\nprint(f\"Verdict: {result.verdict}\")\n\nfor metric in result.metrics:\n    print(f\"{metric.name}: {metric.score}/100\")\n    print(f\"Rationale: {metric.rationale}\")\n    print(f\"Suggestion: {metric.suggestion}\")\n</code></pre></p>"},{"location":"metamorphosis/mcp/TextModifiers/#utility-methods","title":"Utility Methods","text":""},{"location":"metamorphosis/mcp/TextModifiers/#model-information-introspection","title":"Model Information Introspection","text":"<pre><code>def get_model_info(self, method: str) -&gt; dict[str, Any] | None:\n</code></pre> <p>Purpose: Retrieve model configuration for debugging and monitoring.</p> <p>Parameters: - <code>method</code>: The method name (\"summarize\", \"rationalize_text\", \"extract_achievements\", \"evaluate_review_text\")</p> <p>Returns: Dictionary containing model configuration or None if method not found.</p> <p>Example Usage: <pre><code>info = modifier.get_model_info(\"summarize\")\nprint(f\"Model: {info['model']}\")\nprint(f\"Temperature: {info['temperature']}\")\nprint(f\"Max Tokens: {info['max_tokens']}\")\nprint(f\"Timeout: {info['timeout']}\")\n</code></pre></p>"},{"location":"metamorphosis/mcp/TextModifiers/#model-details-logging","title":"Model Details Logging","text":"<pre><code>def _log_model_details_table(self, method: str) -&gt; None:\n</code></pre> <p>Purpose: Log LLM model details in a formatted table for traceability.</p> <p>This private method is automatically called by all processing methods to provide detailed logging of the model configuration being used. This is crucial for debugging and monitoring in production environments.</p> <p>Example Log Output: <pre><code>LLM Model Details for 'summarize':\nParameter       | Value\n----------------|--------------------\nModel           | gpt-4o\nTemperature     | 0.0\nMax Tokens      | 2000\nTimeout         | 120\n</code></pre></p>"},{"location":"metamorphosis/mcp/TextModifiers/#design-patterns","title":"Design Patterns","text":""},{"location":"metamorphosis/mcp/TextModifiers/#validation-first-design","title":"Validation-First Design","text":"<p>All public methods use Pydantic's <code>@validate_call</code> decorator for automatic input validation:</p> <pre><code>@validate_call\ndef summarize(\n    self,\n    *,  # Keyword-only arguments for clarity\n    text: Annotated[str, Field(min_length=1)],  # Must be non-empty\n    max_words: Annotated[int, Field(gt=0)] = 300,  # Must be positive\n) -&gt; SummarizedText:\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#fail-fast-error-handling","title":"Fail-Fast Error Handling","text":"<p>The class implements comprehensive error handling with specific exception types:</p> <pre><code>try:\n    result = summarizer.invoke({})\nexcept Exception as e:\n    logger.error(\"summarize: LLM invocation failed - {}\", str(e))\n    raise PostconditionError(\n        \"Summarization LLM invocation failed\",\n        operation=\"summarize_llm_invocation\"\n    ) from e\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#structured-output-pattern","title":"Structured Output Pattern","text":"<p>All methods return strongly-typed Pydantic models for type safety:</p> <pre><code># Instead of returning raw strings or dictionaries\nreturn SummarizedText(\n    summarized_text=processed_text,\n    size=token_estimate,\n    unit=\"tokens\"\n)\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#template-driven-prompting","title":"Template-Driven Prompting","text":"<p>Prompt templates are externalized for maintainability and version control:</p> <pre><code>messages = [\n    (\"system\", self.summarizer_system_prompt),\n    (\"user\", self.summarizer_user_prompt.format(review=text)),\n]\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#error-handling","title":"Error Handling","text":""},{"location":"metamorphosis/mcp/TextModifiers/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>The class raises specific exceptions for different error conditions:</p> <ul> <li><code>ValidationError</code>: Invalid input parameters</li> <li><code>PostconditionError</code>: LLM invocation or output validation failures</li> <li><code>ConfigurationError</code>: Missing or invalid configuration</li> <li><code>FileOperationError</code>: Prompt template loading issues</li> </ul>"},{"location":"metamorphosis/mcp/TextModifiers/#error-context","title":"Error Context","text":"<p>All exceptions include rich context for debugging:</p> <pre><code>raise_postcondition_error(\n    \"Summarization output validation failed\",\n    context={\n        \"result_type\": type(result).__name__,\n        \"has_text\": bool(getattr(result, 'summarized_text', None))\n    },\n    operation=\"summarize_validation\"\n)\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#performance-considerations","title":"Performance Considerations","text":""},{"location":"metamorphosis/mcp/TextModifiers/#llm-client-reuse","title":"LLM Client Reuse","text":"<p>LLM clients are initialized once and reused across all method calls, avoiding expensive re-initialization.</p>"},{"location":"metamorphosis/mcp/TextModifiers/#prompt-template-caching","title":"Prompt Template Caching","text":"<p>Prompt templates are loaded once during initialization and cached in memory.</p>"},{"location":"metamorphosis/mcp/TextModifiers/#token-optimization","title":"Token Optimization","text":"<ul> <li>Efficient prompt design minimizes token usage</li> <li>Structured output reduces parsing overhead</li> <li>Streaming support for real-time applications</li> </ul>"},{"location":"metamorphosis/mcp/TextModifiers/#memory-management","title":"Memory Management","text":"<ul> <li>Large text inputs are processed efficiently</li> <li>Results include size estimates for monitoring</li> <li>Proper cleanup of temporary objects</li> </ul>"},{"location":"metamorphosis/mcp/TextModifiers/#testing","title":"Testing","text":""},{"location":"metamorphosis/mcp/TextModifiers/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom metamorphosis.mcp.text_modifiers import TextModifiers\nfrom metamorphosis.datamodel import SummarizedText, CopyEditedText\n\nclass TestTextModifiers:\n    def setup_method(self):\n        self.modifier = TextModifiers()\n\n    def test_summarization(self):\n        result = self.modifier.summarize(\n            text=\"Long employee review text...\",\n            max_words=50\n        )\n        assert isinstance(result, SummarizedText)\n        assert len(result.summarized_text) &gt; 0\n        assert result.size &gt; 0\n\n    def test_copy_editing(self):\n        result = self.modifier.rationalize_text(\n            text=\"Text with errers and typos.\"\n        )\n        assert isinstance(result, CopyEditedText)\n        assert \"errors\" in result.copy_edited_text.lower()\n        assert result.is_edited is True\n\n    def test_model_info_retrieval(self):\n        info = self.modifier.get_model_info(\"summarize\")\n        assert info is not None\n        assert \"model\" in info\n        assert \"temperature\" in info\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#integration-tests","title":"Integration Tests","text":"<pre><code>def test_end_to_end_processing(self):\n    modifier = TextModifiers()\n\n    # Test full pipeline\n    text = \"I led a project that improved system performance by 40%...\"\n\n    summary = modifier.summarize(text=text, max_words=50)\n    edited = modifier.rationalize_text(text=text)\n    achievements = modifier.extract_achievements(text=text)\n    evaluation = modifier.evaluate_review_text(text=text)\n\n    assert all([summary, edited, achievements, evaluation])\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#migration-guide","title":"Migration Guide","text":""},{"location":"metamorphosis/mcp/TextModifiers/#from-v0x-to-v10","title":"From v0.x to v1.0","text":"<ol> <li> <p>Update Imports:    <pre><code># Old\nfrom metamorphosis.text_utils import TextProcessor\n\n# New\nfrom metamorphosis.mcp.text_modifiers import TextModifiers\n</code></pre></p> </li> <li> <p>Update Method Names:    <pre><code># Old\nprocessor.copy_edit(text)\n\n# New\nmodifier.rationalize_text(text=text)\n</code></pre></p> </li> <li> <p>Update Return Types:    <pre><code># Old - returned plain strings/dicts\nresult = processor.summarize(text)\nsummary_text = result[\"summary\"]\n\n# New - returns Pydantic models\nresult = modifier.summarize(text=text)\nsummary_text = result.summarized_text\n</code></pre></p> </li> </ol>"},{"location":"metamorphosis/mcp/TextModifiers/#see-also","title":"See Also","text":"<ul> <li>MCP Package Overview - Package architecture and design</li> <li>Tools Server - MCP server implementation</li> <li>Data Models - Pydantic model definitions</li> <li>Model Registry - LLM client management</li> <li>Examples - Usage examples and tutorials</li> </ul> <p>This class documentation is automatically generated from the source code and maintained in sync with the implementation.</p>"},{"location":"notebooks/supportvectors-common/","title":"Common Patterns","text":"<pre><code>%%html\n\n&lt;!-- Many of the styles here are inspired by: \n    https://towardsdatascience.com/10-practical-tips-you-need-to-know-to-personalize-jupyter-notebook-fbd202777e20 \n\n\n    On the author's local machine, these exist in the custom.css file. However, in order to keep uniform look and feel, \n    and at the request of participants, I have added it to this common import-file here.\n\n    --&gt;\n&lt;link href=\"https://fonts.googleapis.com/css?family=Lora:400,700|Montserrat:300\" rel=\"stylesheet\"/&gt;\n&lt;link href=\"https://fonts.googleapis.com/css2?family=Crimson+Pro&amp;amp;family=Literata&amp;amp;display=swap\" rel=\"stylesheet\"/&gt;\n&lt;style&gt;\n\n\n#ipython_notebook::before{\n content:\"Neural Architectures\";\n        color: white;\n        font-weight: bold;\n        text-transform: uppercase;\n        font-family: 'Lora',serif;\n        font-size:16pt;\n        margin-bottom:15px;\n        margin-top:15px;\n\n}\nbody &gt; #header {\n    #background: #D15555;\n    background: linear-gradient(to bottom, indianred 0%, #fff 100%);\n    opacity: 0.8;\n\n}\n\n\n.navbar-default .navbar-nav &gt; li &gt; a, #kernel_indicator {\n    color: white;\n    transition: all 0.25s;\n    font-size:10pt;\n    font-family: sans-serif;\n    font-weight:normal;\n}\n.navbar-default {\n    padding-left:100px;\n    background: none;\n    border: none;\n}\n\n\nbody &gt; menubar-container {\n    background-color: wheat;\n}\n#ipython_notebook img{                                                                                        \n    display:block; \n\n    background: url(\"https://www.supportvectors.com/wp-content/uploads/2016/03/logo-poster-smaller.png\") no-repeat;\n    background-size: contain;\n\n    padding-left: 600px;\n    padding-right: 100px;\n\n    -moz-box-sizing: border-box;\n    box-sizing: border-box;\n}\n\n\n\nbody {\n #font-family:  'Literata', serif;\n    font-family:'Lora', san-serif;\n    text-align: justify;\n    font-weight: 400;\n    font-size: 12pt;\n}\n\niframe{\n    width:100%;\n    min-height:600px;\n}\n\nh1, h2, h3, h4, h5, h6 {\n# font-family: 'Montserrat', sans-serif;\n font-family:'Lora', serif;\n font-weight: 200;\n text-transform: uppercase;\n color: #EC7063 ;\n}\n\nh2 {\n    color: #000080;\n}\n\n.checkpoint_status, .autosave_status {\n    color:wheat;\n}\n\n#notebook_name {\n    font-weight: 600;\n    font-size:20pt;\n    text-variant:uppercase;\n    color: wheat; \n    margin-right:20px;\n    margin-left:-500px;\n}\n#notebook_name:hover {\nbackground-color: salmon;\n}\n\n\n.dataframe { /* dataframe atau table */\n    background: white;\n    box-shadow: 0px 1px 2px #bbb;\n}\n.dataframe thead th, .dataframe tbody td {\n    text-align: center;\n    padding: 1em;\n}\n\n.checkpoint_status, .autosave_status {\n    color:wheat;\n}\n\n.output {\n    align-items: center;\n}\n\ndiv.cell {\n    transition: all 0.25s;\n    border: none;\n    position: relative;\n    top: 0;\n}\ndiv.cell.selected, div.cell.selected.jupyter-soft-selected {\n    border: none;\n    background: transparent;\n    box-shadow: 0 6px 18px #aaa;\n    z-index: 10;\n    top: -10px;\n}\n.CodeMirror pre, .CodeMirror-dialog, .CodeMirror-dialog .CodeMirror-search-field, .terminal-app .terminal {\n    font-family: 'Hack' , serif; \n    font-weight: 500;\n    font-size: 14pt;\n}\n\n\n\n&lt;/style&gt;    \n</code></pre> <pre><code># Starting with the standard imports\nimport numpy as np\nimport pandas as pd\n\n\n\n# Preprocessing data\nfrom sklearn.model_selection import train_test_split     # data-splitter\nfrom sklearn.preprocessing import StandardScaler         # data-normalization\nfrom sklearn.preprocessing import PolynomialFeatures     # for polynomials\nfrom sklearn.preprocessing import PowerTransformer       # for power-transformations\nfrom sklearn.pipeline import make_pipeline               # for pipelines\nnp.random.seed (42)                                      # for reproducible results\n\n#\n# Modeling and Metrics\n# \n# --For Regressor\nfrom sklearn.dummy import DummyRegressor                 # baseline regressor (null-hypothesis)\nfrom sklearn.linear_model import LinearRegression        # linear regression\nfrom sklearn.linear_model import ( Ridge, \n                                  Lasso, \n                                  ElasticNet,\n                                 RidgeCV, \n                                 LassoCV,\n                                 ElasticNetCV)           # regularized regressions with CV\nfrom sklearn.metrics import mean_squared_error, r2_score # model-metrics\nfrom sklearn.ensemble import RandomForestRegressor\n\n#\n# For Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.dummy import (DummyClassifier, DummyRegressor)\n\n#\n# For clusterers\nfrom scipy import stats, integrate\nimport sklearn.cluster as cluster\nfrom sklearn.cluster import (DBSCAN, KMeans)\nfrom sklearn import metrics\nfrom sklearn.datasets import make_blobs\n\n# Yellowbrick\n#from yellowbrick.features import FeatureImportances\n#from yellowbrick.classifier import ConfusionMatrix, ClassificationReport, ROCAUC\n\n\nfrom matplotlib import colors\nimport seaborn as sns\nlist_of_cmaps=['Blues','Greens','Reds','Purples']   # some colors to use\n\n# Now the Graphical libraries imports and settings\n%matplotlib inline\nimport matplotlib.pyplot as plt                          # for plotting\nimport seaborn as sns                                    # nicer looking plots\nimport altair as alt                                     # for interactive plots\nimport plotly.express as px                              # for plotly express\nfrom matplotlib import colors                            # for web-color specs\npd.set_option('plotting.backend', 'matplotlib')          # pandas_bokeh, plotly, etc       \nplt.rcParams[ 'figure.figsize' ] = '20,10'               # landscape format figures\nplt.rcParams[ 'legend.fontsize' ] = 13                   # legend font size\nplt.rcParams[ 'axes.labelsize' ] = 13                    # axis label font size\nplt.rcParams['figure.dpi'] = 144                         # high-dpi monitors support\nplt.style.use ('ggplot')                                 # emulate ggplot style\n\n# For latex-quality, i.e., publication quality legends and labels on graphs.\n# Warning: you must have installed LaTeX on your system.\nfrom matplotlib import rc\nrc ('font', family='serif')\nrc ('text', usetex=False) # Enable it selectively \nrc ('font', size=16)\n\n# For youtube video display\nfrom IPython.display import YouTubeVideo\n\nimport warnings\nwarnings.filterwarnings ('ignore')  # suppress warning\n</code></pre> <pre><code>%%capture\n# The automated EDA tools\n#import dataprep \n#import ydata_profiling # replacement for pandas_profiling\n#import pandas_profiling # to be deprecated\n#import autoviz \n#import dtale\n#import sweetviz\n</code></pre> <pre><code>def sv_table_styles():\n\n    th_props = [\n      ('font-size', '11pt'),\n        ('font-family', 'sans'),\n      ('text-align', 'center'),\n      ('font-weight', '300'),\n      ('color', 'cornsilk'),\n      ('background-color', 'salmon')\n      ]\n    # Set CSS properties for td elements in dataframe\n    td_props = [\n      ('font-size', '10px'),\n      #('color', 'cornsilk'),\n        ('font-weight', 'normal')\n      ]\n\n    # Currently, could not make this work!\n    first_col_props = [\n        ('background-color', 'cornsilk'),\n        ('color', 'black'),\n        ('font-weight', '300'),\n    ]\n\n    # Set table styles\n    styles = [\n      dict(selector=\"th\", props=th_props),\n      dict(selector=\"td\", props=td_props),\n      dict(selection=\"tr td:first-child()\", props=first_col_props)\n      ]\n\n    return styles\n</code></pre> <pre><code>#\n# Rotate Pandas dataframe column headers.\n# Taken from:\n# https://stackoverflow.com/questions/46715736/rotating-the-column-name-for-a-panda-dataframe\n#\ndef format_vertical_headers(df):\n    \"\"\"Display a dataframe with vertical column headers\"\"\"\n    styles = [\n        dict(selector=\"th\", props=[('width', '40px')]),\n              dict(selector=\"th.col_heading\",\n                   props=[(\"writing-mode\", \"vertical-rl\"),\n                          ('transform', 'rotateZ(180deg)'), \n                          ('height', '160px'),\n                          ('vertical-align', 'top')])]\n    return (df.fillna('').style.set_table_styles(styles))\n</code></pre> <pre><code>def use_default_plot_style(enable_tex:str = True) -&amp;gt; None:\n    default_plot_style = {\n                    \"text.usetex\": enable_tex,\n                    \"legend.fontsize\": 10,\n                    \"axes.labelsize\": 16,\n                    \"figure.dpi\": 144,\n                    'font.family': 'DejaVu Sans', # 'Lora,serif'\n                    'font.weight':'bold',\n            }\n    plt.rcParams.update(default_plot_style)\n    plt.style.use('ggplot')\n\nuse_default_plot_style(False) # set this to True if you have latex installed\n</code></pre> <pre><code>message = \"\"\"\n&lt;div style=\"color:#aaa;font-size:8pt\"&gt;\n&lt;hr/&gt;\n\u00a9 SupportVectors. All rights reserved. &lt;blockquote&gt;This notebook is the intellectual property of SupportVectors, and part of its training material. \nOnly the participants in SupportVectors workshops are allowed to study the notebooks for educational purposes currently, but is prohibited from copying or using it for any other purposes without written permission.\n\n&lt;b&gt; These notebooks are chapters and sections from Asif Qamar's textbook that he is writing on Data Science. So we request you to not circulate the material to others.&lt;/b&gt;\n&lt;/blockquote&gt;\n&lt;hr/&gt;\n&lt;/div&gt;\n\n\"\"\"\nfrom IPython.display import Markdown\n\ndef copyrights() -&amp;gt; None:\n     display (Markdown(message))\n</code></pre> <pre><code>copyrights()\n</code></pre>    \u00a9 SupportVectors. All rights reserved. This notebook is the intellectual property of SupportVectors, and part of its training material.  Only the participants in SupportVectors workshops are allowed to study the notebooks for educational purposes currently, but is prohibited from copying or using it for any other purposes without written permission.  <p> These notebooks are chapters and sections from Asif Qamar's textbook that he is writing on Data Science. So we request you to not circulate the material to others.</p> <pre><code>import os\nimport sys\nfrom pathlib import Path\n\ndef add_svlib():\n\n    current_dir = str(Path.cwd().resolve())\n\n    # Add parent directory to the python path.\n    parent_dir = str(Path.cwd().parent.parent.resolve())+\"/src\"\n\n    if parent_dir not in sys.path:\n        sys.path.append(parent_dir)\n</code></pre> <pre><code>add_svlib()\n</code></pre> <pre><code>from dotenv import load_dotenv\nload_dotenv(override=True)\n</code></pre>"},{"location":"notebooks/supportvectors-common/#importing-the-necessary-libraries","title":"IMPORTING THE NECESSARY LIBRARIES\u00b6","text":""},{"location":"project-guide/tips-and-tricks/cuda-hell/","title":"Some tips and tricks to deal with the CUDA hell","text":"<p>These are some tips and tricks that may be helpful while dealing with CUDA.</p>"},{"location":"project-guide/tips-and-tricks/cuda-hell/#install-the-cuda-toolkit","title":"Install the cuda-toolkit","text":"<p>It is important to remember that one must install the cuda-toolkit, and set the CUDA_HOME environment variable, in order for some critical tools, such as <code>vLLM</code> to work. To do this, follow the instructions at: CUDA installation guide on Linux</p>"},{"location":"project-guide/tips-and-tricks/cuda-hell/#flush-cuda-memory-from-python-code","title":"Flush cuda memory from Python code","text":"<p>It is helpful to start all Python code that uses CUDA with the following mantra invocation:</p> <pre><code>import torch, gc\ngc.collect()\ntorch.cuda.empty_cache()\n</code></pre> <p>Despite this invocation, quite often it will not completely flush the cuda memory. In that case, a better invocation at the level of the Linux shell is:</p> <p><pre><code>nvidia-smi | grep 'python' | awk '{ print $5 }' | xargs -n1 kill -9\n</code></pre> This pearl of wisdom is gleaned from: How to flush GPU memory using CUDA </p> <p>If you're still hitting unexpected memory errors or similar problems then try:</p> <pre><code>sudo fuser -v /dev/nvidia* | cut -d' ' -f2- | sudo xargs -n1 kill -9\n</code></pre>"},{"location":"project-guide/tips-and-tricks/cuda-hell/#nvtop-is-your-friend","title":"<code>nvtop</code> is your friend!","text":"<p>A very useful and visual tool to see what is happening in CUDA is to use the tool <code>nvtop</code>. Install it with the mantra:</p> <p><pre><code>sudo dnf install nvtop\n</code></pre> (or its equivalent if you foolishly use anything other than redhat/centos/rocky-linux).</p>"}]}