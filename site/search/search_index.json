{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Metamorphosis: AI-Powered Self-Review Processing System","text":""},{"location":"#overview","title":"Overview","text":"<p>Metamorphosis is a comprehensive AI-powered text processing system designed for employee self-review workflows. It leverages LangGraph orchestration, OpenAI's language models, and Model Context Protocol (MCP) integration to provide intelligent text processing capabilities including copy editing, summarization, achievement extraction, and quality evaluation.</p>"},{"location":"#project-architecture","title":"Project Architecture","text":"graph TB     subgraph \"User Interfaces\"         A[Streamlit UI]         B[FastAPI REST API]     end      subgraph \"Orchestration Layer\"         C[LangGraph Agent Workflows]         D[Self-Reviewer Agent]     end      subgraph \"Processing Layer\"         E[MCP Tools Server]         F[TextModifiers]     end      subgraph \"LLM Services\"         G[OpenAI GPT Models]         H[Model Registry]     end      subgraph \"Data Models\"         I[Pydantic Schemas]         J[Structured Outputs]     end      A --&gt; B     B --&gt; C     C --&gt; D     D --&gt; E     E --&gt; F     F --&gt; H     H --&gt; G     F --&gt; I     I --&gt; J"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#multi-stage-text-processing-pipeline","title":"\ud83d\udd04 Multi-Stage Text Processing Pipeline","text":"<ul> <li>Copy Editing: Grammar, spelling, and style improvements</li> <li>Summarization: Abstractive summaries with configurable length</li> <li>Achievement Extraction: Structured extraction of key accomplishments</li> <li>Quality Evaluation: Comprehensive writing quality assessment</li> <li>Visualization: Word cloud and radar chart generation</li> </ul>"},{"location":"#ai-agent-orchestration","title":"\ud83e\udd16 AI Agent Orchestration","text":"<ul> <li>LangGraph Workflows: State-based multi-agent processing</li> <li>Parallel Processing: Concurrent execution of independent tasks</li> <li>Real-time Streaming: Live updates via Server-Sent Events</li> <li>State Persistence: Thread-based conversation management</li> </ul>"},{"location":"#mcp-integration","title":"\ud83d\udee0\ufe0f MCP Integration","text":"<ul> <li>Standardized Tools: MCP-compliant text processing functions</li> <li>Type Safety: Pydantic models for structured outputs</li> <li>Error Handling: Comprehensive exception management</li> <li>Caching: LRU-cached model instances for performance</li> </ul>"},{"location":"#package-structure","title":"Package Structure","text":"<p>The project is organized into several specialized packages:</p>"},{"location":"#core-packages","title":"\ud83d\udce6 Core Packages","text":"Package Description Documentation <code>metamorphosis</code> Main package with configuration and utilities View \u2192 <code>metamorphosis.mcp</code> MCP tools and text processing utilities View \u2192 <code>metamorphosis.agents</code> LangGraph workflows and FastAPI service View \u2192 <code>metamorphosis.ui</code> Streamlit user interface View \u2192"},{"location":"#examples","title":"\ud83d\udcc1 Examples","text":"Script Purpose Documentation <code>summarizer_usage.py</code> Demonstrates text summarization View \u2192 <code>rationalize_usage.py</code> Shows copy editing capabilities View \u2192 <code>extract_achievements_usage.py</code> Achievement extraction demo View \u2192 <code>review_text_evaluator_usage.py</code> Quality evaluation example View \u2192 <code>visualize_evaluation_radar.py</code> Radar chart visualization View \u2192"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+</li> <li>OpenAI API key</li> <li>Required dependencies (see <code>pyproject.toml</code>)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li> <p>Environment Setup:    <pre><code># Clone the repository\ngit clone &lt;repository-url&gt;\ncd metamorphosis\n\n# Install dependencies\nuv sync\n\n# Set up environment variables\ncp .env.example .env\n# Edit .env with your OpenAI API key\n</code></pre></p> </li> <li> <p>Basic Usage:    <pre><code>from metamorphosis.mcp.text_modifiers import TextModifiers\n\n# Initialize the text processor\nmodifier = TextModifiers()\n\n# Process text\nresult = modifier.summarize(\n    text=\"Your review text here...\",\n    max_words=100\n)\nprint(result.summarized_text)\n</code></pre></p> </li> <li> <p>Run the Full Application:    <pre><code># Start the FastAPI service\npython -m metamorphosis.agents.agent_service\n\n# Launch the Streamlit UI (in another terminal)\nstreamlit run src/metamorphosis/ui/streamlit_ui.py\n</code></pre></p> </li> </ol>"},{"location":"#configuration","title":"Configuration","text":"<p>The system uses a hierarchical configuration approach:</p>"},{"location":"#environment-variables","title":"Environment Variables","text":"<pre><code># Required\nOPENAI_API_KEY=sk-...\n\n# Optional\nPROJECT_ROOT_DIR=/path/to/project\nMCP_SERVER_HOST=localhost\nMCP_SERVER_PORT=8000\nFASTAPI_HOST=0.0.0.0\nFASTAPI_PORT=8001\n</code></pre>"},{"location":"#configuration-file-configyaml","title":"Configuration File (<code>config.yaml</code>)","text":"<pre><code>models:\n  summarizer:\n    model: gpt-4o\n    temperature: 0.0\n    max_tokens: 2000\n    timeout: 120\n\n  copy_editor:\n    model: gpt-4o\n    temperature: 0.0\n    max_tokens: 4000\n    timeout: 180\n</code></pre>"},{"location":"#key-data-models","title":"Key Data Models","text":""},{"location":"#processing-results","title":"Processing Results","text":"<pre><code>from metamorphosis.datamodel import (\n    SummarizedText,\n    CopyEditedText,\n    AchievementsList,\n    ReviewScorecard\n)\n\n# Each model provides structured, validated outputs\n# with comprehensive metadata and error handling\n</code></pre>"},{"location":"#workflow-state","title":"Workflow State","text":"<pre><code>from metamorphosis.agents.self_reviewer import GraphState\n\n# LangGraph state management for multi-step processing\n# with type safety and serialization support\n</code></pre>"},{"location":"#development","title":"Development","text":""},{"location":"#code-quality-standards","title":"Code Quality Standards","text":"<p>The project follows strict coding standards:</p> <ul> <li>Complexity: Cyclomatic complexity \u2264 10</li> <li>Type Safety: Full type annotations with Python 3.12+ syntax</li> <li>Documentation: Google-style docstrings for all public APIs</li> <li>Formatting: Ruff with 100-character line limit</li> <li>Testing: Comprehensive test coverage with pytest</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<ol> <li>Follow the established code style and patterns</li> <li>Add comprehensive documentation for new features</li> <li>Include type annotations and docstrings</li> <li>Write tests for new functionality</li> <li>Update this documentation as needed</li> </ol>"},{"location":"#troubleshooting","title":"Troubleshooting","text":""},{"location":"#common-issues","title":"Common Issues","text":"<ul> <li>OpenAI API Errors: Check your API key and rate limits</li> <li>Import Errors: Ensure <code>PYTHONPATH</code> includes the <code>src</code> directory</li> <li>Configuration Issues: Verify <code>config.yaml</code> format and environment variables</li> <li>Performance: Monitor token usage and consider caching strategies</li> </ul>"},{"location":"#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre>"},{"location":"#license","title":"License","text":"<p>[Add license information here]</p>"},{"location":"#support","title":"Support","text":"<p>For issues and questions: - Check the documentation - Review the examples directory - File issues on the project repository</p> <p>This documentation is automatically generated and maintained in sync with the codebase.</p>"},{"location":"architecture/","title":"System Architecture","text":""},{"location":"architecture/#overview","title":"Overview","text":"<p>The Metamorphosis system implements a sophisticated AI-powered text processing architecture using modern Python frameworks and design patterns. This document provides a comprehensive view of the system's architecture, component relationships, and data flow.</p>"},{"location":"architecture/#high-level-architecture","title":"High-Level Architecture","text":"graph TB     subgraph \"Presentation Layer\"         UI[Streamlit UI]         API[FastAPI REST API]         CLI[Command Line Tools]     end      subgraph \"Orchestration Layer\"         LG[LangGraph Workflows\\n(self_reviewer)]         EX[WorkflowExecutor]         GB[GraphBuilder]         SM[GraphState]     end      subgraph \"Processing Layer\"         MCP[MCP Tools Server]         TM[TextModifiers Core]         WF[Workflow Nodes]     end      subgraph \"Model Layer\"         MR[Model Registry]         LLM1[Summarizer LLM]         LLM2[Copy Editor LLM]         LLM3[Achievements LLM]         LLM4[Evaluator LLM]     end      subgraph \"Data Layer\"         DM[Pydantic Models]         ST[State Storage]         FS[File System]     end      subgraph \"External Services\"         OPENAI[OpenAI API]         WC[Word Cloud Service]     end      UI --&gt; API     CLI --&gt; MCP     API --&gt; EX     EX --&gt; GB     GB --&gt; LG     LG --&gt; SM     SM --&gt; WF     WF --&gt; TM     TM --&gt; MCP     MCP --&gt; MR     MR --&gt; LLM1     MR --&gt; LLM2     MR --&gt; LLM3     MR --&gt; LLM4     LLM1 --&gt; OPENAI     LLM2 --&gt; OPENAI     LLM3 --&gt; OPENAI     LLM4 --&gt; OPENAI     WF --&gt; WC     TM --&gt; DM     SM --&gt; ST     ST --&gt; FS"},{"location":"architecture/#component-architecture","title":"Component Architecture","text":""},{"location":"architecture/#core-classes-and-relationships","title":"Core Classes and Relationships","text":"classDiagram     class TextModifiers {         -summarizer_llm: ChatOpenAI         -copy_editor_llm: ChatOpenAI         -key_achievements_llm: ChatOpenAI         -review_text_evaluator_llm: ChatOpenAI         +summarize(text, max_words) SummarizedText         +rationalize_text(text) CopyEditedText         +extract_achievements(text) AchievementsList         +evaluate_review_text(text) ReviewScorecard         +get_model_info(method) dict         -_log_model_details_table(method)     }      class ModelRegistry {         -_instance: ModelRegistry         -_initialized: bool         +summarizer_llm: ChatOpenAI         +copy_editor_llm: ChatOpenAI         +key_achievements_llm: ChatOpenAI         +review_text_evaluator_llm: ChatOpenAI         +__new__() ModelRegistry         -_build_chat_openai(config, api_key) ChatOpenAI     }      class GraphState {         +review_text: str         +copy_edited_text: str | None         +summary: str | None         +word_cloud_path: str | None     }      class FastAPIApp {         +invoke(request) InvokeResponse         +stream(request) StreamingResponse         +health() dict     }      class StreamlitUI {         +patch_state(dst, delta) dict         +stream_from_server(url, data) Iterator         +extract_values_from_event(event) dict     }      class SummarizedText {         +summarized_text: str         +size: int         +unit: str     }      class CopyEditedText {         +copy_edited_text: str         +size: int         +is_edited: bool     }      class AchievementsList {         +items: List[Achievement]         +size: int         +unit: str     }      class ReviewScorecard {         +metrics: List[MetricScore]         +overall: int         +verdict: Verdict         +notes: List[str]         +radar_labels: List[str]         +radar_values: List[int]     }      class Achievement {         +title: str         +outcome: str         +impact_area: ImpactArea         +metric_strings: List[str]         +timeframe: Optional[str]         +ownership_scope: Optional[OwnershipScope]         +collaborators: List[str]     }      class MetricScore {         +name: str         +score: int         +rationale: str         +suggestion: str     }      TextModifiers --&gt; ModelRegistry : uses     TextModifiers --&gt; SummarizedText : creates     TextModifiers --&gt; CopyEditedText : creates     TextModifiers --&gt; AchievementsList : creates     TextModifiers --&gt; ReviewScorecard : creates     AchievementsList --&gt; Achievement : contains     ReviewScorecard --&gt; MetricScore : contains     FastAPIApp --&gt; GraphState : manages     StreamlitUI --&gt; FastAPIApp : communicates     GraphState --&gt; TextModifiers : processes_with"},{"location":"architecture/#data-flow-architecture","title":"Data Flow Architecture","text":""},{"location":"architecture/#processing-pipeline","title":"Processing Pipeline","text":"sequenceDiagram     participant User     participant UI as Streamlit UI     participant API as FastAPI Service     participant LG as LangGraph     participant TM as TextModifiers     participant LLM as OpenAI API     participant FS as File System      User-&gt;&gt;UI: Enter review text     UI-&gt;&gt;API: POST /stream {text, thread_id}     API-&gt;&gt;API: Generate thread_id     API-&gt;&gt;LG: run_graph(text, thread_id)      LG-&gt;&gt;TM: rationalize_text(text)     TM-&gt;&gt;LLM: Copy editing request     LLM-&gt;&gt;TM: Edited text     TM-&gt;&gt;LG: CopyEditedText     LG-&gt;&gt;API: Stream event {copy_edited_text}     API-&gt;&gt;UI: SSE event      par Parallel Processing         LG-&gt;&gt;TM: summarize(edited_text)         TM-&gt;&gt;LLM: Summarization request         LLM-&gt;&gt;TM: Summary         TM-&gt;&gt;LG: SummarizedText         LG-&gt;&gt;API: Stream event {summary}         API-&gt;&gt;UI: SSE event     and         LG-&gt;&gt;FS: create_word_cloud(edited_text)         FS-&gt;&gt;LG: Word cloud path         LG-&gt;&gt;API: Stream event {word_cloud_path}         API-&gt;&gt;UI: SSE event     end      UI-&gt;&gt;User: Display results"},{"location":"architecture/#state-management-flow","title":"State Management Flow","text":"stateDiagram-v2     [*] --&gt; Initialized     Initialized --&gt; Processing: User submits text     Processing --&gt; CopyEditing: Start workflow     CopyEditing --&gt; ParallelProcessing: Text edited      state ParallelProcessing {         [*] --&gt; Summarizing         [*] --&gt; WordCloudGen         Summarizing --&gt; SummaryComplete         WordCloudGen --&gt; WordCloudComplete         SummaryComplete --&gt; [*]         WordCloudComplete --&gt; [*]     }      ParallelProcessing --&gt; Complete: All tasks done     Complete --&gt; [*]: Results delivered      Processing --&gt; Error: Exception occurred     Error --&gt; [*]: Error handled"},{"location":"architecture/#design-patterns","title":"Design Patterns","text":""},{"location":"architecture/#singleton-pattern-model-registry","title":"Singleton Pattern (Model Registry)","text":"<pre><code>class ModelRegistry:\n    _instance = None\n    _initialized = False\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n\n    def __init__(self):\n        if not self._initialized:\n            # Initialize LLM clients\n            self._initialize_llms()\n            self._initialized = True\n</code></pre>"},{"location":"architecture/#factory-pattern-configuration-loading","title":"Factory Pattern (Configuration Loading)","text":"<pre><code>def get_model_registry() -&gt; ModelRegistry:\n    \"\"\"Factory function for ModelRegistry singleton.\"\"\"\n    return ModelRegistry()\n\ndef _load_config() -&gt; dict[str, Any]:\n    \"\"\"Factory for configuration loading with fallbacks.\"\"\"\n    # Configuration loading logic\n    return config_data\n</code></pre>"},{"location":"architecture/#strategy-pattern-text-processing","title":"Strategy Pattern (Text Processing)","text":"<pre><code>class TextModifiers:\n    def __init__(self):\n        # Different strategies for different processing types\n        self.strategies = {\n            'summarize': self._create_summarizer_chain(),\n            'copy_edit': self._create_copy_editor_chain(),\n            'extract_achievements': self._create_achievements_chain(),\n            'evaluate': self._create_evaluator_chain()\n        }\n</code></pre>"},{"location":"architecture/#observer-pattern-streaming-events","title":"Observer Pattern (Streaming Events)","text":"<pre><code>async def _generate_stream_events(review_text: str, thread_id: str):\n    \"\"\"Observer pattern for streaming workflow events.\"\"\"\n    async for event in run_graph(graph, review_text, thread_id):\n        # Notify observers (UI clients) of state changes\n        yield f\"data: {json.dumps(event)}\\n\\n\".encode(\"utf-8\")\n</code></pre>"},{"location":"architecture/#decorator-pattern-validation","title":"Decorator Pattern (Validation)","text":"<pre><code>@validate_call\ndef summarize(\n    self,\n    *,\n    text: Annotated[str, Field(min_length=1)],\n    max_words: Annotated[int, Field(gt=0)] = 300,\n) -&gt; SummarizedText:\n    # Method implementation with automatic validation\n</code></pre>"},{"location":"architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"architecture/#api-security","title":"API Security","text":"graph LR     subgraph \"Security Layers\"         A[CORS Middleware]         B[Input Validation]         C[Rate Limiting]         D[Error Sanitization]     end      subgraph \"Data Protection\"         E[Environment Variables]         F[Secret Management]         G[Input Sanitization]     end      Request --&gt; A     A --&gt; B     B --&gt; C     C --&gt; D     D --&gt; Processing      Processing --&gt; E     E --&gt; F     F --&gt; G"},{"location":"architecture/#configuration-security","title":"Configuration Security","text":"<ul> <li>API Key Management: Environment variable isolation</li> <li>Input Validation: Pydantic model validation</li> <li>Error Handling: Sanitized error responses</li> <li>CORS Configuration: Restricted origins in production</li> </ul>"},{"location":"architecture/#performance-architecture","title":"Performance Architecture","text":""},{"location":"architecture/#caching-strategy","title":"Caching Strategy","text":"graph TB     subgraph \"Caching Layers\"         A[LLM Client CacheSingleton Registry]         B[Prompt Template CacheFile System]         C[Processing Results CacheLRU Cache]     end      subgraph \"Performance Optimizations\"         D[Parallel ProcessingAsyncIO]         E[Connection PoolingHTTP Clients]         F[Resource ManagementMemory Cleanup]     end      Request --&gt; A     A --&gt; B     B --&gt; C     C --&gt; D     D --&gt; E     E --&gt; F     F --&gt; Response"},{"location":"architecture/#scalability-considerations","title":"Scalability Considerations","text":"<ul> <li>Horizontal Scaling: Stateless service design</li> <li>Load Balancing: Multiple service instances</li> <li>Resource Optimization: Efficient memory usage</li> <li>Connection Management: HTTP connection pooling</li> </ul>"},{"location":"architecture/#error-handling-architecture","title":"Error Handling Architecture","text":""},{"location":"architecture/#exception-hierarchy","title":"Exception Hierarchy","text":"classDiagram     class Exception {         +message: str         +args: tuple     }      class ReviewError {         +message: str         +context: dict         +operation: str         +error_code: str     }      class ValidationError {         +validation_context: dict     }      class PostconditionError {         +postcondition_context: dict     }      class ConfigurationError {         +config_context: dict     }      class MCPToolError {         +tool_context: dict     }      class FileOperationError {         +file_context: dict     }      Exception &lt;|-- ReviewError     ReviewError &lt;|-- ValidationError     ReviewError &lt;|-- PostconditionError     ReviewError &lt;|-- ConfigurationError     ReviewError &lt;|-- MCPToolError     ReviewError &lt;|-- FileOperationError"},{"location":"architecture/#error-flow","title":"Error Flow","text":"sequenceDiagram     participant Client     participant API     participant Service     participant LLM      Client-&gt;&gt;API: Request     API-&gt;&gt;Service: Process     Service-&gt;&gt;LLM: LLM Call     LLM-&gt;&gt;Service: Error Response     Service-&gt;&gt;Service: Create PostconditionError     Service-&gt;&gt;API: Structured Error     API-&gt;&gt;API: Log Error Context     API-&gt;&gt;Client: JSON Error Response"},{"location":"architecture/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"architecture/#logging-architecture","title":"Logging Architecture","text":"graph TB     subgraph \"Logging Sources\"         A[TextModifiers]         B[Model Registry]         C[FastAPI Service]         D[LangGraph Workflows]     end      subgraph \"Log Processing\"         E[Loguru Logger]         F[Structured Logging]         G[Log Aggregation]     end      subgraph \"Output Destinations\"         H[Console Output]         I[File Logs]         J[Debug Logs]     end      A --&gt; E     B --&gt; E     C --&gt; E     D --&gt; E     E --&gt; F     F --&gt; G     G --&gt; H     G --&gt; I     G --&gt; J"},{"location":"architecture/#metrics-collection","title":"Metrics Collection","text":"<ul> <li>Processing Metrics: Token usage, processing time</li> <li>Error Metrics: Error rates, failure patterns  </li> <li>Performance Metrics: Response times, throughput</li> <li>Resource Metrics: Memory usage, CPU utilization</li> </ul>"},{"location":"architecture/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"architecture/#container-architecture","title":"Container Architecture","text":"graph TB     subgraph \"Application Container\"         A[Python 3.12 Runtime]         B[FastAPI Service]         C[Streamlit UI]         D[MCP Tools]     end      subgraph \"Configuration\"         E[Environment Variables]         F[Config Files]         G[Prompt Templates]     end      subgraph \"External Dependencies\"         H[OpenAI API]         I[File System Storage]     end      A --&gt; B     A --&gt; C     A --&gt; D     E --&gt; A     F --&gt; A     G --&gt; A     B --&gt; H     D --&gt; I"},{"location":"architecture/#service-dependencies","title":"Service Dependencies","text":"<ul> <li>Runtime: Python 3.12+</li> <li>Core Dependencies: FastAPI, LangChain, Pydantic, Streamlit</li> <li>AI Services: OpenAI API</li> <li>Utilities: Rich, Plotly, Loguru</li> <li>Development: Ruff, Pytest, Radon</li> </ul>"},{"location":"architecture/#future-architecture-considerations","title":"Future Architecture Considerations","text":""},{"location":"architecture/#extensibility-points","title":"Extensibility Points","text":"<ol> <li>New Processing Types: Additional text processing capabilities</li> <li>Multiple LLM Providers: Support for different AI services</li> <li>Enhanced Workflows: More complex multi-agent workflows</li> <li>Real-time Processing: WebSocket-based real-time updates</li> <li>Batch Processing: Large-scale document processing</li> </ol>"},{"location":"architecture/#scalability-enhancements","title":"Scalability Enhancements","text":"<ol> <li>Microservices: Split into smaller, focused services</li> <li>Message Queues: Asynchronous processing with queues</li> <li>Caching Layer: Redis or similar for distributed caching</li> <li>Database Integration: Persistent storage for results</li> <li>Load Balancing: Multiple service instances</li> </ol> <p>This architecture documentation is maintained in sync with the system implementation and updated as the system evolves.</p>"},{"location":"examples/","title":"Examples Directory","text":"<p>The <code>examples/</code> directory contains comprehensive usage examples and demonstrations of the Metamorphosis text processing capabilities. Each example is a standalone script that showcases specific features with detailed explanations and rich output formatting.</p>"},{"location":"examples/#overview","title":"Overview","text":"graph TB     subgraph \"Text Processing Examples\"         A[summarizer_usage.pyBasic Summarization]         B[rationalize_usage.pyCopy Editing Demo]     end      subgraph \"Advanced Processing Examples\"         C[extract_achievements_usage.pyAchievement Extraction]         D[review_text_evaluator_usage.pyQuality Evaluation]     end      subgraph \"Visualization Examples\"         E[visualize_evaluation_radar.pyRadar Chart Generation]     end      subgraph \"Input Data\"         F[sample_reviews/Example Review Files]     end      A --&gt; F     B --&gt; F     C --&gt; F     D --&gt; F     D --&gt; E"},{"location":"examples/#example-scripts","title":"Example Scripts","text":""},{"location":"examples/#basic-text-processing","title":"Basic Text Processing","text":"Script Purpose Features Output <code>summarizer_usage.py</code> Text summarization demo Configurable word limits, rich formatting Console display + JSONL <code>rationalize_usage.py</code> Copy editing showcase Before/after comparison, change tracking Rich table output"},{"location":"examples/#advanced-processing","title":"Advanced Processing","text":"Script Purpose Features Output <code>extract_achievements_usage.py</code> Achievement extraction Structured metadata, ranking Rich table + JSONL <code>review_text_evaluator_usage.py</code> Quality assessment 6-dimension scoring, suggestions Rich table + JSONL"},{"location":"examples/#visualization","title":"Visualization","text":"Script Purpose Features Output <code>visualize_evaluation_radar.py</code> Radar chart creation Interactive plots, goal references HTML file + console"},{"location":"examples/#common-features","title":"Common Features","text":""},{"location":"examples/#rich-console-output","title":"Rich Console Output","text":"<p>All examples use the <code>rich</code> library for beautiful console formatting:</p> <ul> <li>Color-coded output: Different colors for different types of information</li> <li>Progress indicators: Visual feedback during processing</li> <li>Structured tables: Organized display of results</li> <li>Panel formatting: Highlighted sections and tips</li> </ul>"},{"location":"examples/#error-handling","title":"Error Handling","text":"<p>Comprehensive error handling with user-friendly messages:</p> <pre><code>try:\n    # Processing logic\n    result = modifier.process_text(text=review_text)\nexcept ValidationError as e:\n    console.print(f\"\u274c Validation Error: {e}\", style=\"bold red\")\nexcept PostconditionError as e:\n    console.print(f\"\u274c Processing Error: {e}\", style=\"bold red\")\nexcept Exception as e:\n    console.print(f\"\u274c Unexpected Error: {e}\", style=\"bold red\")\n</code></pre>"},{"location":"examples/#data-persistence","title":"Data Persistence","text":"<p>Examples demonstrate both console output and data persistence:</p> <ul> <li>JSONL files: Structured data for further processing</li> <li>HTML files: Interactive visualizations</li> <li>Logging: Detailed operation tracking</li> </ul>"},{"location":"examples/#usage-patterns","title":"Usage Patterns","text":""},{"location":"examples/#running-examples","title":"Running Examples","text":"<p>All examples can be run from the project root using Python's module syntax:</p> <pre><code># Basic usage\npython -m src.examples.summarizer_usage\npython -m src.examples.rationalize_usage\n\n# Advanced features\npython -m src.examples.extract_achievements_usage\npython -m src.examples.review_text_evaluator_usage\n\n# Visualization\npython -m src.examples.visualize_evaluation_radar\n</code></pre>"},{"location":"examples/#using-with-uv","title":"Using with uv","text":"<p>For consistent dependency management:</p> <pre><code>uv run python -m src.examples.summarizer_usage\nuv run python -m src.examples.extract_achievements_usage\n</code></pre>"},{"location":"examples/#custom-input","title":"Custom Input","text":"<p>Examples can be modified to use custom input:</p> <pre><code># Modify the review_text variable in any example\nreview_text = \"\"\"\nYour custom employee review text here...\n\"\"\"\n</code></pre>"},{"location":"examples/#integration-examples","title":"Integration Examples","text":""},{"location":"examples/#basic-integration","title":"Basic Integration","text":"<pre><code>from metamorphosis.mcp.text_modifiers import TextModifiers\nfrom rich.console import Console\n\ndef simple_processing_example():\n    console = Console()\n    modifier = TextModifiers()\n\n    # Process text\n    result = modifier.summarize(\n        text=\"Your review text...\",\n        max_words=100\n    )\n\n    # Display result\n    console.print(f\"Summary: {result.summarized_text}\")\n</code></pre>"},{"location":"examples/#batch-processing","title":"Batch Processing","text":"<pre><code>def batch_processing_example():\n    modifier = TextModifiers()\n    reviews = load_multiple_reviews()\n\n    results = []\n    for review in reviews:\n        try:\n            result = modifier.extract_achievements(text=review)\n            results.append(result)\n        except Exception as e:\n            print(f\"Error processing review: {e}\")\n\n    return results\n</code></pre>"},{"location":"examples/#pipeline-integration","title":"Pipeline Integration","text":"<pre><code>def full_pipeline_example():\n    modifier = TextModifiers()\n    review_text = load_review()\n\n    # Step 1: Copy edit\n    edited = modifier.rationalize_text(text=review_text)\n\n    # Step 2: Extract achievements\n    achievements = modifier.extract_achievements(text=edited.copy_edited_text)\n\n    # Step 3: Evaluate quality\n    evaluation = modifier.evaluate_review_text(text=edited.copy_edited_text)\n\n    # Step 4: Summarize\n    summary = modifier.summarize(text=edited.copy_edited_text, max_words=100)\n\n    return {\n        \"edited\": edited,\n        \"achievements\": achievements,\n        \"evaluation\": evaluation,\n        \"summary\": summary\n    }\n</code></pre>"},{"location":"examples/#sample-data","title":"Sample Data","text":""},{"location":"examples/#review-files","title":"Review Files","text":"<p>The <code>sample_reviews/</code> directory contains example employee review files:</p> <ul> <li><code>raw_review.md</code>: Original, unedited review text</li> <li><code>copy_edited.md</code>: Professionally edited version</li> <li><code>data_engineer_review.md</code>: Technical role-specific example</li> </ul>"},{"location":"examples/#generated-outputs","title":"Generated Outputs","text":"<p>Examples generate various output files:</p> <ul> <li><code>key_achievements.jsonl</code>: Extracted achievements data</li> <li><code>text_evaluator_results.jsonl</code>: Quality evaluation results</li> <li><code>evaluation_radar_plot.html</code>: Interactive visualization</li> </ul>"},{"location":"examples/#customization-guide","title":"Customization Guide","text":""},{"location":"examples/#adding-new-examples","title":"Adding New Examples","text":"<p>To create a new example script:</p> <ol> <li> <p>Create the script file:    <pre><code># src/examples/my_custom_example.py\nfrom metamorphosis.mcp.text_modifiers import TextModifiers\nfrom rich.console import Console\n\ndef main():\n    console = Console()\n    modifier = TextModifiers()\n\n    # Your custom logic here\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p> </li> <li> <p>Add file header:    <pre><code># =============================================================================\n#  Filename: my_custom_example.py\n#\n#  Short Description: Custom example demonstrating specific feature\n#\n#  Creation date: YYYY-MM-DD\n#  Author: Your Name\n# =============================================================================\n</code></pre></p> </li> <li> <p>Follow established patterns:</p> </li> <li>Use rich for output formatting</li> <li>Include comprehensive error handling</li> <li>Provide clear documentation</li> <li>Save results to files when appropriate</li> </ol>"},{"location":"examples/#modifying-existing-examples","title":"Modifying Existing Examples","text":"<p>Common customizations:</p> <ol> <li> <p>Change input data:    <pre><code># Use different sample file\nreview_file = project_root / \"sample_reviews\" / \"your_review.md\"\n</code></pre></p> </li> <li> <p>Adjust processing parameters:    <pre><code># Different word limits for summarization\nresult = modifier.summarize(text=review_text, max_words=200)\n</code></pre></p> </li> <li> <p>Modify output format:    <pre><code># Add custom table columns\ntable.add_column(\"Custom Field\", style=\"cyan\")\n</code></pre></p> </li> </ol>"},{"location":"examples/#testing-examples","title":"Testing Examples","text":""},{"location":"examples/#automated-testing","title":"Automated Testing","text":"<p>Examples can be tested programmatically:</p> <pre><code>import subprocess\nimport sys\n\ndef test_example(example_name):\n    \"\"\"Test an example script.\"\"\"\n    try:\n        result = subprocess.run(\n            [sys.executable, \"-m\", f\"src.examples.{example_name}\"],\n            capture_output=True,\n            text=True,\n            timeout=300\n        )\n        return result.returncode == 0\n    except subprocess.TimeoutExpired:\n        return False\n\n# Test all examples\nexamples = [\n    \"summarizer_usage\",\n    \"rationalize_usage\",\n    \"extract_achievements_usage\",\n    \"review_text_evaluator_usage\",\n    \"visualize_evaluation_radar\"\n]\n\nfor example in examples:\n    success = test_example(example)\n    print(f\"{example}: {'\u2705 PASS' if success else '\u274c FAIL'}\")\n</code></pre>"},{"location":"examples/#manual-testing","title":"Manual Testing","text":"<ol> <li>Run each example individually</li> <li>Verify console output is formatted correctly</li> <li>Check generated files exist and contain valid data</li> <li>Test with different input data</li> </ol>"},{"location":"examples/#performance-considerations","title":"Performance Considerations","text":""},{"location":"examples/#resource-usage","title":"Resource Usage","text":"<p>Examples are designed to be educational rather than production-optimized:</p> <ul> <li>Token usage: Monitor OpenAI API costs when running multiple examples</li> <li>Memory usage: Large review texts may consume significant memory</li> <li>Processing time: Complex operations may take several seconds</li> </ul>"},{"location":"examples/#optimization-tips","title":"Optimization Tips","text":"<p>For production use:</p> <ol> <li>Implement caching: Reuse TextModifiers instances</li> <li>Batch processing: Process multiple reviews efficiently</li> <li>Async processing: Use async/await for concurrent operations</li> <li>Resource monitoring: Track API usage and costs</li> </ol>"},{"location":"examples/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Import errors:    <pre><code># Ensure PYTHONPATH is set correctly\nexport PYTHONPATH=\"${PYTHONPATH}:$(pwd)/src\"\n</code></pre></p> </li> <li> <p>API key issues:    <pre><code># Verify environment variables\necho $OPENAI_API_KEY\n</code></pre></p> </li> <li> <p>File not found errors:    <pre><code># Run from project root directory\ncd /path/to/metamorphosis\npython -m src.examples.example_name\n</code></pre></p> </li> </ol>"},{"location":"examples/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging for troubleshooting:</p> <pre><code>import logging\nfrom loguru import logger\n\n# Add debug logging\nlogger.add(\"debug.log\", level=\"DEBUG\")\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre>"},{"location":"examples/#contributing","title":"Contributing","text":""},{"location":"examples/#adding-new-examples_1","title":"Adding New Examples","text":"<p>When contributing new examples:</p> <ol> <li>Follow the established code style and patterns</li> <li>Include comprehensive documentation</li> <li>Add error handling and user feedback</li> <li>Test with various input scenarios</li> <li>Update this documentation</li> </ol>"},{"location":"examples/#improving-existing-examples","title":"Improving Existing Examples","text":"<p>Areas for improvement:</p> <ul> <li>Enhanced error messages</li> <li>Additional output formats</li> <li>Performance optimizations</li> <li>More comprehensive examples</li> <li>Better visualization options</li> </ul>"},{"location":"examples/#see-also","title":"See Also","text":"<ul> <li>MCP Package - Core text processing utilities</li> <li>Agents Package - Workflow orchestration</li> <li>Data Models - Pydantic schemas</li> <li>Configuration - Setup and configuration</li> </ul> <p>This documentation is automatically generated and maintained in sync with the example implementations.</p>"},{"location":"examples/extract_achievements_usage/","title":"Achievement Extraction Usage Example","text":"<p>Example usage of the TextModifiers.extract_achievements method.</p> <p>This script demonstrates how to: 1. Load a sample employee review from the sample_reviews directory 2. Extract key achievements using the TextModifiers class 3. Display the results in a beautiful rich table format 4. Save the structured achievements data to a JSONL file</p> Run this script from the project root <p>python -m src.examples.extract_achievements_usage</p>"},{"location":"examples/extract_achievements_usage/#examples.extract_achievements_usage.load_sample_review","title":"<code>load_sample_review()</code>","text":"<p>Load the sample copy-edited review from sample_reviews directory.</p> <p>Returns:</p> Type Description <code>str</code> <p>The content of the copy_edited.md file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the sample review file doesn't exist.</p> Source code in <code>src/examples/extract_achievements_usage.py</code> <pre><code>def load_sample_review() -&gt; str:\n    \"\"\"Load the sample copy-edited review from sample_reviews directory.\n\n    Returns:\n        The content of the copy_edited.md file.\n\n    Raises:\n        FileNotFoundError: If the sample review file doesn't exist.\n    \"\"\"\n    project_root = get_project_root()\n    sample_file = project_root / \"sample_reviews\" / \"copy_edited.md\"\n\n    if not sample_file.exists():\n        raise FileNotFoundError(f\"Sample review file not found: {sample_file}\")\n\n    return sample_file.read_text(encoding=\"utf-8\")\n</code></pre>"},{"location":"examples/extract_achievements_usage/#examples.extract_achievements_usage.main","title":"<code>main()</code>","text":"<p>Main function that demonstrates achievement extraction.</p> Source code in <code>src/examples/extract_achievements_usage.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main function that demonstrates achievement extraction.\"\"\"\n    console = Console()\n\n    try:\n        # Display header\n        console.print(\"\\n\")\n        console.print(\n            Panel.fit(\n                \"\ud83d\udd0d Achievement Extraction Demo\\n\\n\"\n                \"This example loads a sample employee review and extracts\\n\"\n                \"key achievements using the TextModifiers.extract_achievements method.\",\n                title=\"Metamorphosis Text Processing\",\n                style=\"bold blue\",\n            )\n        )\n\n        # Load the sample review\n        console.print(\"\\n\ud83d\udcd6 Loading sample review from sample_reviews/copy_edited.md...\")\n        review_text = load_sample_review()\n        console.print(f\"\u2705 Loaded review text ({len(review_text):,} characters)\")\n\n        # Initialize TextModifiers\n        console.print(\"\\n\ud83e\udd16 Initializing TextModifiers with LLM clients...\")\n        modifier = TextModifiers()\n        console.print(\"\u2705 TextModifiers initialized successfully\")\n\n        # Extract achievements\n        console.print(\"\\n\ud83d\udd0d Extracting key achievements from the review...\")\n        with console.status(\"[bold green]Processing with LLM...\"):\n            achievements = modifier.extract_achievements(text=review_text)\n\n        console.print(f\"\u2705 Extracted {len(achievements.items)} achievements\")\n\n        # Write achievements to JSONL file\n        console.print(\"\\n\ud83d\udcdd Writing achievements to JSONL file...\")\n        project_root = get_project_root()\n        jsonl_output_path = project_root / \"sample_reviews\" / \"key_achievements.jsonl\"\n        write_achievements_to_jsonl(achievements, jsonl_output_path)\n        console.print(f\"\u2705 Saved achievements to: {jsonl_output_path}\")\n\n        # Display results\n        console.print(\"\\n\")\n        console.print(create_summary_panel(achievements))\n        console.print(\"\\n\")\n        console.print(create_achievements_table(achievements))\n\n        # Display raw data option\n        console.print(\"\\n\")\n        console.print(\n            Panel(\n                \"\ud83d\udca1 Tip: The achievements are returned as structured Pydantic objects\\n\"\n                \"that can be easily serialized to JSON or integrated into other systems.\\n\\n\"\n                \"Each Achievement object contains: title, outcome, impact_area,\\n\"\n                \"metric_strings, timeframe, ownership_scope, and collaborators.\\n\\n\"\n                f\"\ud83d\udcc4 The complete data has been saved to: {jsonl_output_path.name}\",\n                title=\"\u2139\ufe0f  Integration Notes\",\n                style=\"dim cyan\",\n            )\n        )\n\n    except FileNotFoundError as e:\n        console.print(f\"\u274c Error: {e}\", style=\"bold red\")\n        sys.exit(1)\n    except Exception as e:\n        logger.exception(\"Unexpected error during achievement extraction\")\n        console.print(f\"\u274c Unexpected error: {e}\", style=\"bold red\")\n        sys.exit(1)\n</code></pre>"},{"location":"examples/extract_achievements_usage/#examples.extract_achievements_usage.write_achievements_to_jsonl","title":"<code>write_achievements_to_jsonl(achievements_list, output_path)</code>","text":"<p>Write the achievements to a JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>achievements_list</code> <code>AchievementsList</code> <p>The AchievementsList object containing extracted achievements.</p> required <code>output_path</code> <code>Path</code> <p>Path to the output JSONL file.</p> required Source code in <code>src/examples/extract_achievements_usage.py</code> <pre><code>def write_achievements_to_jsonl(achievements_list: AchievementsList, output_path: Path) -&gt; None:\n    \"\"\"Write the achievements to a JSONL file.\n\n    Args:\n        achievements_list: The AchievementsList object containing extracted achievements.\n        output_path: Path to the output JSONL file.\n    \"\"\"\n    # Create the directory if it doesn't exist\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Convert the AchievementsList to dict and write to JSONL\n    with output_path.open(\"w\", encoding=\"utf-8\") as f:\n        # Write the full AchievementsList object as one JSON line\n        achievements_dict = achievements_list.model_dump()\n        f.write(json.dumps(achievements_dict, ensure_ascii=False) + \"\\n\")\n\n    logger.debug(\"Wrote achievements to JSONL file: {}\", output_path)\n</code></pre>"},{"location":"examples/extract_achievements_usage/#overview","title":"Overview","text":"<p>The <code>extract_achievements_usage.py</code> script demonstrates the powerful achievement extraction capabilities of the Metamorphosis system. It showcases how to extract, rank, and display structured accomplishments from employee self-review text using advanced LLM processing.</p>"},{"location":"examples/extract_achievements_usage/#features","title":"Features","text":""},{"location":"examples/extract_achievements_usage/#core-functionality","title":"Core Functionality","text":"<ul> <li>Structured Achievement Extraction: Identifies up to 10 key accomplishments</li> <li>Rich Metadata: Impact areas, metrics, timeframes, and collaborators</li> <li>Ranking Algorithm: Achievements ranked by business impact and quality</li> <li>Professional Display: Beautiful rich table formatting with color coding</li> <li>Data Persistence: JSONL output for further processing and analysis</li> </ul>"},{"location":"examples/extract_achievements_usage/#visual-output","title":"Visual Output","text":"graph LR     A[Raw Review Text] --&gt; B[TextModifiers]     B --&gt; C[Achievement Extraction]     C --&gt; D[Rich Table Display]     C --&gt; E[JSONL File Output]     D --&gt; F[Console Output]     E --&gt; G[key_achievements.jsonl]"},{"location":"examples/extract_achievements_usage/#usage","title":"Usage","text":""},{"location":"examples/extract_achievements_usage/#basic-execution","title":"Basic Execution","text":"<p>Run from the project root directory:</p> <pre><code># Using Python directly\npython -m src.examples.extract_achievements_usage\n\n# Using uv (recommended)\nuv run python -m src.examples.extract_achievements_usage\n</code></pre>"},{"location":"examples/extract_achievements_usage/#expected-output","title":"Expected Output","text":"<p>The script produces rich console output with:</p> <ol> <li>Processing Status: Real-time feedback on extraction progress</li> <li>Achievement Table: Formatted display of extracted accomplishments</li> <li>Summary Panel: Overview of results and integration notes</li> <li>File Output: JSONL file creation confirmation</li> </ol>"},{"location":"examples/extract_achievements_usage/#code-structure","title":"Code Structure","text":""},{"location":"examples/extract_achievements_usage/#main-processing-flow","title":"Main Processing Flow","text":"<pre><code>def main() -&gt; None:\n    \"\"\"Main execution function with comprehensive error handling.\"\"\"\n    console = Console()\n\n    try:\n        # 1. Load sample review text\n        review_text = load_sample_review()\n\n        # 2. Initialize text processor\n        modifier = TextModifiers()\n\n        # 3. Extract achievements\n        achievements = modifier.extract_achievements(text=review_text)\n\n        # 4. Display results\n        display_results(console, achievements)\n\n        # 5. Save to JSONL\n        save_to_jsonl(achievements)\n\n    except Exception as e:\n        handle_error(console, e)\n</code></pre>"},{"location":"examples/extract_achievements_usage/#key-functions","title":"Key Functions","text":""},{"location":"examples/extract_achievements_usage/#achievement-table-creation","title":"Achievement Table Creation","text":"<pre><code>def create_achievements_table(achievements_list: AchievementsList) -&gt; Table:\n    \"\"\"Create a rich table displaying extracted achievements.\"\"\"\n    table = Table(\n        title=(\n            f\"\ud83c\udfc6 Extracted Key Achievements \"\n            f\"({len(achievements_list.items)} items, ~{achievements_list.size} tokens)\"\n        ),\n        box=box.ROUNDED,\n        show_header=True,\n        header_style=\"bold magenta\"\n    )\n\n    # Add columns with appropriate styling\n    table.add_column(\"Title\", style=\"bold cyan\", width=25)\n    table.add_column(\"Outcome\", style=\"white\", width=40)\n    table.add_column(\"Impact\", style=\"green\", width=15)\n    table.add_column(\"Metrics\", style=\"yellow\", width=20)\n    table.add_column(\"Timeline\", style=\"blue\", width=12)\n\n    # Populate table rows\n    for i, achievement in enumerate(achievements_list.items, 1):\n        table.add_row(\n            f\"{i}. {achievement.title}\",\n            achievement.outcome,\n            format_impact_area(achievement.impact_area),\n            format_metrics(achievement.metric_strings),\n            achievement.timeframe or \"Not specified\"\n        )\n\n    return table\n</code></pre>"},{"location":"examples/extract_achievements_usage/#jsonl-output","title":"JSONL Output","text":"<pre><code>def write_achievements_to_jsonl(\n    achievements_list: AchievementsList, \n    output_path: Path\n) -&gt; None:\n    \"\"\"Write achievements to JSONL format for further processing.\"\"\"\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with output_path.open(\"w\", encoding=\"utf-8\") as f:\n        achievements_dict = achievements_list.model_dump()\n        f.write(json.dumps(achievements_dict, ensure_ascii=False) + \"\\n\")\n\n    logger.debug(f\"Achievements written to {output_path}\")\n</code></pre>"},{"location":"examples/extract_achievements_usage/#sample-input","title":"Sample Input","text":"<p>The script uses a sample employee review from <code>sample_reviews/copy_edited.md</code>:</p> <pre><code>I had an eventful cycle this summer. Learnt agentic workflows and \nimplemented a self-reviewer agent process. It significantly improved \nemployee productivity for the organization.\n\nKey accomplishments include:\n- Reduced review processing time by 60%\n- Implemented automated quality checks\n- Collaborated with HR and Engineering teams\n- Delivered training to 50+ employees\n</code></pre>"},{"location":"examples/extract_achievements_usage/#sample-output","title":"Sample Output","text":""},{"location":"examples/extract_achievements_usage/#console-display","title":"Console Display","text":"<p>The script produces a beautifully formatted table:</p> <pre><code>\ud83c\udfc6 Extracted Key Achievements (3 items, ~145 tokens)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Title                       \u2502 Outcome                              \u2502 Impact          \u2502 Metrics              \u2502 Timeline     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1. Implemented self-reviewer\u2502 Delivered automated agent process   \u2502 \ud83d\ude80 Innovation   \u2502 60%                  \u2502 Summer 2025  \u2502\n\u2502    agent process           \u2502 that improved productivity           \u2502                 \u2502                      \u2502              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2. Reduced review processing\u2502 Achieved 60% reduction in processing\u2502 \u26a1 Performance  \u2502 60%, 50+             \u2502 Not specified\u2502\n\u2502    time                    \u2502 time through automation              \u2502                 \u2502                      \u2502              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 3. Delivered employee       \u2502 Trained 50+ employees on new        \u2502 \ud83d\udc65 Productivity \u2502 50+                  \u2502 Not specified\u2502\n\u2502    training program        \u2502 workflow processes                   \u2502                 \u2502                      \u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"examples/extract_achievements_usage/#jsonl-output_1","title":"JSONL Output","text":"<p>The generated <code>key_achievements.jsonl</code> file contains:</p> <pre><code>{\n  \"items\": [\n    {\n      \"title\": \"Implemented self-reviewer agent process\",\n      \"outcome\": \"Delivered automated agent process that improved productivity\",\n      \"impact_area\": \"innovation\",\n      \"metric_strings\": [\"60%\"],\n      \"timeframe\": \"Summer 2025\",\n      \"ownership_scope\": \"TechLead\",\n      \"collaborators\": [\"HR team\", \"Engineering teams\"]\n    }\n  ],\n  \"size\": 145,\n  \"unit\": \"tokens\"\n}\n</code></pre>"},{"location":"examples/extract_achievements_usage/#advanced-features","title":"Advanced Features","text":""},{"location":"examples/extract_achievements_usage/#impact-area-classification","title":"Impact Area Classification","text":"<p>Achievements are automatically classified into impact areas:</p> <ul> <li>\ud83d\udd27 Reliability: System stability and uptime improvements</li> <li>\u26a1 Performance: Speed and efficiency optimizations</li> <li>\ud83d\udd12 Security: Security enhancements and compliance</li> <li>\ud83d\udcb0 Cost Efficiency: Resource optimization and savings</li> <li>\ud83d\udc64 User Experience: User-facing improvements</li> <li>\ud83d\udc65 Productivity: Developer and team productivity gains</li> <li>\ud83d\udcc8 Scalability: System scaling and capacity improvements</li> <li>\ud83d\ude80 Innovation: New technologies and methodologies</li> <li>\ud83d\udccb Compliance: Regulatory and policy adherence</li> <li>\u2699\ufe0f Operations: Operational excellence improvements</li> <li>\ud83d\udcca Business Growth: Revenue and business impact</li> <li>\ud83d\udd28 Technical Debt: Code quality and maintenance</li> </ul>"},{"location":"examples/extract_achievements_usage/#metric-extraction","title":"Metric Extraction","text":"<p>The system extracts quantitative metrics directly from the text:</p> <pre><code># Examples of extracted metrics\nachievement.metric_strings = [\n    \"60%\",           # Percentage improvements\n    \"480ms\",         # Latency measurements  \n    \"50+\",           # Quantity indicators\n    \"$100K\",         # Cost savings\n    \"99.9%\",         # Availability metrics\n    \"10x\"            # Multiplier improvements\n]\n</code></pre>"},{"location":"examples/extract_achievements_usage/#collaboration-tracking","title":"Collaboration Tracking","text":"<p>Identifies and tracks collaboration patterns:</p> <pre><code>achievement.collaborators = [\n    \"HR team\",\n    \"Engineering teams\", \n    \"Product Management\",\n    \"Data Science team\"\n]\n</code></pre>"},{"location":"examples/extract_achievements_usage/#timeframe-detection","title":"Timeframe Detection","text":"<p>Automatically detects and extracts timeframes:</p> <pre><code>achievement.timeframe = \"H1 2025\"  # Half-year periods\nachievement.timeframe = \"Q3\"       # Quarter periods  \nachievement.timeframe = \"Summer\"   # Seasonal periods\nachievement.timeframe = \"2024\"     # Annual periods\n</code></pre>"},{"location":"examples/extract_achievements_usage/#customization","title":"Customization","text":""},{"location":"examples/extract_achievements_usage/#input-modification","title":"Input Modification","text":"<p>To use different input text:</p> <pre><code># Option 1: Modify the review file\n# Edit sample_reviews/copy_edited.md\n\n# Option 2: Use different file\nreview_file = project_root / \"sample_reviews\" / \"your_review.md\"\n\n# Option 3: Use inline text\nreview_text = \"\"\"\nYour custom employee review text here...\n\"\"\"\n</code></pre>"},{"location":"examples/extract_achievements_usage/#output-customization","title":"Output Customization","text":"<p>Modify the table display:</p> <pre><code># Add custom columns\ntable.add_column(\"Custom Field\", style=\"cyan\")\n\n# Change color scheme\ntable = Table(\n    title=\"Custom Title\",\n    box=box.SIMPLE,  # Different box style\n    header_style=\"bold green\"  # Different header color\n)\n\n# Custom row formatting\nfor achievement in achievements.items:\n    custom_value = process_achievement(achievement)\n    table.add_row(\n        achievement.title,\n        achievement.outcome,\n        custom_value  # Your custom processing\n    )\n</code></pre>"},{"location":"examples/extract_achievements_usage/#processing-parameters","title":"Processing Parameters","text":"<p>The extraction can be customized through prompt modification:</p> <pre><code># Modify prompts/key_achievements_system_prompt.md to:\n# - Change ranking criteria\n# - Adjust output format\n# - Modify quality thresholds\n# - Add custom instructions\n</code></pre>"},{"location":"examples/extract_achievements_usage/#error-handling","title":"Error Handling","text":""},{"location":"examples/extract_achievements_usage/#comprehensive-error-management","title":"Comprehensive Error Management","text":"<pre><code>try:\n    achievements = modifier.extract_achievements(text=review_text)\nexcept ValidationError as e:\n    console.print(f\"\u274c Input validation failed: {e}\", style=\"bold red\")\nexcept PostconditionError as e:\n    console.print(f\"\u274c Processing failed: {e}\", style=\"bold red\")\nexcept FileNotFoundError as e:\n    console.print(f\"\u274c Sample file not found: {e}\", style=\"bold red\")\nexcept Exception as e:\n    console.print(f\"\u274c Unexpected error: {e}\", style=\"bold red\")\n    logger.exception(\"Unexpected error in main execution\")\n</code></pre>"},{"location":"examples/extract_achievements_usage/#graceful-degradation","title":"Graceful Degradation","text":"<p>The script handles various edge cases:</p> <ul> <li>Empty input text: Provides helpful error message</li> <li>No achievements found: Displays appropriate message</li> <li>File access issues: Suggests alternative approaches</li> <li>API failures: Provides debugging information</li> </ul>"},{"location":"examples/extract_achievements_usage/#performance-considerations","title":"Performance Considerations","text":""},{"location":"examples/extract_achievements_usage/#token-usage","title":"Token Usage","text":"<ul> <li>Typical usage: 100-500 tokens per review</li> <li>Large reviews: May use 1000+ tokens</li> <li>Cost estimation: Monitor OpenAI API usage</li> </ul>"},{"location":"examples/extract_achievements_usage/#processing-time","title":"Processing Time","text":"<ul> <li>Simple reviews: 2-5 seconds</li> <li>Complex reviews: 5-15 seconds</li> <li>Network dependent: API latency affects total time</li> </ul>"},{"location":"examples/extract_achievements_usage/#memory-usage","title":"Memory Usage","text":"<ul> <li>Lightweight: Minimal memory footprint</li> <li>Scalable: Can process multiple reviews sequentially</li> <li>Efficient: Proper cleanup of temporary objects</li> </ul>"},{"location":"examples/extract_achievements_usage/#integration-examples","title":"Integration Examples","text":""},{"location":"examples/extract_achievements_usage/#batch-processing","title":"Batch Processing","text":"<pre><code>def process_multiple_reviews():\n    modifier = TextModifiers()\n    results = []\n\n    for review_file in review_files:\n        try:\n            text = load_review(review_file)\n            achievements = modifier.extract_achievements(text=text)\n            results.append({\n                'file': review_file,\n                'achievements': achievements\n            })\n        except Exception as e:\n            logger.error(f\"Failed to process {review_file}: {e}\")\n\n    return results\n</code></pre>"},{"location":"examples/extract_achievements_usage/#api-integration","title":"API Integration","text":"<pre><code>from fastapi import FastAPI\nfrom metamorphosis.mcp.text_modifiers import TextModifiers\n\napp = FastAPI()\nmodifier = TextModifiers()\n\n@app.post(\"/extract-achievements\")\nasync def extract_achievements(request: dict):\n    try:\n        achievements = modifier.extract_achievements(\n            text=request[\"review_text\"]\n        )\n        return achievements.model_dump()\n    except Exception as e:\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"examples/extract_achievements_usage/#data-pipeline-integration","title":"Data Pipeline Integration","text":"<pre><code>def create_achievement_pipeline():\n    \"\"\"Create a data processing pipeline.\"\"\"\n    modifier = TextModifiers()\n\n    def process_review(review_text: str) -&gt; dict:\n        achievements = modifier.extract_achievements(text=review_text)\n        return {\n            'achievements_count': len(achievements.items),\n            'total_tokens': achievements.size,\n            'top_impact_area': get_top_impact_area(achievements),\n            'data': achievements.model_dump()\n        }\n\n    return process_review\n</code></pre>"},{"location":"examples/extract_achievements_usage/#testing","title":"Testing","text":""},{"location":"examples/extract_achievements_usage/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom examples.extract_achievements_usage import (\n    create_achievements_table,\n    write_achievements_to_jsonl\n)\n\ndef test_table_creation():\n    # Create mock achievements\n    achievements = create_mock_achievements()\n\n    # Test table creation\n    table = create_achievements_table(achievements)\n    assert table.title\n    assert len(table.columns) == 5\n\ndef test_jsonl_output(tmp_path):\n    achievements = create_mock_achievements()\n    output_file = tmp_path / \"test_output.jsonl\"\n\n    write_achievements_to_jsonl(achievements, output_file)\n\n    assert output_file.exists()\n    # Verify content is valid JSON\n    import json\n    with output_file.open() as f:\n        data = json.load(f)\n        assert \"items\" in data\n</code></pre>"},{"location":"examples/extract_achievements_usage/#integration-tests","title":"Integration Tests","text":"<pre><code>def test_full_extraction_pipeline():\n    \"\"\"Test the complete extraction process.\"\"\"\n    from examples.extract_achievements_usage import main\n\n    # Mock the input and run the main function\n    # Verify output files are created\n    # Check console output formatting\n</code></pre>"},{"location":"examples/extract_achievements_usage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/extract_achievements_usage/#common-issues","title":"Common Issues","text":"<ol> <li>No achievements extracted:</li> <li>Review text may be too vague or activity-focused</li> <li>Try text with more concrete outcomes and metrics</li> <li> <p>Check the prompt templates for guidance</p> </li> <li> <p>Low-quality extractions:</p> </li> <li>Input text lacks specific details</li> <li>Missing quantitative information</li> <li> <p>No clear impact statements</p> </li> <li> <p>API errors:</p> </li> <li>Verify OpenAI API key is valid</li> <li>Check rate limits and quotas</li> <li>Monitor network connectivity</li> </ol>"},{"location":"examples/extract_achievements_usage/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging:</p> <pre><code>import logging\nfrom loguru import logger\n\n# Add debug logging\nlogger.add(\"achievement_debug.log\", level=\"DEBUG\")\nlogging.basicConfig(level=logging.DEBUG)\n\n# Run the script - all operations will be logged\n</code></pre>"},{"location":"examples/extract_achievements_usage/#see-also","title":"See Also","text":"<ul> <li>TextModifiers Class - Core achievement extraction implementation</li> <li>Data Models - Achievement and AchievementsList schemas</li> <li>Other Examples - Additional usage examples</li> <li>MCP Package - Text processing architecture</li> </ul> <p>This documentation is automatically generated from the source code and maintained in sync with the implementation.</p>"},{"location":"examples/rationalize_usage/","title":"Copy Editor Usage","text":""},{"location":"examples/rationalize_usage/#copy-editor-rationalize-usage-example","title":"Copy Editor (Rationalize) Usage Example","text":""},{"location":"examples/rationalize_usage/#examples.rationalize_usage.pretty_print_comparison","title":"<code>pretty_print_comparison(original, rationalized)</code>","text":"<p>Pretty print a side-by-side comparison of original vs rationalized text.</p> <p>Parameters:</p> Name Type Description Default <code>original</code> <code>str</code> <p>The original text before rationalization.</p> required <code>rationalized</code> <code>str</code> <p>The rationalized text after processing.</p> required Source code in <code>src/examples/rationalize_usage.py</code> <pre><code>def pretty_print_comparison(original: str, rationalized: str) -&gt; None:\n    \"\"\"Pretty print a side-by-side comparison of original vs rationalized text.\n\n    Args:\n        original: The original text before rationalization.\n        rationalized: The rationalized text after processing.\n    \"\"\"\n    print(\"=\" * 100)\n    print(\"TEXT RATIONALIZATION COMPARISON\")\n    print(\"=\" * 100)\n    print()\n\n    print(\"\ud83d\udd34 ORIGINAL TEXT (Raw Draft):\")\n    print(\"-\" * 50)\n    print(original)\n    print()\n\n    print(\"\ud83d\udfe2 RATIONALIZED TEXT (Copy-Edited):\")\n    print(\"-\" * 50)\n    print(rationalized)\n    print()\n\n    print(\"\ud83d\udcca STATISTICS:\")\n    print(\"-\" * 50)\n    print(f\"Original length:     {len(original):,} characters\")\n    print(f\"Rationalized length: {len(rationalized):,} characters\")\n    print(f\"Length difference:   {len(rationalized) - len(original):+,} characters\")\n    print(\n        f\"Change percentage:   {((len(rationalized) - len(original)) / len(original) * 100):+.1f}%\"\n    )\n    print()\n</code></pre>"},{"location":"examples/rationalize_usage/#examples.rationalize_usage.rationalize_data_engineer_review","title":"<code>rationalize_data_engineer_review()</code>","text":"<p>Rationalize the sample data engineer self-review using TextModifiers.</p> <p>This function demonstrates the text rationalization capability by processing a raw, intentionally messy self-review that contains typical issues found in employee self-reviews pasted from personal docs or Slack:</p> <ul> <li>Spelling errors and typos</li> <li>Inconsistent formatting and punctuation</li> <li>Casual shorthand and informal language</li> <li>Grammar issues and inconsistent capitalization</li> </ul> <p>The rationalization process will clean up these issues while preserving the original meaning, structure, and all numerical data.</p> <p>Returns:</p> Name Type Description <code>CopyEditedText</code> <code>CopyEditedText</code> <p>Structured result containing the rationalized text,            token count, and whether changes were made.</p> Source code in <code>src/examples/rationalize_usage.py</code> <pre><code>@validate_call\ndef rationalize_data_engineer_review() -&gt; CopyEditedText:\n    \"\"\"Rationalize the sample data engineer self-review using TextModifiers.\n\n    This function demonstrates the text rationalization capability by processing\n    a raw, intentionally messy self-review that contains typical issues found\n    in employee self-reviews pasted from personal docs or Slack:\n\n    - Spelling errors and typos\n    - Inconsistent formatting and punctuation\n    - Casual shorthand and informal language\n    - Grammar issues and inconsistent capitalization\n\n    The rationalization process will clean up these issues while preserving\n    the original meaning, structure, and all numerical data.\n\n    Returns:\n        CopyEditedText: Structured result containing the rationalized text,\n                       token count, and whether changes were made.\n    \"\"\"\n    project_root = get_project_root()\n    review_path = project_root / \"sample_reviews\" / \"data_engineer_review.md\"\n    logger.info(\"Reading data engineer review from: {}\", review_path)\n\n    # Read the raw review text\n    raw_text = read_text_file(review_path)\n\n    # Extract just the review content (skip the explanatory header)\n    # Find the actual review content starting from the role line\n    lines = raw_text.split(\"\\n\")\n    review_start_idx = None\n    for i, line in enumerate(lines):\n        if line.strip().startswith(\"**role**:\"):\n            review_start_idx = i\n            break\n\n    if review_start_idx is not None:\n        # Take everything from the role line onwards\n        review_content = \"\\n\".join(lines[review_start_idx:])\n    else:\n        # Fallback to using the entire content\n        review_content = raw_text\n\n    # Initialize the TextModifiers and perform rationalization\n    modifiers = TextModifiers()\n    logger.info(\"Running text rationalization (input length: {} chars)\", len(review_content))\n\n    result = modifiers.rationalize_text(text=review_content)\n\n    logger.info(\"Rationalization completed:\")\n    logger.info(\"- Output length: {} chars\", len(result.copy_edited_text))\n    logger.info(\"- Token count: {}\", result.size)\n    logger.info(\"- Text was edited: {}\", result.is_edited)\n\n    return result\n</code></pre>"},{"location":"examples/review_text_evaluator_usage/","title":"Quality Evaluation","text":""},{"location":"examples/review_text_evaluator_usage/#review-text-evaluator-usage-example","title":"Review Text Evaluator Usage Example","text":"<p>Example usage of the TextModifiers.evaluate_review_text method.</p> <p>This script demonstrates how to: 1. Load a sample employee review from the sample_reviews directory 2. Evaluate the writing quality using the TextModifiers class 3. Display the results in a beautiful rich table format 4. Save the structured evaluation data to a JSONL file</p> Run this script from the project root <p>python -m src.examples.review_text_evaluator_usage</p>"},{"location":"examples/review_text_evaluator_usage/#examples.review_text_evaluator_usage.load_sample_review","title":"<code>load_sample_review()</code>","text":"<p>Load the sample copy-edited review from sample_reviews directory.</p> <p>Returns:</p> Type Description <code>str</code> <p>The content of the copy_edited.md file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the sample review file doesn't exist.</p> Source code in <code>src/examples/review_text_evaluator_usage.py</code> <pre><code>def load_sample_review() -&gt; str:\n    \"\"\"Load the sample copy-edited review from sample_reviews directory.\n\n    Returns:\n        The content of the copy_edited.md file.\n\n    Raises:\n        FileNotFoundError: If the sample review file doesn't exist.\n    \"\"\"\n    project_root = get_project_root()\n    sample_file = project_root / \"sample_reviews\" / \"copy_edited.md\"\n\n    if not sample_file.exists():\n        raise FileNotFoundError(f\"Sample review file not found: {sample_file}\")\n\n    return sample_file.read_text(encoding=\"utf-8\")\n</code></pre>"},{"location":"examples/review_text_evaluator_usage/#examples.review_text_evaluator_usage.main","title":"<code>main()</code>","text":"<p>Main function that demonstrates review text evaluation.</p> Source code in <code>src/examples/review_text_evaluator_usage.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main function that demonstrates review text evaluation.\"\"\"\n    console = Console()\n\n    try:\n        # Display header\n        console.print(\"\\n\")\n        console.print(\n            Panel.fit(\n                \"\ud83d\udcca Review Text Quality Evaluation Demo\\n\\n\"\n                \"This example loads a sample employee review and evaluates\\n\"\n                \"its writing quality using the TextModifiers.evaluate_review_text method.\",\n                title=\"Metamorphosis Text Processing\",\n                style=\"bold blue\",\n            )\n        )\n\n        # Load the sample review\n        console.print(\"\\n\ud83d\udcd6 Loading sample review from sample_reviews/copy_edited.md...\")\n        review_text = load_sample_review()\n        console.print(f\"\u2705 Loaded review text ({len(review_text):,} characters)\")\n\n        # Initialize TextModifiers\n        console.print(\"\\n\ud83e\udd16 Initializing TextModifiers with LLM clients...\")\n        modifier = TextModifiers()\n        console.print(\"\u2705 TextModifiers initialized successfully\")\n\n        # Evaluate review text\n        console.print(\"\\n\ud83d\udcca Evaluating review text quality...\")\n        with console.status(\"[bold green]Processing with LLM...\"):\n            evaluation = modifier.evaluate_review_text(text=review_text)\n\n        console.print(\"\u2705 Evaluation completed successfully!\")\n\n        # Write evaluation to JSONL file\n        console.print(\"\\n\ud83d\udcdd Writing evaluation results to JSONL file...\")\n        project_root = get_project_root()\n        jsonl_output_path = project_root / \"sample_reviews\" / \"text_evaluator_results.jsonl\"\n        write_evaluation_to_jsonl(evaluation, jsonl_output_path)\n        console.print(f\"\u2705 Saved evaluation results to: {jsonl_output_path}\")\n\n        # Display results\n        console.print(\"\\n\")\n        console.print(create_summary_panel_evaluation(evaluation))\n        console.print(\"\\n\")\n        console.print(create_metrics_table(evaluation))\n        console.print(\"\\n\")\n        console.print(create_radar_chart_info(evaluation))\n\n        # Display integration notes\n        console.print(\"\\n\")\n        console.print(\n            Panel(\n                \"\ud83d\udca1 Tip: The evaluation results are returned as structured Pydantic objects\\n\"\n                \"that can be easily integrated into HR systems or quality dashboards.\\n\\n\"\n                \"Each MetricScore contains: name, score (0-100), rationale, and suggestion.\\n\"\n                \"The ReviewScorecard includes: metrics, overall score, verdict, notes, \"\n                \"and radar data.\\n\\n\"\n                f\"\ud83d\udcc4 The complete evaluation data has been saved to: {jsonl_output_path.name}\",\n                title=\"\u2139\ufe0f  Integration Notes\",\n                style=\"dim cyan\",\n            )\n        )\n\n    except FileNotFoundError as e:\n        console.print(f\"\u274c Error: {e}\", style=\"bold red\")\n        sys.exit(1)\n    except Exception as e:\n        logger.exception(\"Unexpected error during review text evaluation\")\n        console.print(f\"\u274c Unexpected error: {e}\", style=\"bold red\")\n        sys.exit(1)\n</code></pre>"},{"location":"examples/review_text_evaluator_usage/#examples.review_text_evaluator_usage.write_evaluation_to_jsonl","title":"<code>write_evaluation_to_jsonl(scorecard, output_path)</code>","text":"<p>Write the evaluation results to a JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>scorecard</code> <code>ReviewScorecard</code> <p>The ReviewScorecard object containing evaluation results.</p> required <code>output_path</code> <code>Path</code> <p>Path to the output JSONL file.</p> required Source code in <code>src/examples/review_text_evaluator_usage.py</code> <pre><code>def write_evaluation_to_jsonl(scorecard: ReviewScorecard, output_path: Path) -&gt; None:\n    \"\"\"Write the evaluation results to a JSONL file.\n\n    Args:\n        scorecard: The ReviewScorecard object containing evaluation results.\n        output_path: Path to the output JSONL file.\n    \"\"\"\n    # Create the directory if it doesn't exist\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Convert the ReviewScorecard to dict and write to JSONL\n    with output_path.open(\"w\", encoding=\"utf-8\") as f:\n        # Write the full ReviewScorecard object as one JSON line\n        scorecard_dict = scorecard.model_dump()\n        f.write(json.dumps(scorecard_dict, ensure_ascii=False) + \"\\n\")\n\n    logger.debug(\"Wrote evaluation results to JSONL file: {}\", output_path)\n</code></pre>"},{"location":"examples/summarizer_usage/","title":"Summarizer Usage","text":""},{"location":"examples/summarizer_usage/#summarizer-usage-example","title":"Summarizer Usage Example","text":""},{"location":"examples/summarizer_usage/#examples.summarizer_usage.summarize_raw_review","title":"<code>summarize_raw_review(max_words=200)</code>","text":"<p>Summarize the sample self-review markdown using TextModifiers.</p> <p>Parameters:</p> Name Type Description Default <code>max_words</code> <code>Annotated[PositiveInt, Field(gt=0)]</code> <p>Target maximum words for the summary (best-effort).</p> <code>200</code> <p>Returns:</p> Name Type Description <code>SummarizedText</code> <code>SummarizedText</code> <p>Structured summary result.</p> Source code in <code>src/examples/summarizer_usage.py</code> <pre><code>@validate_call\ndef summarize_raw_review(max_words: Annotated[PositiveInt, Field(gt=0)] = 200) -&gt; SummarizedText:\n    \"\"\"Summarize the sample self-review markdown using TextModifiers.\n\n    Args:\n        max_words: Target maximum words for the summary (best-effort).\n\n    Returns:\n        SummarizedText: Structured summary result.\n    \"\"\"\n    project_root = get_project_root()\n    raw_path = project_root / \"sample_reviews\" / \"raw_review.md\"\n    logger.info(\"Reading review from: {}\", raw_path)\n\n    # -------------------------------------------------------------------------\n\n    # Create TextModifiers instance\n    modifiers = TextModifiers()\n\n    # Log model details for summarizer before running\n    modifiers._log_model_details_table(method=\"summarize\")\n\n    raw_text = read_text_file(raw_path)\n    logger.info(\"Running summarization (max_words={})\", max_words)\n    result = modifiers.summarize(text=raw_text, max_words=int(max_words))\n\n    logger.info(\"Summary size (tokens reported): {}\", result.size)\n    logger.info(\"Summary text:\\n{}\", result.summarized_text)\n    return result\n</code></pre>"},{"location":"examples/visualize_evaluation_radar/","title":"Radar Charts","text":""},{"location":"examples/visualize_evaluation_radar/#visualize-evaluation-radar-example","title":"Visualize Evaluation Radar Example","text":"<p>Create radar plot visualization from review evaluation results.</p> <p>This script demonstrates how to: 1. Load evaluation results from the text_evaluator_results.jsonl file 2. Create an interactive radar plot using Plotly 3. Display the chart in browser and save as HTML file</p> Run this script from the project root <p>python -m src.examples.visualize_evaluation_radar</p>"},{"location":"examples/visualize_evaluation_radar/#examples.visualize_evaluation_radar.create_summary_info","title":"<code>create_summary_info(evaluation_data)</code>","text":"<p>Create a summary text from evaluation data.</p> <p>Parameters:</p> Name Type Description Default <code>evaluation_data</code> <code>dict</code> <p>Dictionary containing evaluation results.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted summary string.</p> Source code in <code>src/examples/visualize_evaluation_radar.py</code> <pre><code>def create_summary_info(evaluation_data: dict) -&gt; str:\n    \"\"\"Create a summary text from evaluation data.\n\n    Args:\n        evaluation_data: Dictionary containing evaluation results.\n\n    Returns:\n        Formatted summary string.\n    \"\"\"\n    metrics_info = []\n    for metric in evaluation_data[\"metrics\"]:\n        metrics_info.append(f\"  \u2022 {metric['name']}: {metric['score']}/100\")\n\n    summary = f\"\"\"\n\ud83d\udcca Evaluation Summary:\n  Overall Score: {evaluation_data[\"overall\"]}/100\n  Verdict: {evaluation_data[\"verdict\"].title()}\n\n\ud83d\udcc8 Individual Metrics:\n{chr(10).join(metrics_info)}\n\n\ud83c\udff7\ufe0f  Quality Flags: {len(evaluation_data[\"notes\"])} detected\n\"\"\"\n\n    if evaluation_data[\"notes\"]:\n        summary += f\"   \u2022 {', '.join(evaluation_data['notes'])}\\n\"\n    else:\n        summary += \"   \u2022 No issues detected\\n\"\n\n    return summary\n</code></pre>"},{"location":"examples/visualize_evaluation_radar/#examples.visualize_evaluation_radar.load_evaluation_results","title":"<code>load_evaluation_results(jsonl_path)</code>","text":"<p>Load evaluation results from JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>jsonl_path</code> <code>Path</code> <p>Path to the text_evaluator_results.jsonl file.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the evaluation results.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the JSONL file doesn't exist.</p> <code>JSONDecodeError</code> <p>If the file contains invalid JSON.</p> Source code in <code>src/examples/visualize_evaluation_radar.py</code> <pre><code>def load_evaluation_results(jsonl_path: Path) -&gt; dict:\n    \"\"\"Load evaluation results from JSONL file.\n\n    Args:\n        jsonl_path: Path to the text_evaluator_results.jsonl file.\n\n    Returns:\n        Dictionary containing the evaluation results.\n\n    Raises:\n        FileNotFoundError: If the JSONL file doesn't exist.\n        json.JSONDecodeError: If the file contains invalid JSON.\n    \"\"\"\n    if not jsonl_path.exists():\n        raise FileNotFoundError(f\"Evaluation results file not found: {jsonl_path}\")\n\n    with jsonl_path.open(\"r\", encoding=\"utf-8\") as f:\n        # Read the single line JSON object\n        line = f.readline().strip()\n        if not line:\n            raise ValueError(\"Empty evaluation results file\")\n        return json.loads(line)\n</code></pre>"},{"location":"examples/visualize_evaluation_radar/#examples.visualize_evaluation_radar.main","title":"<code>main()</code>","text":"<p>Main function that creates the radar plot visualization.</p> Source code in <code>src/examples/visualize_evaluation_radar.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main function that creates the radar plot visualization.\"\"\"\n    console = Console()\n\n    try:\n        # Display header\n        console.print(\"\\n\")\n        console.print(\n            Panel.fit(\n                \"\ud83d\udcca Review Evaluation Radar Plot Visualization\\n\\n\"\n                \"This script creates an interactive radar chart from the\\n\"\n                \"text_evaluator_results.jsonl evaluation data using Plotly.\",\n                title=\"Metamorphosis Visualization\",\n                style=\"bold blue\",\n            )\n        )\n\n        # Load evaluation results\n        console.print(\"\\n\ud83d\udcd6 Loading evaluation results from text_evaluator_results.jsonl...\")\n        project_root = get_project_root()\n        jsonl_path = project_root / \"sample_reviews\" / \"text_evaluator_results.jsonl\"\n        evaluation_data = load_evaluation_results(jsonl_path)\n        console.print(\"\u2705 Loaded evaluation results successfully\")\n\n        # Display summary\n        summary_text = create_summary_info(evaluation_data)\n        console.print(Panel(summary_text, title=\"\ud83d\udccb Evaluation Data\", style=\"dim blue\"))\n\n        # Create radar plot\n        console.print(\"\\n\ud83d\udcca Creating radar plot visualization...\")\n        fig = create_radar_plot(evaluation_data)\n        console.print(\"\u2705 Radar plot created successfully\")\n\n        # Save and display\n        console.print(\"\\n\ud83d\udcbe Saving plot and opening in browser...\")\n        output_path = project_root / \"sample_reviews\" / \"evaluation_radar_plot.html\"\n        save_and_display_plot(fig, output_path)\n        console.print(f\"\u2705 Radar plot saved to: {output_path}\")\n        console.print(\"\u2705 Interactive plot opened in your default browser\")\n\n        # Display final info\n        console.print(\"\\n\")\n        console.print(\n            Panel(\n                \"\ud83d\udca1 Tip: The radar plot is interactive! You can:\\n\"\n                \"  \u2022 Hover over data points to see detailed scores\\n\"\n                \"  \u2022 Compare current performance vs. the 90% writing goal (orange dashed line)\\n\"\n                \"  \u2022 Use the toolbar to zoom, pan, and download the chart\\n\"\n                \"  \u2022 Toggle traces on/off using the legend\\n\"\n                \"  \u2022 The HTML file can be shared or embedded in reports\\n\\n\"\n                f\"\ud83c\udfaf The orange dashed polygon shows the 90% writing goal target\\n\"\n                f\"\ud83d\udcc4 Interactive chart saved as: {output_path.name}\",\n                title=\"\u2139\ufe0f  Visualization Notes\",\n                style=\"dim cyan\",\n            )\n        )\n\n    except FileNotFoundError as e:\n        console.print(f\"\u274c Error: {e}\", style=\"bold red\")\n        console.print(\n            \"\\n\ud83d\udca1 Hint: Run the review_text_evaluator_usage.py script first to generate the evaluation results.\",\n            style=\"dim yellow\",\n        )\n        sys.exit(1)\n    except json.JSONDecodeError as e:\n        console.print(f\"\u274c JSON Error: {e}\", style=\"bold red\")\n        console.print(\"The evaluation results file appears to be corrupted.\", style=\"dim red\")\n        sys.exit(1)\n    except Exception as e:\n        logger.exception(\"Unexpected error during radar plot creation\")\n        console.print(f\"\u274c Unexpected error: {e}\", style=\"bold red\")\n        sys.exit(1)\n</code></pre>"},{"location":"examples/visualize_evaluation_radar/#examples.visualize_evaluation_radar.save_and_display_plot","title":"<code>save_and_display_plot(fig, output_path)</code>","text":"<p>Save the plot as HTML and display in browser.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>The Plotly figure to save and display.</p> required <code>output_path</code> <code>Path</code> <p>Path where to save the HTML file.</p> required Source code in <code>src/examples/visualize_evaluation_radar.py</code> <pre><code>def save_and_display_plot(fig: go.Figure, output_path: Path) -&gt; None:\n    \"\"\"Save the plot as HTML and display in browser.\n\n    Args:\n        fig: The Plotly figure to save and display.\n        output_path: Path where to save the HTML file.\n    \"\"\"\n    # Save as HTML file\n    fig.write_html(str(output_path), include_plotlyjs=\"cdn\")\n    logger.debug(\"Saved radar plot to: {}\", output_path)\n\n    # Display in browser\n    fig.show()\n</code></pre>"},{"location":"metamorphosis/","title":"Metamorphosis Package","text":"<p>The <code>metamorphosis</code> package is the core module that provides AI-powered text processing capabilities for employee self-review workflows. It serves as the central orchestration point for LLM-based text utilities, agent workflows, and user interfaces.</p>"},{"location":"metamorphosis/#package-architecture","title":"Package Architecture","text":"%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#e0f2ff', 'edgeLabelBackground':'#ffffffaa', 'clusterBkg':'#f7fbff', 'fontFamily':'Inter'}}}%% graph TB     subgraph \"metamorphosis Package\"         A[__init__.pyConfiguration &amp; Registry]         B[datamodel.pyPydantic Models]         C[exceptions.pyError Handling]         D[utilities.pyCommon Utilities]         E[model_registry.pyLLM Management]     end      subgraph \"Sub-packages\"         F[mcp/MCP Tools]         G[agents/LangGraph Workflows]         H[ui/User Interfaces]     end      A --&gt; E     A --&gt; B     A --&gt; C     B --&gt; F     E --&gt; F     F --&gt; G     G --&gt; H      subgraph \"External Dependencies\"         I[OpenAI API]         J[LangChain]         K[Pydantic]     end      E --&gt; I     F --&gt; J     B --&gt; K"},{"location":"metamorphosis/#core-components","title":"Core Components","text":""},{"location":"metamorphosis/#configuration-management","title":"Configuration Management","text":"<p>The package provides centralized configuration management through:</p> <ul> <li>Environment Variable Loading: Simple <code>.env</code> style via <code>config.yaml</code></li> <li>Model Registry: Central place to get LLM clients</li> <li>Project Root Resolution: Utility to resolve paths reliably</li> </ul>"},{"location":"metamorphosis/#data-models","title":"Data Models","text":"<p>Type-safe Pydantic models for data handling:</p> <ul> <li>Processing Results: Structured outputs for text processing</li> <li>Request/Response Models: API contracts</li> <li>Workflow State: LangGraph state schemas</li> </ul>"},{"location":"metamorphosis/#error-handling","title":"Error Handling","text":"<p>Clear exception types with helpful context. This keeps examples easy to debug and learn from.</p>"},{"location":"metamorphosis/#module-documentation","title":"Module Documentation","text":""},{"location":"metamorphosis/#core-modules","title":"Core Modules","text":"Module Description Key Components <code>__init__.py</code> Package initialization and configuration <code>_load_config()</code>, <code>get_model_registry()</code> <code>datamodel.py</code> Pydantic data models <code>SummarizedText</code>, <code>CopyEditedText</code>, <code>AchievementsList</code> <code>exceptions.py</code> Exception hierarchy <code>PostconditionError</code>, <code>ValidationError</code>, <code>MCPToolError</code> <code>utilities.py</code> Common utility functions <code>get_project_root()</code>, <code>read_text_file()</code> <code>model_registry.py</code> LLM client management <code>ModelRegistry</code>, <code>_LLMSettings</code>"},{"location":"metamorphosis/#sub-packages","title":"Sub-packages","text":"Package Purpose Documentation <code>mcp/</code> Model Context Protocol integration View Details \u2192 <code>agents/</code> LangGraph agent workflows (see <code>self_reviewer/</code>) View Details \u2192 <code>ui/</code> Streamlit user interfaces View Details \u2192"},{"location":"metamorphosis/#usage-patterns","title":"Usage Patterns","text":""},{"location":"metamorphosis/#basic-initialization","title":"Basic Initialization","text":"<pre><code>from metamorphosis import get_model_registry\nfrom metamorphosis.mcp.text_modifiers import TextModifiers\n\n# Get configured LLM clients\nregistry = get_model_registry()\n\n# Initialize text processing utilities\nmodifiers = TextModifiers()\n</code></pre>"},{"location":"metamorphosis/#configuration-access","title":"Configuration Access","text":"<pre><code>from metamorphosis import config\n\n# Access configuration values\nopenai_key = config.get('openai_api_key')\nmodel_settings = config.get('models', {})\n</code></pre>"},{"location":"metamorphosis/#error-handling_1","title":"Error Handling","text":"<pre><code>from metamorphosis.exceptions import PostconditionError, ValidationError\n\ntry:\n    result = modifiers.summarize(text=\"...\")\nexcept ValidationError as e:\n    print(f\"Validation failed: {e}\")\nexcept PostconditionError as e:\n    print(f\"Processing failed: {e}\")\n</code></pre>"},{"location":"metamorphosis/#design-patterns","title":"Design Patterns","text":""},{"location":"metamorphosis/#singleton-registry","title":"Singleton Registry","text":"<p>The <code>ModelRegistry</code> implements a lightweight singleton to keep code simple:</p> <pre><code>class ModelRegistry:\n    _instance = None\n    _initialized = False\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n</code></pre>"},{"location":"metamorphosis/#validation-decorators","title":"Validation Decorators","text":"<p>Pydantic <code>@validate_call</code> is used for friendly runtime checks in examples:</p> <pre><code>@validate_call\ndef process_text(\n    text: Annotated[str, Field(min_length=1)],\n    max_words: Annotated[int, Field(gt=0)] = 300\n) -&gt; SummarizedText:\n    # Implementation with automatic validation\n</code></pre>"},{"location":"metamorphosis/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Make sure <code>PYTHONPATH</code> includes <code>src</code> (already set in MkDocs config)</li> <li>Check that environment variables like <code>OPENAI_API_KEY</code> are available</li> <li>Verify dependencies are installed</li> </ul>"},{"location":"metamorphosis/#see-also","title":"See Also","text":"<ul> <li>MCP Package Documentation - Text processing tools</li> <li>Agents Package Documentation - Workflow orchestration (<code>self_reviewer/</code>)</li> <li>UI Package Documentation - User interfaces</li> <li>Examples - Usage examples and tutorials</li> </ul> <p>This project is a learning\u2011friendly starter for exploring AI agents with LangGraph, MCP tools, and Streamlit. The goal is clarity over complexity.</p>"},{"location":"metamorphosis/datamodel/","title":"Data Models","text":""},{"location":"metamorphosis/datamodel/#metamorphosis.datamodel.CopyEditedText","title":"<code>CopyEditedText</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Rationalized text with typos and grammar errors corrected.</p> Source code in <code>src/metamorphosis/datamodel.py</code> <pre><code>class CopyEditedText(BaseModel):\n    \"\"\"Rationalized text with typos and grammar errors corrected.\"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    copy_edited_text: str = Field(\n        ..., description=\"The lightly normalized and corrected text\", min_length=1\n    )\n    size: int = Field(..., description=\"The size of the copy-edited text in tokens\")\n    is_edited: bool = Field(..., description=\"Whether the text was edited\")\n</code></pre>"},{"location":"metamorphosis/datamodel/#metamorphosis.datamodel.InvokeRequest","title":"<code>InvokeRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request model for synchronous self-review processing (/invoke).</p> Source code in <code>src/metamorphosis/datamodel.py</code> <pre><code>class InvokeRequest(BaseModel):\n    \"\"\"Request model for synchronous self-review processing (/invoke).\"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    review_text: str = Field(\n        ...,\n        min_length=1,\n        description=\"The review text to process through the LangGraph\",\n        example=(\n            \"I had an eventful cycle this summer. Learnt agentic workflows and implemented a \"\n            \"self-reviewer agent for the periodic employee self-review process. It significantly \"\n            \"improved employee productivity for the organization.\"\n        ),\n    )\n    thread_id: str | None = Field(\n        None,\n        description=(\n            \"Optional thread ID for state persistence. If not provided, a new UUID will be generated.\"\n        ),\n        example=\"thread_123\",\n    )\n</code></pre>"},{"location":"metamorphosis/datamodel/#metamorphosis.datamodel.InvokeResponse","title":"<code>InvokeResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response model for synchronous self-review processing results.</p> Source code in <code>src/metamorphosis/datamodel.py</code> <pre><code>class InvokeResponse(BaseModel):\n    \"\"\"Response model for synchronous self-review processing results.\"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    original_text: str = Field(..., description=\"The original review text\")\n    copy_edited_text: str | None = Field(None, description=\"The copy-edited text\")\n    summary: str | None = Field(None, description=\"The summary of the copy-edited text\")\n    word_cloud_path: str | None = Field(None, description=\"The path to the word cloud image\")\n    achievements: AchievementsList | None = Field(None, description=\"The achievements list\")\n    review_scorecard: ReviewScorecard | None = Field(None, description=\"The review scorecard\")\n    review_complete: bool | None = Field(None, description='''\n    A flag indicating if the review is complete, ie, \n    the list of achievements has at least 3 items\n    ''')\n</code></pre>"},{"location":"metamorphosis/datamodel/#metamorphosis.datamodel.StreamRequest","title":"<code>StreamRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request model for streaming self-review processing via SSE (/stream).</p> Source code in <code>src/metamorphosis/datamodel.py</code> <pre><code>class StreamRequest(BaseModel):\n    \"\"\"Request model for streaming self-review processing via SSE (/stream).\"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    review_text: str = Field(\n        ..., min_length=1, description=\"The review text to process through the LangGraph\"\n    )\n    thread_id: str = Field(\n        ..., min_length=1, description=\"Unique identifier for the conversation thread\"\n    )\n    mode: str = Field(\n        \"values\",\n        pattern=r\"^(values|updates)$\",\n        description=(\n            \"Streaming mode - 'updates' for state changes only, 'values' for full state each step\"\n        ),\n    )\n</code></pre>"},{"location":"metamorphosis/datamodel/#metamorphosis.datamodel.SummarizedText","title":"<code>SummarizedText</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Structured summary output returned by summarization routines.</p> Source code in <code>src/metamorphosis/datamodel.py</code> <pre><code>class SummarizedText(BaseModel):\n    \"\"\"Structured summary output returned by summarization routines.\"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n    summarized_text: str = Field(\n        ..., description=\"The generated abstractive summary text\", min_length=1\n    )\n    size: int = Field(..., description=\"The size of the summary in tokens\")\n</code></pre>"},{"location":"metamorphosis/datamodel/#overview","title":"Overview","text":"<p>The <code>metamorphosis.datamodel</code> module provides comprehensive Pydantic data models for type-safe handling of text processing inputs, outputs, and workflow state. All models implement strict validation, serialization, and provide rich metadata for debugging and monitoring.</p>"},{"location":"metamorphosis/datamodel/#core-processing-models","title":"Core Processing Models","text":""},{"location":"metamorphosis/datamodel/#text-processing-results","title":"Text Processing Results","text":"classDiagram     class SummarizedText {         +summarized_text: str         +size: int         +unit: str         +model_config: ConfigDict     }      class CopyEditedText {         +copy_edited_text: str         +size: int         +is_edited: bool         +model_config: ConfigDict     }      class Achievement {         +title: str         +outcome: str         +impact_area: ImpactArea         +metric_strings: List[str]         +timeframe: Optional[str]         +ownership_scope: Optional[OwnershipScope]         +collaborators: List[str]     }      class AchievementsList {         +items: List[Achievement]         +size: int         +unit: str     }      class MetricScore {         +name: str         +score: int         +rationale: str         +suggestion: str     }      class ReviewScorecard {         +metrics: List[MetricScore]         +overall: int         +verdict: Verdict         +notes: List[str]         +radar_labels: List[str]         +radar_values: List[int]     }      AchievementsList --&gt; Achievement : contains     ReviewScorecard --&gt; MetricScore : contains"},{"location":"metamorphosis/datamodel/#enumeration-types","title":"Enumeration Types","text":"classDiagram     class ImpactArea {         &lt;&gt;         reliability         performance         security         cost_efficiency         user_experience         developer_productivity         scalability         innovation         compliance         operational_excellence         business_growth         technical_debt         other     }      class OwnershipScope {         &lt;&gt;         IC         TechLead         Manager         Director         VP         Org_wide         Cross_functional         other     }      class Verdict {         &lt;&gt;         excellent         strong         mixed         weak     }      Achievement --&gt; ImpactArea     Achievement --&gt; OwnershipScope     ReviewScorecard --&gt; Verdict"},{"location":"metamorphosis/datamodel/#api-models","title":"API Models","text":""},{"location":"metamorphosis/datamodel/#request-models","title":"Request Models","text":""},{"location":"metamorphosis/datamodel/#invokerequest","title":"InvokeRequest","text":"<p>Used for synchronous workflow execution via the <code>/invoke</code> endpoint.</p>"},{"location":"metamorphosis/datamodel/#streamrequest","title":"StreamRequest","text":"<p>Used for asynchronous streaming workflow execution via the <code>/stream</code> endpoint.</p>"},{"location":"metamorphosis/datamodel/#response-models","title":"Response Models","text":""},{"location":"metamorphosis/datamodel/#invokeresponse","title":"InvokeResponse","text":"<p>Structured response for synchronous processing results.</p>"},{"location":"metamorphosis/datamodel/#workflowstate","title":"WorkflowState","text":"<p>Internal state management for LangGraph workflows.</p>"},{"location":"metamorphosis/datamodel/#usage-examples","title":"Usage Examples","text":""},{"location":"metamorphosis/datamodel/#processing-results","title":"Processing Results","text":"<pre><code>from metamorphosis.datamodel import SummarizedText, CopyEditedText\n\n# Create a summary result\nsummary = SummarizedText(\n    summarized_text=\"Concise summary of the review...\",\n    size=150,\n    unit=\"tokens\"\n)\n\n# Create a copy editing result\nedited = CopyEditedText(\n    copy_edited_text=\"Professionally edited text...\",\n    size=500,\n    is_edited=True\n)\n\n# Access validated data\nprint(f\"Summary: {summary.summarized_text}\")\nprint(f\"Tokens: {summary.size} {summary.unit}\")\nprint(f\"Was edited: {edited.is_edited}\")\n</code></pre>"},{"location":"metamorphosis/datamodel/#achievement-extraction","title":"Achievement Extraction","text":"<pre><code>from metamorphosis.datamodel import Achievement, AchievementsList, ImpactArea\n\n# Create an achievement\nachievement = Achievement(\n    title=\"Reduced checkout latency\",\n    outcome=\"Improved p95 latency from 480ms to 190ms through cache optimization\",\n    impact_area=ImpactArea.performance,\n    metric_strings=[\"480ms\", \"190ms\"],\n    timeframe=\"H1 2025\",\n    ownership_scope=OwnershipScope.TechLead,\n    collaborators=[\"Payments team\", \"SRE team\"]\n)\n\n# Create achievements list\nachievements = AchievementsList(\n    items=[achievement],\n    size=45,\n    unit=\"tokens\"\n)\n</code></pre>"},{"location":"metamorphosis/datamodel/#quality-evaluation","title":"Quality Evaluation","text":"<pre><code>from metamorphosis.datamodel import MetricScore, ReviewScorecard, Verdict\n\n# Create metric scores\nmetrics = [\n    MetricScore(\n        name=\"OutcomeOverActivity\",\n        score=85,\n        rationale=\"Strong focus on concrete outcomes\",\n        suggestion=\"Continue emphasizing results\"\n    ),\n    MetricScore(\n        name=\"QuantitativeSpecificity\",\n        score=90,\n        rationale=\"Excellent use of specific metrics\",\n        suggestion=\"Maintain this level of detail\"\n    )\n]\n\n# Create scorecard\nscorecard = ReviewScorecard(\n    metrics=metrics,\n    overall=87,\n    verdict=Verdict.excellent,\n    notes=[\"strong_metrics\", \"clear_outcomes\"],\n    radar_labels=[\"OutcomeOverActivity\", \"QuantitativeSpecificity\"],\n    radar_values=[85, 90]\n)\n</code></pre>"},{"location":"metamorphosis/datamodel/#validation-features","title":"Validation Features","text":""},{"location":"metamorphosis/datamodel/#automatic-validation","title":"Automatic Validation","text":"<p>All models use Pydantic's automatic validation:</p> <pre><code>from pydantic import ValidationError\n\ntry:\n    # This will fail validation\n    summary = SummarizedText(\n        summarized_text=\"\",  # Empty string not allowed\n        size=-1,  # Negative size not allowed\n        unit=\"\"  # Empty unit not allowed\n    )\nexcept ValidationError as e:\n    print(f\"Validation errors: {e}\")\n</code></pre>"},{"location":"metamorphosis/datamodel/#field-constraints","title":"Field Constraints","text":"<p>Models implement comprehensive field constraints:</p> <pre><code>class Achievement(BaseModel):\n    title: str = Field(..., min_length=1, max_length=120)\n    outcome: str = Field(..., min_length=1, max_length=400)\n    metric_strings: List[str] = Field(default_factory=list)\n    # ... additional constraints\n</code></pre>"},{"location":"metamorphosis/datamodel/#custom-validators","title":"Custom Validators","text":"<p>Some models include custom validation logic:</p> <pre><code>@field_validator(\"overall\")\n@classmethod\ndef validate_overall_score(cls, v: int) -&gt; int:\n    if not (0 &lt;= v &lt;= 100):\n        raise ValueError(\"Overall score must be between 0 and 100\")\n    return v\n</code></pre>"},{"location":"metamorphosis/datamodel/#serialization","title":"Serialization","text":""},{"location":"metamorphosis/datamodel/#json-serialization","title":"JSON Serialization","text":"<p>All models support JSON serialization:</p> <pre><code># Serialize to JSON\njson_data = summary.model_dump()\njson_string = summary.model_dump_json()\n\n# Deserialize from JSON\nsummary_restored = SummarizedText.model_validate(json_data)\nsummary_from_string = SummarizedText.model_validate_json(json_string)\n</code></pre>"},{"location":"metamorphosis/datamodel/#jsonl-output","title":"JSONL Output","text":"<p>Models work seamlessly with JSONL format:</p> <pre><code>import json\n\n# Write to JSONL\nwith open(\"results.jsonl\", \"w\") as f:\n    for result in processing_results:\n        f.write(result.model_dump_json() + \"\\n\")\n\n# Read from JSONL\nresults = []\nwith open(\"results.jsonl\", \"r\") as f:\n    for line in f:\n        data = json.loads(line)\n        result = SummarizedText.model_validate(data)\n        results.append(result)\n</code></pre>"},{"location":"metamorphosis/datamodel/#configuration","title":"Configuration","text":""},{"location":"metamorphosis/datamodel/#model-configuration","title":"Model Configuration","text":"<p>All models use strict configuration:</p> <pre><code>model_config = ConfigDict(\n    str_strip_whitespace=True,  # Auto-strip whitespace\n    validate_assignment=True,   # Validate on assignment\n    extra=\"forbid\",            # Forbid extra fields\n    frozen=False               # Allow field modification\n)\n</code></pre>"},{"location":"metamorphosis/datamodel/#field-documentation","title":"Field Documentation","text":"<p>Models include comprehensive field documentation:</p> <pre><code>class SummarizedText(BaseModel):\n    summarized_text: str = Field(\n        ...,\n        description=\"The generated summary text\",\n        min_length=1\n    )\n    size: int = Field(\n        ...,\n        description=\"Estimated token count of the summary\",\n        ge=0\n    )\n</code></pre>"},{"location":"metamorphosis/datamodel/#testing","title":"Testing","text":""},{"location":"metamorphosis/datamodel/#model-validation-tests","title":"Model Validation Tests","text":"<pre><code>import pytest\nfrom metamorphosis.datamodel import SummarizedText\n\ndef test_summarized_text_validation():\n    # Valid data\n    summary = SummarizedText(\n        summarized_text=\"Valid summary\",\n        size=50,\n        unit=\"tokens\"\n    )\n    assert summary.summarized_text == \"Valid summary\"\n\n    # Invalid data\n    with pytest.raises(ValidationError):\n        SummarizedText(\n            summarized_text=\"\",  # Empty string\n            size=50,\n            unit=\"tokens\"\n        )\n</code></pre>"},{"location":"metamorphosis/datamodel/#serialization-tests","title":"Serialization Tests","text":"<pre><code>def test_json_serialization():\n    original = SummarizedText(\n        summarized_text=\"Test summary\",\n        size=25,\n        unit=\"tokens\"\n    )\n\n    # Round-trip serialization\n    json_data = original.model_dump()\n    restored = SummarizedText.model_validate(json_data)\n\n    assert original == restored\n</code></pre>"},{"location":"metamorphosis/datamodel/#migration-guide","title":"Migration Guide","text":""},{"location":"metamorphosis/datamodel/#from-v0x-to-v10","title":"From v0.x to v1.0","text":"<ol> <li> <p>Update import statements:    <pre><code># Old\nfrom metamorphosis.models import Summary, EditResult\n\n# New\nfrom metamorphosis.datamodel import SummarizedText, CopyEditedText\n</code></pre></p> </li> <li> <p>Update field names:    <pre><code># Old\nresult.summary_text\nresult.token_count\n\n# New\nresult.summarized_text\nresult.size\n</code></pre></p> </li> <li> <p>Handle new validation:    <pre><code># Add proper error handling for validation\ntry:\n    model = SummarizedText(**data)\nexcept ValidationError as e:\n    # Handle validation errors\n    pass\n</code></pre></p> </li> </ol>"},{"location":"metamorphosis/datamodel/#see-also","title":"See Also","text":"<ul> <li>MCP Package - Text processing utilities that use these models</li> <li>Agents Package - Workflow orchestration with state management</li> <li>Examples - Usage examples with real data</li> <li>Pydantic Documentation - Underlying validation framework</li> </ul> <p>This documentation is automatically generated from the source code and maintained in sync with the model definitions.</p>"},{"location":"metamorphosis/exceptions/","title":"Exceptions","text":""},{"location":"metamorphosis/exceptions/#exceptions","title":"Exceptions","text":"<p>Custom exception hierarchy for metamorphosis project.</p> <p>This module defines a comprehensive exception hierarchy that provides contextual error information across all components of the metamorphosis system. All exceptions inherit from ReviewError base class for consistent error handling.</p> <p>Exception Hierarchy: - ReviewError (base)   \u251c\u2500\u2500 ConfigurationError (configuration/environment issues)   \u251c\u2500\u2500 ValidationError (input validation failures)   \u251c\u2500\u2500 ProcessingError (text processing failures)   \u2502   \u251c\u2500\u2500 LLMProcessingError (LLM-specific failures)   \u2502   \u251c\u2500\u2500 PromptError (prompt template issues)   \u2502   \u2514\u2500\u2500 PostconditionError (output validation failures)   \u251c\u2500\u2500 MCPError (MCP protocol/tool issues)   \u2502   \u251c\u2500\u2500 MCPConnectionError (connection failures)   \u2502   \u251c\u2500\u2500 MCPToolError (tool execution failures)   \u2502   \u2514\u2500\u2500 MCPServerError (server-side issues)   \u251c\u2500\u2500 WorkflowError (LangGraph workflow issues)   \u2502   \u251c\u2500\u2500 GraphBuildError (graph construction failures)   \u2502   \u251c\u2500\u2500 NodeExecutionError (node processing failures)   \u2502   \u2514\u2500\u2500 StateError (state management issues)   \u2514\u2500\u2500 FileOperationError (file system operations)</p>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.ConfigurationError","title":"<code>ConfigurationError</code>","text":"<p>               Bases: <code>ReviewError</code></p> <p>Raised when configuration or environment setup fails.</p> <p>This exception is used for missing environment variables, invalid configuration values, or setup issues that prevent the system from initializing properly.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class ConfigurationError(ReviewError):\n    \"\"\"Raised when configuration or environment setup fails.\n\n    This exception is used for missing environment variables, invalid configuration\n    values, or setup issues that prevent the system from initializing properly.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        missing_vars: Optional[list[str]] = None,\n        invalid_vars: Optional[dict[str, str]] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize ConfigurationError with environment context.\n\n        Args:\n            message: Error description.\n            missing_vars: List of missing environment variables.\n            invalid_vars: Dictionary of invalid variable names and their values.\n            **kwargs: Additional ReviewError arguments.\n        \"\"\"\n        context = kwargs.get(\"context\", {})\n        if missing_vars:\n            context[\"missing_variables\"] = missing_vars\n        if invalid_vars:\n            context[\"invalid_variables\"] = invalid_vars\n\n        super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.ConfigurationError.__init__","title":"<code>__init__(message, *, missing_vars=None, invalid_vars=None, **kwargs)</code>","text":"<p>Initialize ConfigurationError with environment context.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description.</p> required <code>missing_vars</code> <code>Optional[list[str]]</code> <p>List of missing environment variables.</p> <code>None</code> <code>invalid_vars</code> <code>Optional[dict[str, str]]</code> <p>Dictionary of invalid variable names and their values.</p> <code>None</code> <code>**kwargs</code> <p>Additional ReviewError arguments.</p> <code>{}</code> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    *,\n    missing_vars: Optional[list[str]] = None,\n    invalid_vars: Optional[dict[str, str]] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize ConfigurationError with environment context.\n\n    Args:\n        message: Error description.\n        missing_vars: List of missing environment variables.\n        invalid_vars: Dictionary of invalid variable names and their values.\n        **kwargs: Additional ReviewError arguments.\n    \"\"\"\n    context = kwargs.get(\"context\", {})\n    if missing_vars:\n        context[\"missing_variables\"] = missing_vars\n    if invalid_vars:\n        context[\"invalid_variables\"] = invalid_vars\n\n    super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.FileOperationError","title":"<code>FileOperationError</code>","text":"<p>               Bases: <code>ReviewError</code></p> <p>Raised when file system operations fail.</p> <p>This exception is used for file reading, writing, or path resolution errors.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class FileOperationError(ReviewError):\n    \"\"\"Raised when file system operations fail.\n\n    This exception is used for file reading, writing, or path resolution errors.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        file_path: Optional[str] = None,\n        operation_type: Optional[str] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize FileOperationError with file context.\n\n        Args:\n            message: Error description.\n            file_path: Path to the file that caused the error.\n            operation_type: Type of operation that failed (read, write, create).\n            **kwargs: Additional ReviewError arguments.\n        \"\"\"\n        context = kwargs.get(\"context\", {})\n        if file_path:\n            context[\"file_path\"] = file_path\n        if operation_type:\n            context[\"operation_type\"] = operation_type\n\n        super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.FileOperationError.__init__","title":"<code>__init__(message, *, file_path=None, operation_type=None, **kwargs)</code>","text":"<p>Initialize FileOperationError with file context.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description.</p> required <code>file_path</code> <code>Optional[str]</code> <p>Path to the file that caused the error.</p> <code>None</code> <code>operation_type</code> <code>Optional[str]</code> <p>Type of operation that failed (read, write, create).</p> <code>None</code> <code>**kwargs</code> <p>Additional ReviewError arguments.</p> <code>{}</code> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    *,\n    file_path: Optional[str] = None,\n    operation_type: Optional[str] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize FileOperationError with file context.\n\n    Args:\n        message: Error description.\n        file_path: Path to the file that caused the error.\n        operation_type: Type of operation that failed (read, write, create).\n        **kwargs: Additional ReviewError arguments.\n    \"\"\"\n    context = kwargs.get(\"context\", {})\n    if file_path:\n        context[\"file_path\"] = file_path\n    if operation_type:\n        context[\"operation_type\"] = operation_type\n\n    super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.GraphBuildError","title":"<code>GraphBuildError</code>","text":"<p>               Bases: <code>WorkflowError</code></p> <p>Raised when LangGraph construction fails.</p> <p>This exception is used for errors during graph building, compilation, or configuration that prevent the workflow from being created.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class GraphBuildError(WorkflowError):\n    \"\"\"Raised when LangGraph construction fails.\n\n    This exception is used for errors during graph building, compilation,\n    or configuration that prevent the workflow from being created.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        graph_type: Optional[str] = None,\n        node_count: Optional[int] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize GraphBuildError with graph context.\n\n        Args:\n            message: Error description.\n            graph_type: Type of graph being built.\n            node_count: Number of nodes in the graph.\n            **kwargs: Additional ReviewError arguments.\n        \"\"\"\n        context = kwargs.get(\"context\", {})\n        if graph_type:\n            context[\"graph_type\"] = graph_type\n        if node_count is not None:\n            context[\"node_count\"] = node_count\n\n        super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.GraphBuildError.__init__","title":"<code>__init__(message, *, graph_type=None, node_count=None, **kwargs)</code>","text":"<p>Initialize GraphBuildError with graph context.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description.</p> required <code>graph_type</code> <code>Optional[str]</code> <p>Type of graph being built.</p> <code>None</code> <code>node_count</code> <code>Optional[int]</code> <p>Number of nodes in the graph.</p> <code>None</code> <code>**kwargs</code> <p>Additional ReviewError arguments.</p> <code>{}</code> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    *,\n    graph_type: Optional[str] = None,\n    node_count: Optional[int] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize GraphBuildError with graph context.\n\n    Args:\n        message: Error description.\n        graph_type: Type of graph being built.\n        node_count: Number of nodes in the graph.\n        **kwargs: Additional ReviewError arguments.\n    \"\"\"\n    context = kwargs.get(\"context\", {})\n    if graph_type:\n        context[\"graph_type\"] = graph_type\n    if node_count is not None:\n        context[\"node_count\"] = node_count\n\n    super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.LLMProcessingError","title":"<code>LLMProcessingError</code>","text":"<p>               Bases: <code>ProcessingError</code></p> <p>Raised when LLM processing fails.</p> <p>This exception is used for LLM-specific errors such as API failures, token limits, model unavailability, or unexpected LLM responses.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class LLMProcessingError(ProcessingError):\n    \"\"\"Raised when LLM processing fails.\n\n    This exception is used for LLM-specific errors such as API failures,\n    token limits, model unavailability, or unexpected LLM responses.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        model_name: Optional[str] = None,\n        prompt_length: Optional[int] = None,\n        response_length: Optional[int] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize LLMProcessingError with LLM context.\n\n        Args:\n            message: Error description.\n            model_name: Name of the LLM model that failed.\n            prompt_length: Length of the prompt that was sent.\n            response_length: Length of the response received (if any).\n            **kwargs: Additional ReviewError arguments.\n        \"\"\"\n        context = kwargs.get(\"context\", {})\n        if model_name:\n            context[\"model\"] = model_name\n        if prompt_length is not None:\n            context[\"prompt_length\"] = prompt_length\n        if response_length is not None:\n            context[\"response_length\"] = response_length\n\n        super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.LLMProcessingError.__init__","title":"<code>__init__(message, *, model_name=None, prompt_length=None, response_length=None, **kwargs)</code>","text":"<p>Initialize LLMProcessingError with LLM context.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description.</p> required <code>model_name</code> <code>Optional[str]</code> <p>Name of the LLM model that failed.</p> <code>None</code> <code>prompt_length</code> <code>Optional[int]</code> <p>Length of the prompt that was sent.</p> <code>None</code> <code>response_length</code> <code>Optional[int]</code> <p>Length of the response received (if any).</p> <code>None</code> <code>**kwargs</code> <p>Additional ReviewError arguments.</p> <code>{}</code> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    *,\n    model_name: Optional[str] = None,\n    prompt_length: Optional[int] = None,\n    response_length: Optional[int] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize LLMProcessingError with LLM context.\n\n    Args:\n        message: Error description.\n        model_name: Name of the LLM model that failed.\n        prompt_length: Length of the prompt that was sent.\n        response_length: Length of the response received (if any).\n        **kwargs: Additional ReviewError arguments.\n    \"\"\"\n    context = kwargs.get(\"context\", {})\n    if model_name:\n        context[\"model\"] = model_name\n    if prompt_length is not None:\n        context[\"prompt_length\"] = prompt_length\n    if response_length is not None:\n        context[\"response_length\"] = response_length\n\n    super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.MCPConnectionError","title":"<code>MCPConnectionError</code>","text":"<p>               Bases: <code>MCPError</code></p> <p>Raised when MCP server connection fails.</p> <p>This exception is used for network connectivity issues, server unavailability, or authentication problems with MCP servers.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class MCPConnectionError(MCPError):\n    \"\"\"Raised when MCP server connection fails.\n\n    This exception is used for network connectivity issues, server unavailability,\n    or authentication problems with MCP servers.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        server_url: Optional[str] = None,\n        timeout: Optional[float] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize MCPConnectionError with connection context.\n\n        Args:\n            message: Error description.\n            server_url: URL of the MCP server that failed.\n            timeout: Connection timeout value used.\n            **kwargs: Additional ReviewError arguments.\n        \"\"\"\n        context = kwargs.get(\"context\", {})\n        if server_url:\n            context[\"server_url\"] = server_url\n        if timeout is not None:\n            context[\"timeout\"] = timeout\n\n        super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.MCPConnectionError.__init__","title":"<code>__init__(message, *, server_url=None, timeout=None, **kwargs)</code>","text":"<p>Initialize MCPConnectionError with connection context.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description.</p> required <code>server_url</code> <code>Optional[str]</code> <p>URL of the MCP server that failed.</p> <code>None</code> <code>timeout</code> <code>Optional[float]</code> <p>Connection timeout value used.</p> <code>None</code> <code>**kwargs</code> <p>Additional ReviewError arguments.</p> <code>{}</code> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    *,\n    server_url: Optional[str] = None,\n    timeout: Optional[float] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize MCPConnectionError with connection context.\n\n    Args:\n        message: Error description.\n        server_url: URL of the MCP server that failed.\n        timeout: Connection timeout value used.\n        **kwargs: Additional ReviewError arguments.\n    \"\"\"\n    context = kwargs.get(\"context\", {})\n    if server_url:\n        context[\"server_url\"] = server_url\n    if timeout is not None:\n        context[\"timeout\"] = timeout\n\n    super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.MCPError","title":"<code>MCPError</code>","text":"<p>               Bases: <code>ReviewError</code></p> <p>Base class for MCP (Model Context Protocol) errors.</p> <p>This exception is used as a base for all MCP-related errors including connection issues, tool failures, and protocol violations.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class MCPError(ReviewError):\n    \"\"\"Base class for MCP (Model Context Protocol) errors.\n\n    This exception is used as a base for all MCP-related errors including\n    connection issues, tool failures, and protocol violations.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.MCPServerError","title":"<code>MCPServerError</code>","text":"<p>               Bases: <code>MCPError</code></p> <p>Raised when MCP server returns an error.</p> <p>This exception is used for server-side errors returned by MCP servers, including internal server errors and protocol violations.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class MCPServerError(MCPError):\n    \"\"\"Raised when MCP server returns an error.\n\n    This exception is used for server-side errors returned by MCP servers,\n    including internal server errors and protocol violations.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.MCPToolError","title":"<code>MCPToolError</code>","text":"<p>               Bases: <code>MCPError</code></p> <p>Raised when MCP tool execution fails.</p> <p>This exception is used for tool-specific errors such as missing tools, invalid tool parameters, or tool execution failures.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class MCPToolError(MCPError):\n    \"\"\"Raised when MCP tool execution fails.\n\n    This exception is used for tool-specific errors such as missing tools,\n    invalid tool parameters, or tool execution failures.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        tool_name: Optional[str] = None,\n        tool_params: Optional[dict[str, Any]] = None,\n        available_tools: Optional[list[str]] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize MCPToolError with tool context.\n\n        Args:\n            message: Error description.\n            tool_name: Name of the tool that failed.\n            tool_params: Parameters that were passed to the tool.\n            available_tools: List of available tools for debugging.\n            **kwargs: Additional ReviewError arguments.\n        \"\"\"\n        context = kwargs.get(\"context\", {})\n        if tool_name:\n            context[\"tool_name\"] = tool_name\n        if tool_params:\n            context[\"tool_parameters\"] = tool_params\n        if available_tools:\n            context[\"available_tools\"] = available_tools\n\n        super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.MCPToolError.__init__","title":"<code>__init__(message, *, tool_name=None, tool_params=None, available_tools=None, **kwargs)</code>","text":"<p>Initialize MCPToolError with tool context.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description.</p> required <code>tool_name</code> <code>Optional[str]</code> <p>Name of the tool that failed.</p> <code>None</code> <code>tool_params</code> <code>Optional[dict[str, Any]]</code> <p>Parameters that were passed to the tool.</p> <code>None</code> <code>available_tools</code> <code>Optional[list[str]]</code> <p>List of available tools for debugging.</p> <code>None</code> <code>**kwargs</code> <p>Additional ReviewError arguments.</p> <code>{}</code> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    *,\n    tool_name: Optional[str] = None,\n    tool_params: Optional[dict[str, Any]] = None,\n    available_tools: Optional[list[str]] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize MCPToolError with tool context.\n\n    Args:\n        message: Error description.\n        tool_name: Name of the tool that failed.\n        tool_params: Parameters that were passed to the tool.\n        available_tools: List of available tools for debugging.\n        **kwargs: Additional ReviewError arguments.\n    \"\"\"\n    context = kwargs.get(\"context\", {})\n    if tool_name:\n        context[\"tool_name\"] = tool_name\n    if tool_params:\n        context[\"tool_parameters\"] = tool_params\n    if available_tools:\n        context[\"available_tools\"] = available_tools\n\n    super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.NodeExecutionError","title":"<code>NodeExecutionError</code>","text":"<p>               Bases: <code>WorkflowError</code></p> <p>Raised when a workflow node fails to execute.</p> <p>This exception is used for errors during individual node execution within a LangGraph workflow.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class NodeExecutionError(WorkflowError):\n    \"\"\"Raised when a workflow node fails to execute.\n\n    This exception is used for errors during individual node execution\n    within a LangGraph workflow.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        node_name: Optional[str] = None,\n        thread_id: Optional[str] = None,\n        state_keys: Optional[list[str]] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize NodeExecutionError with execution context.\n\n        Args:\n            message: Error description.\n            node_name: Name of the node that failed.\n            thread_id: Thread ID of the execution.\n            state_keys: Available keys in the state when the error occurred.\n            **kwargs: Additional ReviewError arguments.\n        \"\"\"\n        context = kwargs.get(\"context\", {})\n        if node_name:\n            context[\"node_name\"] = node_name\n        if thread_id:\n            context[\"thread_id\"] = thread_id\n        if state_keys:\n            context[\"available_state_keys\"] = state_keys\n\n        super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.NodeExecutionError.__init__","title":"<code>__init__(message, *, node_name=None, thread_id=None, state_keys=None, **kwargs)</code>","text":"<p>Initialize NodeExecutionError with execution context.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description.</p> required <code>node_name</code> <code>Optional[str]</code> <p>Name of the node that failed.</p> <code>None</code> <code>thread_id</code> <code>Optional[str]</code> <p>Thread ID of the execution.</p> <code>None</code> <code>state_keys</code> <code>Optional[list[str]]</code> <p>Available keys in the state when the error occurred.</p> <code>None</code> <code>**kwargs</code> <p>Additional ReviewError arguments.</p> <code>{}</code> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    *,\n    node_name: Optional[str] = None,\n    thread_id: Optional[str] = None,\n    state_keys: Optional[list[str]] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize NodeExecutionError with execution context.\n\n    Args:\n        message: Error description.\n        node_name: Name of the node that failed.\n        thread_id: Thread ID of the execution.\n        state_keys: Available keys in the state when the error occurred.\n        **kwargs: Additional ReviewError arguments.\n    \"\"\"\n    context = kwargs.get(\"context\", {})\n    if node_name:\n        context[\"node_name\"] = node_name\n    if thread_id:\n        context[\"thread_id\"] = thread_id\n    if state_keys:\n        context[\"available_state_keys\"] = state_keys\n\n    super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.PostconditionError","title":"<code>PostconditionError</code>","text":"<p>               Bases: <code>ReviewError</code></p> <p>Raised when postcondition validation fails.</p> <p>This exception is used when a function's output doesn't meet its expected postconditions, indicating a logic error or unexpected result.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class PostconditionError(ReviewError):\n    \"\"\"Raised when postcondition validation fails.\n\n    This exception is used when a function's output doesn't meet its\n    expected postconditions, indicating a logic error or unexpected result.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        expected: Optional[str] = None,\n        actual: Optional[str] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize PostconditionError with expectation context.\n\n        Args:\n            message: Error description.\n            expected: Description of what was expected.\n            actual: Description of what was actually received.\n            **kwargs: Additional ReviewError arguments.\n        \"\"\"\n        context = kwargs.get(\"context\", {})\n        if expected:\n            context[\"expected\"] = expected\n        if actual:\n            context[\"actual\"] = actual\n\n        super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.PostconditionError.__init__","title":"<code>__init__(message, *, expected=None, actual=None, **kwargs)</code>","text":"<p>Initialize PostconditionError with expectation context.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description.</p> required <code>expected</code> <code>Optional[str]</code> <p>Description of what was expected.</p> <code>None</code> <code>actual</code> <code>Optional[str]</code> <p>Description of what was actually received.</p> <code>None</code> <code>**kwargs</code> <p>Additional ReviewError arguments.</p> <code>{}</code> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    *,\n    expected: Optional[str] = None,\n    actual: Optional[str] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize PostconditionError with expectation context.\n\n    Args:\n        message: Error description.\n        expected: Description of what was expected.\n        actual: Description of what was actually received.\n        **kwargs: Additional ReviewError arguments.\n    \"\"\"\n    context = kwargs.get(\"context\", {})\n    if expected:\n        context[\"expected\"] = expected\n    if actual:\n        context[\"actual\"] = actual\n\n    super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.ProcessingError","title":"<code>ProcessingError</code>","text":"<p>               Bases: <code>ReviewError</code></p> <p>Base class for text processing errors.</p> <p>This exception is used as a base for all text processing related errors including LLM failures, prompt issues, and processing pipeline errors.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class ProcessingError(ReviewError):\n    \"\"\"Base class for text processing errors.\n\n    This exception is used as a base for all text processing related errors\n    including LLM failures, prompt issues, and processing pipeline errors.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.PromptError","title":"<code>PromptError</code>","text":"<p>               Bases: <code>ProcessingError</code></p> <p>Raised when prompt template operations fail.</p> <p>This exception is used for prompt template loading, parsing, or formatting errors.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class PromptError(ProcessingError):\n    \"\"\"Raised when prompt template operations fail.\n\n    This exception is used for prompt template loading, parsing, or\n    formatting errors.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        prompt_file: Optional[str] = None,\n        template_vars: Optional[dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize PromptError with prompt context.\n\n        Args:\n            message: Error description.\n            prompt_file: Path to the prompt file that caused the error.\n            template_vars: Variables that were being substituted in the template.\n            **kwargs: Additional ReviewError arguments.\n        \"\"\"\n        context = kwargs.get(\"context\", {})\n        if prompt_file:\n            context[\"prompt_file\"] = prompt_file\n        if template_vars:\n            context[\"template_variables\"] = template_vars\n\n        super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.PromptError.__init__","title":"<code>__init__(message, *, prompt_file=None, template_vars=None, **kwargs)</code>","text":"<p>Initialize PromptError with prompt context.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description.</p> required <code>prompt_file</code> <code>Optional[str]</code> <p>Path to the prompt file that caused the error.</p> <code>None</code> <code>template_vars</code> <code>Optional[dict[str, Any]]</code> <p>Variables that were being substituted in the template.</p> <code>None</code> <code>**kwargs</code> <p>Additional ReviewError arguments.</p> <code>{}</code> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    *,\n    prompt_file: Optional[str] = None,\n    template_vars: Optional[dict[str, Any]] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize PromptError with prompt context.\n\n    Args:\n        message: Error description.\n        prompt_file: Path to the prompt file that caused the error.\n        template_vars: Variables that were being substituted in the template.\n        **kwargs: Additional ReviewError arguments.\n    \"\"\"\n    context = kwargs.get(\"context\", {})\n    if prompt_file:\n        context[\"prompt_file\"] = prompt_file\n    if template_vars:\n        context[\"template_variables\"] = template_vars\n\n    super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.ReviewError","title":"<code>ReviewError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all metamorphosis review processing errors.</p> <p>This is the root exception class that all other custom exceptions inherit from. It provides structured error information with context, operation details, and optional error codes for programmatic handling.</p> <p>Attributes:</p> Name Type Description <code>message</code> <p>Human-readable error description.</p> <code>context</code> <p>Optional dictionary with additional error context.</p> <code>operation</code> <p>Optional string describing what operation failed.</p> <code>error_code</code> <p>Optional string for programmatic error handling.</p> <code>original_error</code> <p>Optional reference to the underlying exception.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class ReviewError(Exception):\n    \"\"\"Base exception for all metamorphosis review processing errors.\n\n    This is the root exception class that all other custom exceptions inherit from.\n    It provides structured error information with context, operation details, and\n    optional error codes for programmatic handling.\n\n    Attributes:\n        message: Human-readable error description.\n        context: Optional dictionary with additional error context.\n        operation: Optional string describing what operation failed.\n        error_code: Optional string for programmatic error handling.\n        original_error: Optional reference to the underlying exception.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        context: Optional[dict[str, Any]] = None,\n        operation: Optional[str] = None,\n        error_code: Optional[str] = None,\n        original_error: Optional[Exception] = None,\n    ) -&gt; None:\n        \"\"\"Initialize ReviewError with contextual information.\n\n        Args:\n            message: Human-readable error description.\n            context: Additional error context (e.g., {\"text_length\": 1500, \"thread_id\": \"abc\"}).\n            operation: Description of the operation that failed (e.g., \"copy_editing\").\n            error_code: Machine-readable error code (e.g., \"INVALID_INPUT\").\n            original_error: The underlying exception that caused this error.\n        \"\"\"\n        super().__init__(message)\n        self.message = message\n        self.context = context or {}\n        self.operation = operation\n        self.error_code = error_code\n        self.original_error = original_error\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return formatted error message with context.\"\"\"\n        parts = [self.message]\n\n        if self.operation:\n            parts.append(f\"Operation: {self.operation}\")\n\n        if self.error_code:\n            parts.append(f\"Code: {self.error_code}\")\n\n        if self.context:\n            context_str = \", \".join(f\"{k}={v}\" for k, v in self.context.items())\n            parts.append(f\"Context: {context_str}\")\n\n        if self.original_error:\n            parts.append(f\"Caused by: {type(self.original_error).__name__}: {self.original_error}\")\n\n        return \" | \".join(parts)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.ReviewError.__init__","title":"<code>__init__(message, *, context=None, operation=None, error_code=None, original_error=None)</code>","text":"<p>Initialize ReviewError with contextual information.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Human-readable error description.</p> required <code>context</code> <code>Optional[dict[str, Any]]</code> <p>Additional error context (e.g., {\"text_length\": 1500, \"thread_id\": \"abc\"}).</p> <code>None</code> <code>operation</code> <code>Optional[str]</code> <p>Description of the operation that failed (e.g., \"copy_editing\").</p> <code>None</code> <code>error_code</code> <code>Optional[str]</code> <p>Machine-readable error code (e.g., \"INVALID_INPUT\").</p> <code>None</code> <code>original_error</code> <code>Optional[Exception]</code> <p>The underlying exception that caused this error.</p> <code>None</code> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    *,\n    context: Optional[dict[str, Any]] = None,\n    operation: Optional[str] = None,\n    error_code: Optional[str] = None,\n    original_error: Optional[Exception] = None,\n) -&gt; None:\n    \"\"\"Initialize ReviewError with contextual information.\n\n    Args:\n        message: Human-readable error description.\n        context: Additional error context (e.g., {\"text_length\": 1500, \"thread_id\": \"abc\"}).\n        operation: Description of the operation that failed (e.g., \"copy_editing\").\n        error_code: Machine-readable error code (e.g., \"INVALID_INPUT\").\n        original_error: The underlying exception that caused this error.\n    \"\"\"\n    super().__init__(message)\n    self.message = message\n    self.context = context or {}\n    self.operation = operation\n    self.error_code = error_code\n    self.original_error = original_error\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.ReviewError.__str__","title":"<code>__str__()</code>","text":"<p>Return formatted error message with context.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return formatted error message with context.\"\"\"\n    parts = [self.message]\n\n    if self.operation:\n        parts.append(f\"Operation: {self.operation}\")\n\n    if self.error_code:\n        parts.append(f\"Code: {self.error_code}\")\n\n    if self.context:\n        context_str = \", \".join(f\"{k}={v}\" for k, v in self.context.items())\n        parts.append(f\"Context: {context_str}\")\n\n    if self.original_error:\n        parts.append(f\"Caused by: {type(self.original_error).__name__}: {self.original_error}\")\n\n    return \" | \".join(parts)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.StateError","title":"<code>StateError</code>","text":"<p>               Bases: <code>WorkflowError</code></p> <p>Raised when workflow state management fails.</p> <p>This exception is used for state validation errors, missing state keys, or state corruption issues in LangGraph workflows.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class StateError(WorkflowError):\n    \"\"\"Raised when workflow state management fails.\n\n    This exception is used for state validation errors, missing state keys,\n    or state corruption issues in LangGraph workflows.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        expected_keys: Optional[list[str]] = None,\n        actual_keys: Optional[list[str]] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize StateError with state context.\n\n        Args:\n            message: Error description.\n            expected_keys: List of expected state keys.\n            actual_keys: List of actual state keys present.\n            **kwargs: Additional ReviewError arguments.\n        \"\"\"\n        context = kwargs.get(\"context\", {})\n        if expected_keys:\n            context[\"expected_keys\"] = expected_keys\n        if actual_keys:\n            context[\"actual_keys\"] = actual_keys\n\n        super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.StateError.__init__","title":"<code>__init__(message, *, expected_keys=None, actual_keys=None, **kwargs)</code>","text":"<p>Initialize StateError with state context.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description.</p> required <code>expected_keys</code> <code>Optional[list[str]]</code> <p>List of expected state keys.</p> <code>None</code> <code>actual_keys</code> <code>Optional[list[str]]</code> <p>List of actual state keys present.</p> <code>None</code> <code>**kwargs</code> <p>Additional ReviewError arguments.</p> <code>{}</code> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    *,\n    expected_keys: Optional[list[str]] = None,\n    actual_keys: Optional[list[str]] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize StateError with state context.\n\n    Args:\n        message: Error description.\n        expected_keys: List of expected state keys.\n        actual_keys: List of actual state keys present.\n        **kwargs: Additional ReviewError arguments.\n    \"\"\"\n    context = kwargs.get(\"context\", {})\n    if expected_keys:\n        context[\"expected_keys\"] = expected_keys\n    if actual_keys:\n        context[\"actual_keys\"] = actual_keys\n\n    super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>ReviewError</code></p> <p>Raised when input validation fails.</p> <p>This exception is used for invalid input parameters, malformed data, or constraint violations in user-provided data.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class ValidationError(ReviewError):\n    \"\"\"Raised when input validation fails.\n\n    This exception is used for invalid input parameters, malformed data,\n    or constraint violations in user-provided data.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        field_name: Optional[str] = None,\n        field_value: Optional[Any] = None,\n        constraint: Optional[str] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize ValidationError with field context.\n\n        Args:\n            message: Error description.\n            field_name: Name of the field that failed validation.\n            field_value: The invalid value that caused the error.\n            constraint: Description of the validation constraint that was violated.\n            **kwargs: Additional ReviewError arguments.\n        \"\"\"\n        context = kwargs.get(\"context\", {})\n        if field_name:\n            context[\"field\"] = field_name\n        if field_value is not None:\n            context[\"value\"] = str(field_value)\n        if constraint:\n            context[\"constraint\"] = constraint\n\n        super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.ValidationError.__init__","title":"<code>__init__(message, *, field_name=None, field_value=None, constraint=None, **kwargs)</code>","text":"<p>Initialize ValidationError with field context.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description.</p> required <code>field_name</code> <code>Optional[str]</code> <p>Name of the field that failed validation.</p> <code>None</code> <code>field_value</code> <code>Optional[Any]</code> <p>The invalid value that caused the error.</p> <code>None</code> <code>constraint</code> <code>Optional[str]</code> <p>Description of the validation constraint that was violated.</p> <code>None</code> <code>**kwargs</code> <p>Additional ReviewError arguments.</p> <code>{}</code> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    *,\n    field_name: Optional[str] = None,\n    field_value: Optional[Any] = None,\n    constraint: Optional[str] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize ValidationError with field context.\n\n    Args:\n        message: Error description.\n        field_name: Name of the field that failed validation.\n        field_value: The invalid value that caused the error.\n        constraint: Description of the validation constraint that was violated.\n        **kwargs: Additional ReviewError arguments.\n    \"\"\"\n    context = kwargs.get(\"context\", {})\n    if field_name:\n        context[\"field\"] = field_name\n    if field_value is not None:\n        context[\"value\"] = str(field_value)\n    if constraint:\n        context[\"constraint\"] = constraint\n\n    super().__init__(message, context=context, **kwargs)\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.WorkflowError","title":"<code>WorkflowError</code>","text":"<p>               Bases: <code>ReviewError</code></p> <p>Base class for LangGraph workflow errors.</p> <p>This exception is used as a base for all workflow-related errors including graph construction, node execution, and state management issues.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>class WorkflowError(ReviewError):\n    \"\"\"Base class for LangGraph workflow errors.\n\n    This exception is used as a base for all workflow-related errors including\n    graph construction, node execution, and state management issues.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.raise_configuration_error","title":"<code>raise_configuration_error(message, *, missing_var=None, invalid_var=None, invalid_value=None)</code>","text":"<p>Convenience function to raise ConfigurationError with common patterns.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description.</p> required <code>missing_var</code> <code>Optional[str]</code> <p>Name of missing environment variable.</p> <code>None</code> <code>invalid_var</code> <code>Optional[str]</code> <p>Name of invalid environment variable.</p> <code>None</code> <code>invalid_value</code> <code>Optional[str]</code> <p>Invalid value that was provided.</p> <code>None</code> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>Always raises this exception.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def raise_configuration_error(\n    message: str,\n    *,\n    missing_var: Optional[str] = None,\n    invalid_var: Optional[str] = None,\n    invalid_value: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Convenience function to raise ConfigurationError with common patterns.\n\n    Args:\n        message: Error description.\n        missing_var: Name of missing environment variable.\n        invalid_var: Name of invalid environment variable.\n        invalid_value: Invalid value that was provided.\n\n    Raises:\n        ConfigurationError: Always raises this exception.\n    \"\"\"\n    context = {}\n    if missing_var:\n        context[\"missing_variable\"] = missing_var\n    if invalid_var and invalid_value:\n        context[\"invalid_variable\"] = invalid_var\n        context[\"invalid_value\"] = invalid_value\n\n    raise ConfigurationError(\n        message, context=context, operation=\"configuration_validation\", error_code=\"CONFIG_ERROR\"\n    )\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.raise_mcp_tool_error","title":"<code>raise_mcp_tool_error(message, *, tool_name, available_tools=None, tool_params=None)</code>","text":"<p>Convenience function to raise MCPToolError with tool context.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description.</p> required <code>tool_name</code> <code>str</code> <p>Name of the tool that failed.</p> required <code>available_tools</code> <code>Optional[list[str]]</code> <p>List of available tools.</p> <code>None</code> <code>tool_params</code> <code>Optional[dict[str, Any]]</code> <p>Parameters passed to the tool.</p> <code>None</code> <p>Raises:</p> Type Description <code>MCPToolError</code> <p>Always raises this exception.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def raise_mcp_tool_error(\n    message: str,\n    *,\n    tool_name: str,\n    available_tools: Optional[list[str]] = None,\n    tool_params: Optional[dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"Convenience function to raise MCPToolError with tool context.\n\n    Args:\n        message: Error description.\n        tool_name: Name of the tool that failed.\n        available_tools: List of available tools.\n        tool_params: Parameters passed to the tool.\n\n    Raises:\n        MCPToolError: Always raises this exception.\n    \"\"\"\n    raise MCPToolError(\n        message,\n        tool_name=tool_name,\n        available_tools=available_tools,\n        tool_params=tool_params,\n        operation=\"mcp_tool_execution\",\n        error_code=\"TOOL_ERROR\",\n    )\n</code></pre>"},{"location":"metamorphosis/exceptions/#metamorphosis.exceptions.raise_postcondition_error","title":"<code>raise_postcondition_error(message, *, function_name, expected, actual)</code>","text":"<p>Convenience function to raise PostconditionError with validation context.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description.</p> required <code>function_name</code> <code>str</code> <p>Name of the function where postcondition failed.</p> required <code>expected</code> <code>str</code> <p>Description of expected result.</p> required <code>actual</code> <code>str</code> <p>Description of actual result.</p> required <p>Raises:</p> Type Description <code>PostconditionError</code> <p>Always raises this exception.</p> Source code in <code>src/metamorphosis/exceptions.py</code> <pre><code>def raise_postcondition_error(\n    message: str,\n    *,\n    function_name: str,\n    expected: str,\n    actual: str,\n) -&gt; None:\n    \"\"\"Convenience function to raise PostconditionError with validation context.\n\n    Args:\n        message: Error description.\n        function_name: Name of the function where postcondition failed.\n        expected: Description of expected result.\n        actual: Description of actual result.\n\n    Raises:\n        PostconditionError: Always raises this exception.\n    \"\"\"\n    raise PostconditionError(\n        message,\n        expected=expected,\n        actual=actual,\n        operation=function_name,\n        error_code=\"POSTCONDITION_FAILED\",\n    )\n</code></pre>"},{"location":"metamorphosis/init/","title":"Package Init","text":""},{"location":"metamorphosis/init/#metamorphosisinit","title":"metamorphosis.init","text":"<p>Metamorphosis package entry.</p> <p>This module loads environment variables and exposes a loaded configuration object for consumers who rely on <code>svlearn</code> configuration conventions.</p>"},{"location":"metamorphosis/init/#metamorphosis.get_model_registry","title":"<code>get_model_registry()</code>  <code>cached</code>","text":"<p>Return the singleton <code>ModelRegistry</code> instance (lazy import to avoid cycles).</p> Source code in <code>src/metamorphosis/__init__.py</code> <pre><code>@lru_cache(maxsize=1)\ndef get_model_registry():\n    \"\"\"Return the singleton `ModelRegistry` instance (lazy import to avoid cycles).\"\"\"\n    from metamorphosis.model_registry import ModelRegistry  # local import\n\n    return ModelRegistry()\n</code></pre>"},{"location":"metamorphosis/model_registry/","title":"Model Registry","text":""},{"location":"metamorphosis/model_registry/#model-registry","title":"Model Registry","text":"<p>Singleton that provides configured LLM clients.</p> <p>The registry reads the project's configuration and constructs four <code>ChatOpenAI</code> clients: <code>summarizer_llm</code>, <code>copy_editor_llm</code>, <code>key_achievements_llm</code>, and <code>review_text_evaluator_llm</code>.</p> Source code in <code>src/metamorphosis/model_registry.py</code> <pre><code>class ModelRegistry:\n    \"\"\"Singleton that provides configured LLM clients.\n\n    The registry reads the project's configuration and constructs four\n    `ChatOpenAI` clients: `summarizer_llm`, `copy_editor_llm`, `key_achievements_llm`,\n    and `review_text_evaluator_llm`.\n    \"\"\"\n\n    _instance: \"ModelRegistry | None\" = None\n    _lock = threading.Lock()\n\n    def __new__(cls) -&gt; \"ModelRegistry\":  # type: ignore[override]\n        if cls._instance is not None:\n            return cls._instance\n        with cls._lock:\n            if cls._instance is None:\n                cls._instance = super().__new__(cls)\n        return cls._instance\n\n    def __init__(self) -&gt; None:\n        # Guard against re-initialization when called multiple times\n        if hasattr(self, \"_initialized\") and self._initialized:\n            return\n\n        logger.debug(\"Initializing ModelRegistry from configuration\")\n\n        # Load project .env explicitly to avoid cwd/parent ambiguity\n        try:\n            from metamorphosis.utilities import get_project_root  # local import to avoid cycles\n\n            project_root = get_project_root()\n            load_dotenv(dotenv_path=project_root / \".env\", override=True)\n        except Exception as error:  # noqa: BLE001\n            logger.warning(\"Failed to load .env explicitly: {}\", error)\n\n        # Fail-fast on API key\n        api_key = os.getenv(\"OPENAI_API_KEY\")\n        if not api_key:\n            raise_configuration_error(\n                \"OPENAI_API_KEY environment variable is required\",\n                context={\"env_vars_checked\": [\"OPENAI_API_KEY\"]},\n                operation=\"llm_api_key_validation\",\n            )\n        # Optional basic sanity check (does not log the key)\n        if not api_key.startswith(\"sk-\"):\n            logger.warning(\n                \"OPENAI_API_KEY does not start with expected prefix; double-check your key\"\n            )\n\n        cfg = self._load_text_modifier_section()\n\n        summarizer_cfg = _LLMSettings(**cfg.get(\"summarizer\", {}))\n        copy_editor_cfg = _LLMSettings(**cfg.get(\"copy_editor\", {}))\n        key_achievements_cfg = _LLMSettings(**cfg.get(\"key_achievements\", {}))\n        review_text_evaluator_cfg = _LLMSettings(**cfg.get(\"review_text_evaluator\", {}))\n\n        self.summarizer_llm = self._build_chat_openai(summarizer_cfg, api_key)\n        self.copy_editor_llm = self._build_chat_openai(copy_editor_cfg, api_key)\n        self.key_achievements_llm = self._build_chat_openai(key_achievements_cfg, api_key)\n        self.review_text_evaluator_llm = self._build_chat_openai(review_text_evaluator_cfg, api_key)\n\n        self._initialized = True\n        logger.debug(\"ModelRegistry initialized successfully\")\n\n    @staticmethod\n    def _load_text_modifier_section() -&gt; dict[str, Any]:\n        \"\"\"Return the `text_modifier_models` section from project config.\n\n        Raises:\n            ConfigurationError: If the section is missing or invalid.\n        \"\"\"\n        # Import here to avoid import cycles\n        from metamorphosis import config as project_config  # noqa: WPS433 (local import)\n\n        try:\n            section = project_config[\"text_modifier_models\"]\n        except KeyError as error:\n            raise_configuration_error(\n                \"Missing 'text_modifier_models' in configuration\",\n                context={\"config_keys\": list(project_config.keys())},\n                operation=\"config_section_resolution\",\n                original_error=error,\n            )\n\n        if not isinstance(section, dict) or not section:\n            raise_configuration_error(\n                \"Invalid 'text_modifier_models' configuration section\",\n                context={\"type\": type(section).__name__},\n                operation=\"config_section_validation\",\n            )\n        return section\n\n    @staticmethod\n    @validate_call\n    def _build_chat_openai(settings: _LLMSettings, api_key: str) -&gt; ChatOpenAI:\n        \"\"\"Construct a `ChatOpenAI` client from validated settings.\"\"\"\n        params: dict[str, Any] = {\"model\": settings.model, \"openai_api_key\": api_key}\n\n        if settings.temperature is not None:\n            params[\"temperature\"] = settings.temperature\n        if settings.max_tokens is not None:\n            params[\"max_tokens\"] = settings.max_tokens\n        if settings.top_p is not None:\n            params[\"top_p\"] = settings.top_p\n        if settings.frequency_penalty is not None:\n            params[\"frequency_penalty\"] = settings.frequency_penalty\n        if settings.presence_penalty is not None:\n            params[\"presence_penalty\"] = settings.presence_penalty\n        if settings.stop_sequences is not None:\n            params[\"stop\"] = settings.stop_sequences\n        if settings.timeout is not None:\n            params[\"timeout\"] = settings.timeout\n\n        logger.debug(\"Creating ChatOpenAI with params: {}\", params)\n        return ChatOpenAI(**params)\n</code></pre>"},{"location":"metamorphosis/utilities/","title":"Utilities","text":""},{"location":"metamorphosis/utilities/#utilities","title":"Utilities","text":"<p>Common utility functions for the metamorphosis project.</p> <p>This module provides reusable utility functions that are used across multiple components of the metamorphosis system. All utilities follow Design-by-Contract principles with proper validation and error handling.</p>"},{"location":"metamorphosis/utilities/#metamorphosis.utilities.create_achievements_html_table","title":"<code>create_achievements_html_table(achievements_list)</code>","text":"<p>Create a modern, browser-friendly HTML table for achievements display.</p> <p>This method creates a responsive, aesthetically pleasing HTML table that renders well in browsers and provides better user experience than terminal-based rich tables.</p> <p>Parameters:</p> Name Type Description Default <code>achievements_list</code> <code>AchievementsList</code> <p>The AchievementsList object containing extracted achievements.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Complete HTML string with embedded CSS for a modern table display.</p> Source code in <code>src/metamorphosis/utilities.py</code> <pre><code>def create_achievements_html_table(achievements_list: AchievementsList) -&gt; str:\n    \"\"\"Create a modern, browser-friendly HTML table for achievements display.\n\n    This method creates a responsive, aesthetically pleasing HTML table that renders\n    well in browsers and provides better user experience than terminal-based rich tables.\n\n    Args:\n        achievements_list: The AchievementsList object containing extracted achievements.\n\n    Returns:\n        str: Complete HTML string with embedded CSS for a modern table display.\n    \"\"\"\n    # Define impact area colors and icons for better visual hierarchy\n    impact_styles = {\n        \"reliability\": {\"color\": \"#ef4444\", \"bg\": \"#fef2f2\", \"icon\": \"\ud83d\udd27\"},\n        \"performance\": {\"color\": \"#3b82f6\", \"bg\": \"#eff6ff\", \"icon\": \"\u26a1\"},\n        \"security\": {\"color\": \"#a855f7\", \"bg\": \"#faf5ff\", \"icon\": \"\ud83d\udd12\"},\n        \"cost\": {\"color\": \"#10b981\", \"bg\": \"#f0fdf4\", \"icon\": \"\ud83d\udcb0\"},\n        \"revenue\": {\"color\": \"#059669\", \"bg\": \"#ecfdf5\", \"icon\": \"\ud83d\udcb5\"},\n        \"customer\": {\"color\": \"#06b6d4\", \"bg\": \"#f0fdfa\", \"icon\": \"\ud83d\udc65\"},\n        \"delivery_speed\": {\"color\": \"#f59e0b\", \"bg\": \"#fffbeb\", \"icon\": \"\ud83d\ude80\"},\n        \"quality\": {\"color\": \"#6b7280\", \"bg\": \"#f9fafb\", \"icon\": \"\u2728\"},\n        \"compliance\": {\"color\": \"#4b5563\", \"bg\": \"#f3f4f6\", \"icon\": \"\ud83d\udccb\"},\n        \"team\": {\"color\": \"#1d4ed8\", \"bg\": \"#dbeafe\", \"icon\": \"\ud83e\udd1d\"},\n    }\n\n    # Start building the HTML with embedded modern CSS\n    html = \"\"\"\n    &lt;div style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\"&gt;\n        &lt;style&gt;\n            .achievements-container {\n                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n                border-radius: 16px;\n                padding: 24px;\n                margin: 16px 0;\n                box-shadow: 0 10px 30px rgba(0,0,0,0.1);\n            }\n            .achievements-title {\n                color: white;\n                font-size: 24px;\n                font-weight: 700;\n                margin-bottom: 8px;\n                text-align: center;\n                text-shadow: 0 2px 4px rgba(0,0,0,0.3);\n            }\n            .achievements-subtitle {\n                color: rgba(255,255,255,0.9);\n                font-size: 14px;\n                text-align: center;\n                margin-bottom: 24px;\n            }\n            .achievements-table {\n                width: 100%;\n                border-collapse: collapse;\n                background: white;\n                border-radius: 12px;\n                overflow: hidden;\n                box-shadow: 0 4px 20px rgba(0,0,0,0.1);\n            }\n            .achievements-table th {\n                background: linear-gradient(135deg, #4f46e5 0%, #7c3aed 100%);\n                color: white;\n                padding: 16px 12px;\n                text-align: left;\n                font-weight: 600;\n                font-size: 14px;\n                text-transform: uppercase;\n                letter-spacing: 0.5px;\n                border: none;\n            }\n            .achievements-table td {\n                padding: 16px 12px;\n                border-bottom: 1px solid #f1f5f9;\n                vertical-align: top;\n                line-height: 1.5;\n            }\n            .achievements-table tr:hover {\n                background-color: #f8fafc;\n                transform: scale(1.01);\n                transition: all 0.2s ease;\n            }\n            .achievement-number {\n                font-weight: 700;\n                color: #4f46e5;\n                font-size: 16px;\n            }\n            .achievement-title {\n                font-weight: 600;\n                color: #1e293b;\n                margin-bottom: 4px;\n            }\n            .achievement-outcome {\n                color: #475569;\n                font-size: 14px;\n                line-height: 1.6;\n            }\n            .impact-badge {\n                display: inline-flex;\n                align-items: center;\n                gap: 6px;\n                padding: 6px 12px;\n                border-radius: 20px;\n                font-size: 12px;\n                font-weight: 600;\n                text-transform: uppercase;\n                letter-spacing: 0.5px;\n            }\n            .metrics-list {\n                font-size: 13px;\n                color: #059669;\n                font-weight: 500;\n            }\n            .details-item {\n                display: flex;\n                align-items: center;\n                gap: 6px;\n                font-size: 12px;\n                color: #64748b;\n                margin-bottom: 4px;\n            }\n            .details-item:last-child {\n                margin-bottom: 0;\n            }\n            @media (max-width: 768px) {\n                .achievements-table {\n                    font-size: 12px;\n                }\n                .achievements-table th,\n                .achievements-table td {\n                    padding: 8px 6px;\n                }\n            }\n        &lt;/style&gt;\n        &lt;div class=\"achievements-container\"&gt;\n            &lt;div class=\"achievements-title\"&gt;\ud83c\udfc6 Extracted Key Achievements&lt;/div&gt;\n            &lt;div class=\"achievements-subtitle\"&gt;\n                {count} items \u2022 ~{tokens} tokens\n            &lt;/div&gt;\n            &lt;table class=\"achievements-table\"&gt;\n                &lt;thead&gt;\n                    &lt;tr&gt;\n                        &lt;th style=\"width: 25%;\"&gt;Achievement&lt;/th&gt;\n                        &lt;th style=\"width: 35%;\"&gt;Outcome&lt;/th&gt;\n                        &lt;th style=\"width: 15%;\"&gt;Impact Area&lt;/th&gt;\n                        &lt;th style=\"width: 15%;\"&gt;Metrics&lt;/th&gt;\n                        &lt;th style=\"width: 10%;\"&gt;Details&lt;/th&gt;\n                    &lt;/tr&gt;\n                &lt;/thead&gt;\n                &lt;tbody&gt;\n    \"\"\".format(\n        count=len(achievements_list.items),\n        tokens=achievements_list.size\n    )\n\n    # Add rows for each achievement\n    for i, achievement in enumerate(achievements_list.items, 1):\n        # Format metrics as a list with HTML escaping\n        metrics_html = \"\"\n        if achievement.metric_strings:\n            metrics_items = [f\"&lt;div class='metrics-list'&gt;\u2022 {html.escape(metric)}&lt;/div&gt;\" \n                           for metric in achievement.metric_strings]\n            metrics_html = \"\".join(metrics_items)\n        else:\n            metrics_html = \"&lt;div style='color: #9ca3af;'&gt;\u2014&lt;/div&gt;\"\n\n        # Format additional details with icons and HTML escaping\n        details_html = \"\"\n        if achievement.timeframe:\n            details_html += f\"&lt;div class='details-item'&gt;\u23f0 {html.escape(achievement.timeframe)}&lt;/div&gt;\"\n        if achievement.ownership_scope:\n            details_html += f\"&lt;div class='details-item'&gt;\ud83d\udc64 {html.escape(achievement.ownership_scope)}&lt;/div&gt;\"\n        if achievement.collaborators:\n            collabs = \", \".join(achievement.collaborators[:2])\n            if len(achievement.collaborators) &gt; 2:\n                collabs += f\" +{len(achievement.collaborators) - 2}\"\n            details_html += f\"&lt;div class='details-item'&gt;\ud83e\udd1d {html.escape(collabs)}&lt;/div&gt;\"\n\n        if not details_html:\n            details_html = \"&lt;div style='color: #9ca3af;'&gt;\u2014&lt;/div&gt;\"\n\n        # Get impact area styling\n        impact_style = impact_styles.get(achievement.impact_area, {\n            \"color\": \"#6b7280\", \"bg\": \"#f9fafb\", \"icon\": \"\ud83d\udcca\"\n        })\n\n        # Create the impact badge\n        impact_badge = f'&lt;div class=\"impact-badge\" style=\"color: {impact_style[\"color\"]}; background-color: {impact_style[\"bg\"]}; border: 1px solid {impact_style[\"color\"]}20;\"&gt;{impact_style[\"icon\"]} {achievement.impact_area}&lt;/div&gt;'\n\n        # Add the table row with HTML escaping for safety\n        row_html = f\"\"\"\n                    &lt;tr&gt;\n                        &lt;td&gt;\n                            &lt;div class=\"achievement-number\"&gt;{i}.&lt;/div&gt;\n                            &lt;div class=\"achievement-title\"&gt;{html.escape(achievement.title)}&lt;/div&gt;\n                        &lt;/td&gt;\n                        &lt;td&gt;\n                            &lt;div class=\"achievement-outcome\"&gt;{html.escape(achievement.outcome)}&lt;/div&gt;\n                        &lt;/td&gt;\n                        &lt;td style=\"text-align: center;\"&gt;\n                            {impact_badge}\n                        &lt;/td&gt;\n                        &lt;td&gt;\n                            {metrics_html}\n                        &lt;/td&gt;\n                        &lt;td&gt;\n                            {details_html}\n                        &lt;/td&gt;\n                    &lt;/tr&gt;\n        \"\"\"\n        html += row_html\n\n    # Close the HTML structure\n    html += \"\"\"\n                &lt;/tbody&gt;\n            &lt;/table&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    return html\n</code></pre>"},{"location":"metamorphosis/utilities/#metamorphosis.utilities.create_achievements_table","title":"<code>create_achievements_table(achievements_list)</code>","text":"<p>Create a rich table displaying the extracted achievements.</p> <p>Parameters:</p> Name Type Description Default <code>achievements_list</code> <code>AchievementsList</code> <p>The AchievementsList object containing extracted achievements.</p> required <p>Returns:</p> Type Description <code>Table</code> <p>A rich Table object formatted for display.</p> Source code in <code>src/metamorphosis/utilities.py</code> <pre><code>def create_achievements_table(achievements_list: AchievementsList) -&gt; Table:\n    \"\"\"Create a rich table displaying the extracted achievements.\n\n    Args:\n        achievements_list: The AchievementsList object containing extracted achievements.\n\n    Returns:\n        A rich Table object formatted for display.\n    \"\"\"\n    # Create the main table\n    table = Table(\n        title=(\n            f\"\ud83c\udfc6 Extracted Key Achievements \"\n            f\"({len(achievements_list.items)} items, ~{achievements_list.size} tokens)\"\n        ),\n        box=box.ROUNDED,\n        show_header=True,\n        header_style=\"bold magenta\",\n        title_style=\"bold blue\",\n        expand=True,\n    )\n\n    # Add columns\n    table.add_column(\"Title\", style=\"bold cyan\", width=25)\n    table.add_column(\"Outcome\", style=\"white\", width=40)\n    table.add_column(\"Impact Area\", style=\"bold green\", justify=\"center\", width=12)\n    table.add_column(\"Metrics\", style=\"bold yellow\", width=15)\n    table.add_column(\"Details\", style=\"dim white\", width=20)\n\n    # Add rows for each achievement\n    for i, achievement in enumerate(achievements_list.items, 1):\n        # Format metrics as a comma-separated string\n        metrics_text = \", \".join(achievement.metric_strings) if achievement.metric_strings else \"\u2014\"\n\n        # Format additional details (timeframe, scope, collaborators)\n        details_parts = []\n        if achievement.timeframe:\n            details_parts.append(f\"\u23f0 {achievement.timeframe}\")\n        if achievement.ownership_scope:\n            details_parts.append(f\"\ud83d\udc64 {achievement.ownership_scope}\")\n        if achievement.collaborators:\n            collabs = \", \".join(achievement.collaborators[:2])  # Show first 2 collaborators\n            if len(achievement.collaborators) &gt; 2:\n                collabs += f\" +{len(achievement.collaborators) - 2}\"\n            details_parts.append(f\"\ud83e\udd1d {collabs}\")\n\n        details_text = \"\\n\".join(details_parts) if details_parts else \"\u2014\"\n\n        # Color-code impact areas\n        impact_colors = {\n            \"reliability\": \"red\",\n            \"performance\": \"blue\",\n            \"security\": \"magenta\",\n            \"cost\": \"green\",\n            \"revenue\": \"bold green\",\n            \"customer\": \"cyan\",\n            \"delivery_speed\": \"yellow\",\n            \"quality\": \"white\",\n            \"compliance\": \"dim white\",\n            \"team\": \"bold blue\",\n        }\n        impact_color = impact_colors.get(achievement.impact_area, \"white\")\n        impact_text = Text(achievement.impact_area, style=impact_color)\n\n        # Add the row\n        table.add_row(\n            f\"{i}. {achievement.title}\",\n            achievement.outcome,\n            impact_text,\n            metrics_text,\n            details_text,\n        )\n\n    return table\n</code></pre>"},{"location":"metamorphosis/utilities/#metamorphosis.utilities.create_metrics_table","title":"<code>create_metrics_table(scorecard)</code>","text":"<p>Create a rich table displaying the evaluation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>scorecard</code> <code>ReviewScorecard</code> <p>The ReviewScorecard object containing evaluation results.</p> required <p>Returns:</p> Type Description <code>Table</code> <p>A rich Table object formatted for display.</p> Source code in <code>src/metamorphosis/utilities.py</code> <pre><code>def create_metrics_table(scorecard: ReviewScorecard) -&gt; Table:\n    \"\"\"Create a rich table displaying the evaluation metrics.\n\n    Args:\n        scorecard: The ReviewScorecard object containing evaluation results.\n\n    Returns:\n        A rich Table object formatted for display.\n    \"\"\"\n    # Create the main table\n    table = Table(\n        title=(\n            f\"\ud83d\udcca Review Quality Evaluation \"\n            f\"(Overall: {scorecard.overall}/100 - {scorecard.verdict.title()})\"\n        ),\n        box=box.ROUNDED,\n        show_header=True,\n        header_style=\"bold magenta\",\n        title_style=\"bold blue\",\n        expand=True,\n    )\n\n    # Add columns\n    table.add_column(\"Metric\", style=\"bold cyan\", width=20)\n    table.add_column(\"Score\", style=\"bold white\", justify=\"center\", width=8)\n    table.add_column(\"Rationale\", style=\"white\", width=50)\n    table.add_column(\"Suggestion\", style=\"bold yellow\", width=40)\n\n    # Define weights for display\n    weights = {\n        \"OutcomeOverActivity\": \"25%\",\n        \"QuantitativeSpecificity\": \"25%\",\n        \"ClarityCoherence\": \"15%\",\n        \"Conciseness\": \"15%\",\n        \"OwnershipLeadership\": \"10%\",\n        \"Collaboration\": \"10%\",\n    }\n\n    # Color coding based on score ranges\n    def get_score_color(score: int) -&gt; str:\n        if score &gt;= 85:\n            return \"bright_green\"\n        elif score &gt;= 70:\n            return \"green\"\n        elif score &gt;= 50:\n            return \"yellow\"\n        else:\n            return \"red\"\n\n    # Add rows for each metric\n    for metric in scorecard.metrics:\n        weight = weights.get(metric.name, \"\")\n        metric_name = f\"{metric.name}\\n({weight})\"\n\n        score_color = get_score_color(metric.score)\n        score_text = Text(f\"{metric.score}/100\", style=score_color)\n\n        # Add the row\n        table.add_row(metric_name, score_text, metric.rationale, metric.suggestion)\n\n    return table\n</code></pre>"},{"location":"metamorphosis/utilities/#metamorphosis.utilities.create_radar_chart_info","title":"<code>create_radar_chart_info(scorecard)</code>","text":"<p>Create an info panel with radar chart data.</p> <p>Parameters:</p> Name Type Description Default <code>scorecard</code> <code>ReviewScorecard</code> <p>The ReviewScorecard object containing evaluation results.</p> required <p>Returns:</p> Type Description <code>Panel</code> <p>A rich Panel object with radar chart information.</p> Source code in <code>src/metamorphosis/utilities.py</code> <pre><code>def create_radar_chart_info(scorecard: ReviewScorecard) -&gt; Panel:\n    \"\"\"Create an info panel with radar chart data.\n\n    Args:\n        scorecard: The ReviewScorecard object containing evaluation results.\n\n    Returns:\n        A rich Panel object with radar chart information.\n    \"\"\"\n    # Format radar data\n    radar_data = []\n    for label, value in zip(scorecard.radar_labels, scorecard.radar_values):\n        radar_data.append(f\"  \u2022 {label}: {value}/100\")\n\n    radar_text = \"\ud83d\udce1 Radar Chart Data (for visualization):\\n\\n\" + \"\\n\".join(radar_data)\n\n    return Panel(radar_text, title=\"\ud83d\udcc8 Visualization Data\", style=\"dim blue\", box=box.SIMPLE)\n</code></pre>"},{"location":"metamorphosis/utilities/#metamorphosis.utilities.create_radar_plot","title":"<code>create_radar_plot(evaluation_data)</code>","text":"<p>Create a radar plot from evaluation data.</p> <p>Parameters:</p> Name Type Description Default <code>evaluation_data</code> <code>dict</code> <p>Dictionary containing evaluation results with radar_labels and radar_values.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly Figure object containing the radar plot.</p> Source code in <code>src/metamorphosis/utilities.py</code> <pre><code>def create_radar_plot(evaluation_data: dict) -&gt; go.Figure:\n    \"\"\"Create a radar plot from evaluation data.\n\n    Args:\n        evaluation_data: Dictionary containing evaluation results with radar_labels and radar_values.\n\n    Returns:\n        Plotly Figure object containing the radar plot.\n    \"\"\"\n    # Extract radar data\n    labels = evaluation_data[\"radar_labels\"]\n    values = evaluation_data[\"radar_values\"]\n    overall_score = evaluation_data[\"overall\"]\n    verdict = evaluation_data[\"verdict\"]\n\n    # Create the radar plot\n    fig = go.Figure()\n\n    # Add the \"Writing Goal\" reference polygon (90% on all metrics)\n    goal_values = [90] * len(labels)\n    fig.add_trace(\n        go.Scatterpolar(\n            r=goal_values,\n            theta=labels,\n            fill=\"toself\",\n            name=\"Writing Goal (90% target)\",\n            line=dict(color=\"rgba(255, 165, 0, 0.8)\", width=2, dash=\"dash\"),\n            fillcolor=\"rgba(255, 165, 0, 0.1)\",\n            hovertemplate=\"&lt;b&gt;%{theta}&lt;/b&gt;&lt;br&gt;Writing Goal: %{r}/100&lt;extra&gt;&lt;/extra&gt;\",\n            opacity=0.7,\n        )\n    )\n\n    # Add the main radar trace (actual scores)\n    fig.add_trace(\n        go.Scatterpolar(\n            r=values,\n            theta=labels,\n            fill=\"toself\",\n            name=f\"Current Performance (Overall: {overall_score}/100)\",\n            line=dict(color=\"rgb(0, 123, 255)\", width=3),\n            fillcolor=\"rgba(0, 123, 255, 0.3)\",\n            hovertemplate=\"&lt;b&gt;%{theta}&lt;/b&gt;&lt;br&gt;Current Score: %{r}/100&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n\n    # Customize the layout\n    fig.update_layout(\n        polar=dict(\n            radialaxis=dict(\n                visible=True,\n                range=[0, 105],  # Extended range to show 100% circle clearly\n                tickvals=[20, 40, 60, 80, 100],\n                ticktext=[\"20\", \"40\", \"60\", \"80\", \"100\"],\n                tickfont=dict(size=11, color=\"#333\"),\n                gridcolor=\"rgba(200, 200, 200, 0.5)\",\n                linecolor=\"rgba(150, 150, 150, 0.8)\",\n                gridwidth=1,\n            ),\n            angularaxis=dict(\n                tickfont=dict(size=13, color=\"darkblue\", family=\"Arial Bold\"),\n                rotation=90,  # Start from top\n                direction=\"clockwise\",\n                linecolor=\"rgba(150, 150, 150, 0.8)\",\n                gridcolor=\"rgba(200, 200, 200, 0.3)\",\n            ),\n            bgcolor=\"rgba(248, 249, 250, 0.9)\",\n        ),\n        title=dict(\n            text=f\"\ud83d\udcca Review Quality Evaluation - {verdict.title()}&lt;br&gt;\"\n            f\"&lt;sub style='color:#666;'&gt;Overall Score: {overall_score}/100 | \"\n            f\"\ud83c\udfaf Target: 90% across all metrics&lt;/sub&gt;\",\n            x=0.5,\n            font=dict(size=18, color=\"darkblue\", family=\"Arial Bold\"),\n        ),\n        font=dict(family=\"Arial\", size=12),\n        width=750,\n        height=750,\n        margin=dict(l=90, r=90, t=120, b=90),\n        paper_bgcolor=\"white\",\n        plot_bgcolor=\"rgba(248, 249, 250, 0.3)\",\n        showlegend=True,\n        legend=dict(\n            orientation=\"h\",\n            yanchor=\"bottom\",\n            y=-0.1,\n            xanchor=\"center\",\n            x=0.5,\n            bgcolor=\"rgba(255, 255, 255, 0.8)\",\n            bordercolor=\"rgba(200, 200, 200, 0.5)\",\n            borderwidth=1,\n            font=dict(size=11),\n        ),\n    )\n\n    # Add improved score annotations positioned outside the radar\n    for i, (label, value) in enumerate(zip(labels, values)):\n        # Calculate proper polar coordinates for annotation positioning\n        angle_deg = 90 - i * 360 / len(labels)  # Start from top, go clockwise\n        angle_rad = angle_deg * 3.14159 / 180\n\n        # Position annotations at radius 115 (outside the 105 max range)\n        r_annotation = 115\n        x_pos = r_annotation * 0.01 * math.cos(angle_rad)\n        y_pos = r_annotation * 0.01 * math.sin(angle_rad)\n\n        # Choose color based on performance vs goal\n        score_color = \"green\" if value &gt;= 90 else \"orange\" if value &gt;= 70 else \"red\"\n\n        fig.add_annotation(\n            x=x_pos,\n            y=y_pos,\n            text=f\"&lt;b&gt;{value}&lt;/b&gt;\",\n            showarrow=False,\n            font=dict(size=12, color=score_color, family=\"Arial Bold\"),\n            bgcolor=\"rgba(255, 255, 255, 0.9)\",\n            bordercolor=score_color,\n            borderwidth=2,\n            borderpad=4,\n        )\n\n    return fig\n</code></pre>"},{"location":"metamorphosis/utilities/#metamorphosis.utilities.create_summary_panel","title":"<code>create_summary_panel(achievements_list)</code>","text":"<p>Create a summary panel with statistics about the achievements.</p> <p>Parameters:</p> Name Type Description Default <code>achievements_list</code> <code>AchievementsList</code> <p>The AchievementsList object containing extracted achievements.</p> required <p>Returns:</p> Type Description <code>Panel</code> <p>A rich Panel object with summary statistics.</p> Source code in <code>src/metamorphosis/utilities.py</code> <pre><code>def create_summary_panel(achievements_list: AchievementsList) -&gt; Panel:\n    \"\"\"Create a summary panel with statistics about the achievements.\n\n    Args:\n        achievements_list: The AchievementsList object containing extracted achievements.\n\n    Returns:\n        A rich Panel object with summary statistics.\n    \"\"\"\n    if not achievements_list.items:\n        return Panel(\n            \"No achievements were extracted from the review text.\",\n            title=\"\ud83d\udcca Summary\",\n            style=\"dim red\",\n        )\n\n    # Count achievements by impact area\n    impact_counts: dict[str, int] = {}\n    total_metrics = 0\n    achievements_with_timeframes = 0\n    achievements_with_collaborators = 0\n\n    for achievement in achievements_list.items:\n        impact_counts[achievement.impact_area] = impact_counts.get(achievement.impact_area, 0) + 1\n        total_metrics += len(achievement.metric_strings)\n        if achievement.timeframe:\n            achievements_with_timeframes += 1\n        if achievement.collaborators:\n            achievements_with_collaborators += 1\n\n    # Format the summary text\n    summary_lines = [\n        f\"\ud83d\udcc8 Total Achievements: {len(achievements_list.items)}\",\n        f\"\ud83d\udcca Total Metrics Found: {total_metrics}\",\n        f\"\u23f0 With Timeframes: {achievements_with_timeframes}\",\n        f\"\ud83e\udd1d With Collaborators: {achievements_with_collaborators}\",\n        f\"\ud83c\udfaf Token Estimate: {achievements_list.size}\",\n        \"\",\n        \"\ud83d\udccb Impact Areas:\",\n    ]\n\n    # Add impact area breakdown\n    for impact_area, count in sorted(impact_counts.items()):\n        summary_lines.append(f\"  \u2022 {impact_area}: {count}\")\n\n    return Panel(\n        \"\\n\".join(summary_lines), title=\"\ud83d\udcca Achievements Summary\", style=\"dim blue\", box=box.SIMPLE\n    )\n</code></pre>"},{"location":"metamorphosis/utilities/#metamorphosis.utilities.create_summary_panel_evaluation","title":"<code>create_summary_panel_evaluation(scorecard)</code>","text":"<p>Create a summary panel with overall evaluation statistics.</p> <p>Parameters:</p> Name Type Description Default <code>scorecard</code> <code>ReviewScorecard</code> <p>The ReviewScorecard object containing evaluation results.</p> required <p>Returns:</p> Type Description <code>Panel</code> <p>A rich Panel object with summary statistics.</p> Source code in <code>src/metamorphosis/utilities.py</code> <pre><code>def create_summary_panel_evaluation(scorecard: ReviewScorecard) -&gt; Panel:\n    \"\"\"Create a summary panel with overall evaluation statistics.\n\n    Args:\n        scorecard: The ReviewScorecard object containing evaluation results.\n\n    Returns:\n        A rich Panel object with summary statistics.\n    \"\"\"\n    # Verdict styling\n    verdict_colors = {\n        \"excellent\": \"bright_green\",\n        \"strong\": \"green\",\n        \"mixed\": \"yellow\",\n        \"weak\": \"red\",\n    }\n    verdict_color = verdict_colors.get(scorecard.verdict, \"white\")\n\n    # Calculate score statistics\n    scores = [metric.score for metric in scorecard.metrics]\n    avg_score = sum(scores) / len(scores)\n    max_score = max(scores)\n    min_score = min(scores)\n\n    # Find best and worst performing metrics\n    best_metric = max(scorecard.metrics, key=lambda m: m.score)\n    worst_metric = min(scorecard.metrics, key=lambda m: m.score)\n\n    # Format the summary text\n    summary_lines = [\n        f\"\ud83c\udfaf Overall Score: {scorecard.overall}/100\",\n        f\"\ud83d\udcc8 Verdict: {scorecard.verdict.title()}\",\n        f\"\ud83d\udcca Average Score: {avg_score:.1f}/100\",\n        f\"\ud83d\udd1d Highest Score: {max_score}/100 ({best_metric.name})\",\n        f\"\ud83d\udd3b Lowest Score: {min_score}/100 ({worst_metric.name})\",\n        \"\",\n        f\"\ud83c\udff7\ufe0f  Quality Flags: {len(scorecard.notes)} detected\",\n    ]\n\n    # Add flags if any\n    if scorecard.notes:\n        summary_lines.append(\"   \u2022 \" + \", \".join(scorecard.notes))\n    else:\n        summary_lines.append(\"   \u2022 No quality issues detected\")\n\n    return Panel(\n        \"\\n\".join(summary_lines),\n        title=\"\ud83d\udccb Evaluation Summary\",\n        style=f\"dim {verdict_color}\",\n        box=box.SIMPLE,\n    )\n</code></pre>"},{"location":"metamorphosis/utilities/#metamorphosis.utilities.ensure_directory_exists","title":"<code>ensure_directory_exists(directory_path)</code>","text":"<p>Ensure a directory exists, creating it if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>directory_path</code> <code>Annotated[Path | str, Field(description='Directory path to create if needed')]</code> <p>Path to the directory (Path object or string).</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Absolute path to the directory.</p> <p>Raises:</p> Type Description <code>FileOperationError</code> <p>If directory cannot be created.</p> Source code in <code>src/metamorphosis/utilities.py</code> <pre><code>@validate_call\ndef ensure_directory_exists(\n    directory_path: Annotated[Path | str, Field(description=\"Directory path to create if needed\")],\n) -&gt; Path:\n    \"\"\"Ensure a directory exists, creating it if necessary.\n\n    Args:\n        directory_path: Path to the directory (Path object or string).\n\n    Returns:\n        Path: Absolute path to the directory.\n\n    Raises:\n        FileOperationError: If directory cannot be created.\n    \"\"\"\n    if isinstance(directory_path, str):\n        directory_path = Path(directory_path)\n\n    directory_path = directory_path.resolve()\n\n    if directory_path.exists():\n        if not directory_path.is_dir():\n            raise FileOperationError(\n                f\"Path exists but is not a directory: {directory_path}\",\n                file_path=str(directory_path),\n                operation_type=\"create\",\n                operation=\"directory_validation\",\n                error_code=\"NOT_A_DIRECTORY\",\n            )\n        return directory_path\n\n    try:\n        directory_path.mkdir(parents=True, exist_ok=True)\n        logger.debug(\"Created directory: {}\", directory_path)\n        return directory_path\n    except OSError as e:\n        raise FileOperationError(\n            f\"Failed to create directory: {directory_path}\",\n            file_path=str(directory_path),\n            operation_type=\"create\",\n            operation=\"directory_creation\",\n            error_code=\"CREATE_FAILED\",\n            original_error=e,\n        ) from e\n</code></pre>"},{"location":"metamorphosis/utilities/#metamorphosis.utilities.get_project_root","title":"<code>get_project_root(env_var_name='PROJECT_ROOT_DIR', fallback_levels=3)</code>","text":"<p>Get project root directory from environment variable or path resolution.</p> <p>This utility provides a consistent way to locate the project root directory across different modules, supporting both environment variable configuration and automatic path resolution.</p> <p>Parameters:</p> Name Type Description Default <code>env_var_name</code> <code>Annotated[str, Field(min_length=1)]</code> <p>Name of environment variable containing project root path.</p> <code>'PROJECT_ROOT_DIR'</code> <code>fallback_levels</code> <code>Annotated[int, Field(ge=1, le=10)]</code> <p>Number of parent directories to traverse for fallback.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Absolute path to the project root directory.</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If project root cannot be determined.</p> Source code in <code>src/metamorphosis/utilities.py</code> <pre><code>@validate_call\ndef get_project_root(\n    env_var_name: Annotated[str, Field(min_length=1)] = \"PROJECT_ROOT_DIR\",\n    fallback_levels: Annotated[int, Field(ge=1, le=10)] = 3,\n) -&gt; Path:\n    \"\"\"Get project root directory from environment variable or path resolution.\n\n    This utility provides a consistent way to locate the project root directory\n    across different modules, supporting both environment variable configuration\n    and automatic path resolution.\n\n    Args:\n        env_var_name: Name of environment variable containing project root path.\n        fallback_levels: Number of parent directories to traverse for fallback.\n\n    Returns:\n        Path: Absolute path to the project root directory.\n\n    Raises:\n        ConfigurationError: If project root cannot be determined.\n    \"\"\"\n    logger.debug(\n        \"Resolving project root (env_var={}, fallback_levels={})\", env_var_name, fallback_levels\n    )\n\n    # Try environment variable first\n    project_root_str = os.getenv(env_var_name)\n    if project_root_str:\n        project_root = Path(project_root_str).resolve()\n        if project_root.exists() and project_root.is_dir():\n            logger.debug(\"Using project root from {}: {}\", env_var_name, project_root)\n            return project_root\n        else:\n            raise ConfigurationError(\n                f\"Project root from {env_var_name} does not exist or is not a directory\",\n                context={\"env_var\": env_var_name, \"path\": project_root_str},\n                operation=\"project_root_resolution\",\n                error_code=\"INVALID_PROJECT_ROOT\",\n            )\n\n    # Fallback to path resolution\n    try:\n        # Get the current file's directory and traverse up\n        current_file = Path(__file__).resolve()\n        project_root = current_file.parents[fallback_levels - 1]\n\n        # Postcondition (O(1)): ensure we found a valid directory\n        if not project_root.exists() or not project_root.is_dir():\n            raise ConfigurationError(\n                f\"Fallback project root resolution failed: {project_root}\",\n                context={\"fallback_levels\": fallback_levels, \"current_file\": str(current_file)},\n                operation=\"project_root_fallback\",\n                error_code=\"FALLBACK_FAILED\",\n            )\n\n        logger.debug(\"Using fallback project root: {}\", project_root)\n        return project_root\n\n    except (IndexError, OSError) as e:\n        raise ConfigurationError(\n            \"Could not determine project root directory\",\n            context={\"env_var\": env_var_name, \"fallback_levels\": fallback_levels},\n            operation=\"project_root_resolution\",\n            error_code=\"ROOT_RESOLUTION_FAILED\",\n            original_error=e,\n        ) from e\n</code></pre>"},{"location":"metamorphosis/utilities/#metamorphosis.utilities.read_text_file","title":"<code>read_text_file(file_path)</code>","text":"<p>Read a UTF-8 text file with comprehensive error handling.</p> <p>This utility function provides robust file reading with proper error handling and validation. It's designed to be reused across the project for consistent file operations.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Annotated[Path | str, Field(description='Path to the text file to read')]</code> <p>Path to the text file to read (Path object or string).</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Content of the file, stripped of leading/trailing whitespace.</p> <p>Raises:</p> Type Description <code>FileOperationError</code> <p>If file not found, not accessible, or empty.</p> Source code in <code>src/metamorphosis/utilities.py</code> <pre><code>@validate_call\ndef read_text_file(\n    file_path: Annotated[Path | str, Field(description=\"Path to the text file to read\")],\n) -&gt; str:\n    \"\"\"Read a UTF-8 text file with comprehensive error handling.\n\n    This utility function provides robust file reading with proper error handling\n    and validation. It's designed to be reused across the project for consistent\n    file operations.\n\n    Args:\n        file_path: Path to the text file to read (Path object or string).\n\n    Returns:\n        str: Content of the file, stripped of leading/trailing whitespace.\n\n    Raises:\n        FileOperationError: If file not found, not accessible, or empty.\n    \"\"\"\n    # Convert string to Path if necessary\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n\n    logger.debug(\"Reading text file: {}\", file_path)\n\n    # Precondition validation via pydantic is already done by @validate_call\n\n    if not file_path.exists():\n        raise FileOperationError(\n            f\"File not found: {file_path}\",\n            file_path=str(file_path),\n            operation_type=\"read\",\n            operation=\"file_existence_check\",\n            error_code=\"FILE_NOT_FOUND\",\n        )\n\n    if not file_path.is_file():\n        raise FileOperationError(\n            f\"Path is not a file: {file_path}\",\n            file_path=str(file_path),\n            operation_type=\"read\",\n            operation=\"file_type_check\",\n            error_code=\"NOT_A_FILE\",\n        )\n\n    try:\n        content = file_path.read_text(encoding=\"utf-8\").strip()\n    except (OSError, UnicodeDecodeError) as e:\n        raise FileOperationError(\n            f\"Failed to read file: {file_path}\",\n            file_path=str(file_path),\n            operation_type=\"read\",\n            operation=\"file_content_read\",\n            error_code=\"READ_FAILED\",\n            original_error=e,\n        ) from e\n\n    if not content:\n        raise FileOperationError(\n            f\"File is empty: {file_path}\",\n            file_path=str(file_path),\n            operation_type=\"read\",\n            operation=\"content_validation\",\n            error_code=\"EMPTY_FILE\",\n        )\n\n    # Postcondition (O(1)): ensure we return non-empty string\n    if not isinstance(content, str) or not content:\n        raise FileOperationError(\n            \"Postcondition failed: content must be non-empty string\",\n            file_path=str(file_path),\n            operation_type=\"read\",\n            operation=\"postcondition_check\",\n            error_code=\"POSTCONDITION_FAILED\",\n        )\n\n    logger.debug(\"Successfully read file: {} ({} chars)\", file_path, len(content))\n    return content\n</code></pre>"},{"location":"metamorphosis/agents/","title":"Agents Package","text":"<p>The <code>metamorphosis.agents</code> package includes LangGraph-based agent workflows for multi\u2011stage text processing. It offers simple synchronous and streaming examples via a small FastAPI service.</p>"},{"location":"metamorphosis/agents/#package-architecture","title":"Package Architecture","text":"%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#e0f2ff', 'edgeLabelBackground':'#ffffffaa', 'clusterBkg':'#f7fbff', 'fontFamily':'Inter'}}}%% graph TB     subgraph \"FastAPI Service Layer\"         A[agent_service.pyREST API Endpoints]         B[Request/Response ModelsPydantic Schemas]     end      subgraph \"LangGraph Workflow Layer\"         C[WorkflowExecutorexecutor.py]         D[GraphStatestate.py]         E[Workflow Nodesnodes.py]         GB[GraphBuildergraph_builder.py]     end      subgraph \"Processing Integration\"         F[MCP Toolstext_modifiers via MCP]         G[External ServicesWord Cloud Generation]     end      subgraph \"State Management\"         H[Thread-based StorageConversation Persistence]         I[Streaming EventsReal-time Updates]     end      A --&gt; C     A --&gt; B     C --&gt; GB     GB --&gt; D     GB --&gt; E     E --&gt; F     E --&gt; G     D --&gt; H     A --&gt; I"},{"location":"metamorphosis/agents/#core-components","title":"Core Components","text":""},{"location":"metamorphosis/agents/#1-fastapi-service-agent_servicepy","title":"1. FastAPI Service (<code>agent_service.py</code>)","text":"<p>The main service provides REST API endpoints for text processing workflows.</p>"},{"location":"metamorphosis/agents/#2-langgraph-workflow-self_reviewer","title":"2. LangGraph Workflow (<code>self_reviewer/</code>)","text":"<p>Implements a clear multi-step learning example.</p> %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#e0f2ff', 'edgeLabelBackground':'#ffffffaa', 'clusterBkg':'#f7fbff', 'fontFamily':'Inter'}}}%% graph LR     A[START] --&gt; B[Copy Editor]     B --&gt; C[Summarizer]     B --&gt; D[Word Cloud]     B --&gt; E[Achievements Extractor]     E --&gt;|if tools| F[ToolNode: extract_achievements]     F --&gt; G[After Achievements Parser]     G --&gt; H[Review Text Evaluator]     H --&gt;|if tools| I[ToolNode: evaluate_review_text]     I --&gt; J[After Evaluation Parser]     C --&gt; K[END]     D --&gt; K     J --&gt; K"},{"location":"metamorphosis/agents/#module-documentation","title":"Module Documentation","text":""},{"location":"metamorphosis/agents/#core-modules","title":"Core Modules","text":"Module Description Key Components <code>self_reviewer/</code> LangGraph workflow implementation Workflow nodes, state management <code>WorkflowExecutor</code> Entry point for running the workflow initialize, run_workflow <code>GraphBuilder</code> Graph construction nodes, edges, memory <code>WorkflowNodes</code> Node implementations copy edit, summarize, word cloud, tools <code>MCPClientManager</code> MCP client wrapper get_tool, list_available_tools <code>GraphState</code> Workflow state fields and transitions"},{"location":"metamorphosis/agents/#workflow-architecture","title":"Workflow Architecture","text":""},{"location":"metamorphosis/agents/#processing-flow","title":"Processing Flow","text":"sequenceDiagram     participant Client     participant FastAPI     participant LangGraph     participant CopyEditor     participant Summarizer     participant WordCloud      Client-&gt;&gt;FastAPI: POST /stream     FastAPI-&gt;&gt;FastAPI: Generate thread_id     FastAPI-&gt;&gt;LangGraph: run      LangGraph-&gt;&gt;CopyEditor: Process text     CopyEditor-&gt;&gt;LangGraph: Edited text      par Parallel Processing         LangGraph-&gt;&gt;Summarizer: Generate summary         Summarizer-&gt;&gt;LangGraph: Summary result     and         LangGraph-&gt;&gt;WordCloud: Create visualization         WordCloud-&gt;&gt;LangGraph: Image path     end      LangGraph-&gt;&gt;FastAPI: Stream events     FastAPI-&gt;&gt;Client: SSE events  <p>This package aims for clarity, so newcomers can learn agent workflows step by step.</p>"},{"location":"metamorphosis/agents/self_reviewer/","title":"Overview","text":""},{"location":"metamorphosis/agents/self_reviewer/#self-reviewer-package","title":"Self Reviewer Package","text":"<p>The <code>metamorphosis.agents.self_reviewer</code> package provides a simple, beginner\u2011friendly workflow for processing employee self\u2011reviews. It is built with small, focused classes so you can learn how AI agents and LangGraph fit together without extra complexity.</p>"},{"location":"metamorphosis/agents/self_reviewer/#how-it-fits-together","title":"How it fits together","text":"%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#e0f2ff', 'edgeLabelBackground':'#ffffffaa', 'clusterBkg':'#f7fbff', 'fontFamily':'Inter'}}}%% flowchart LR     subgraph User_Code[Your Code]         UX[WorkflowExecutor]     end      subgraph LangGraph_Workflow[LangGraph Workflow]         GB[GraphBuilder]         N[WorkflowNodes]         S[(GraphState)]     end      subgraph Tools[Tools]         MCP[MCPClientManager]         RT[review_tools.py\\n(extract_achievements, evaluate_review_text)]     end      UX --&gt; GB     GB --&gt; N     N --&gt; S     N --&gt; MCP     N --&gt; RT  <ul> <li>WorkflowExecutor: The \"one object\" you use. It initializes everything and runs the workflow.</li> <li>GraphBuilder: Assembles the LangGraph by adding nodes and edges.</li> <li>WorkflowNodes: Small functions that do the work (copy\u2011edit, summarize, word cloud, etc.).</li> <li>GraphState: The shared state dictionary that flows through the graph.</li> <li>MCPClientManager: Finds and calls MCP tools (e.g., copy edit, summarize, word cloud).</li> </ul>"},{"location":"metamorphosis/agents/self_reviewer/#execution-flow-at-a-glance","title":"Execution flow (at a glance)","text":"%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#e0f2ff', 'edgeLabelBackground':'#ffffffaa', 'clusterBkg':'#f7fbff', 'fontFamily':'Inter'}}}%% flowchart LR     START([Start]) --&gt; CE[Copy Editor]     CE --&gt; SUM[Summarizer]     CE --&gt; WC[Word Cloud]      %% Achievements branch     CE --&gt; AE[Achievements Extractor]     AE --&gt;|if tools requested| AET[ToolNode: extract_achievements]     AET --&gt; AEP[After Achievements Parser]      %% Evaluation branch     AEP --&gt; EV[Review Text Evaluator]     EV --&gt;|if tools requested| EVT[ToolNode: evaluate_review_text]     EVT --&gt; EVP[After Evaluation Parser]      SUM --&gt; END([End])     WC --&gt; END     EVP --&gt; END"},{"location":"metamorphosis/agents/self_reviewer/#quick-start","title":"Quick start","text":"<pre><code>import asyncio\nfrom metamorphosis.agents.self_reviewer import WorkflowExecutor\n\nasync def main():\n    executor = WorkflowExecutor()\n    await executor.initialize()\n    result = await executor.run_workflow(\n        \"I improved the data pipeline and reduced costs by 15%.\",\n        thread_id=\"demo-1\",\n    )\n    print(result)\n\nasyncio.run(main())\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/#learn-the-pieces","title":"Learn the pieces","text":"<ul> <li>Workflow interface: <code>WorkflowExecutor</code> (how you run the graph)</li> <li>Graph assembly: <code>GraphBuilder</code> (nodes + edges + memory)</li> <li>Node logic: <code>WorkflowNodes</code> (copy edit, summarize, word cloud, agent/tool steps)</li> <li>State model: <code>GraphState</code> (what data moves through the graph)</li> <li>MCP client: <code>MCPClientManager</code> (discovers and calls MCP tools)</li> </ul> <p>See the dedicated pages for details and examples:</p> <ul> <li>Workflow: <code>WorkflowExecutor</code></li> <li>Graph: <code>GraphBuilder</code></li> <li>Nodes: <code>WorkflowNodes</code></li> <li>State: <code>GraphState</code></li> <li>MCP: <code>MCPClientManager</code></li> </ul>"},{"location":"metamorphosis/agents/self_reviewer/GraphBuilder/","title":"GraphBuilder","text":""},{"location":"metamorphosis/agents/self_reviewer/GraphBuilder/#graphbuilder","title":"GraphBuilder","text":"<p>Builds and configures the LangGraph workflow.</p> <p>This class is responsible for constructing the complete workflow graph that orchestrates the self-review processing pipeline. It handles node registration, edge configuration, and graph compilation.</p> <p>Attributes:</p> Name Type Description <code>nodes</code> <p>The workflow nodes instance containing all node implementations.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/graph_builder.py</code> <pre><code>class GraphBuilder:\n    \"\"\"Builds and configures the LangGraph workflow.\n\n    This class is responsible for constructing the complete workflow graph\n    that orchestrates the self-review processing pipeline. It handles node\n    registration, edge configuration, and graph compilation.\n\n    Attributes:\n        nodes: The workflow nodes instance containing all node implementations.\n    \"\"\"\n\n    def __init__(self, nodes: WorkflowNodes) -&gt; None:\n        \"\"\"Initialize the graph builder.\n\n        Args:\n            nodes: Initialized workflow nodes instance.\n        \"\"\"\n        self.nodes = nodes\n\n    @validate_call\n    @ensure(lambda result: result is not None, \"Graph must be built successfully\")\n    async def build(self) -&gt; StateGraph:\n        \"\"\"Build and configure the LangGraph workflow for self-review processing.\n\n        This method constructs the complete workflow graph that orchestrates the\n        self-review processing pipeline. It initializes components, adds nodes,\n        defines edges, and compiles the graph with memory support.\n\n        Returns:\n            StateGraph: Compiled graph ready for execution.\n\n        Raises:\n            Exception: If graph construction or compilation fails.\n        \"\"\"\n        logger.debug(\"Building LangGraph workflow\")\n\n        builder = StateGraph(GraphState)\n\n        # Add all processing nodes to the graph\n        self._add_nodes(builder)\n\n        # Define the workflow edges (execution flow)\n        self._add_edges(builder)\n\n        # Add conditional edges for decision points\n        self._add_conditional_edges(builder)\n\n        # Compile the graph with memory support\n        memory = InMemorySaver()\n        graph = builder.compile(checkpointer=memory)\n\n        # Generate visual representation for documentation\n        self._generate_graph_visualization(graph)\n\n        logger.debug(\"LangGraph workflow built successfully\")\n        return graph\n\n    # -----------------------------------------------------------------------------\n\n    def _add_nodes(self, builder: StateGraph) -&gt; None:\n        \"\"\"Add all processing nodes to the graph.\n\n        Args:\n            builder: The StateGraph builder instance.\n        \"\"\"\n        # Core processing nodes\n        builder.add_node(\"copy_editor\", self.nodes.copy_editor_node)\n        builder.add_node(\"summarizer\", self.nodes.summarizer_node)\n        builder.add_node(\"wordcloud\", self.nodes.wordcloud_node)\n\n        # Agent-based nodes\n        builder.add_node(\"achievements_extractor\", self.nodes.achievements_extractor_node)\n        builder.add_node(\"review_text_evaluator\", self.nodes.review_text_evaluator_node)\n\n        # Post-processing nodes\n        builder.add_node(\"after_achievements_parser\", self.nodes.after_achievements_parser)\n        builder.add_node(\"after_evaluation_parser\", self.nodes.after_evaluation_parser)\n\n        # Tool nodes\n        builder.add_node(\n            \"achievements_extractor_tool_node\", self.nodes.achievements_extractor_tool_node\n        )\n        builder.add_node(\n            \"review_text_evaluator_tool_node\", self.nodes.review_text_evaluator_tool_node\n        )\n\n    # -----------------------------------------------------------------------------\n\n    def _add_edges(self, builder: StateGraph) -&gt; None:\n        \"\"\"Add basic edges to the graph.\n\n        Args:\n            builder: The StateGraph builder instance.\n        \"\"\"\n        # Main processing flow\n        builder.add_edge(START, \"copy_editor\")\n        builder.add_edge(\"copy_editor\", \"summarizer\")\n        builder.add_edge(\"copy_editor\", \"wordcloud\")\n        builder.add_edge(\"summarizer\", END)\n        builder.add_edge(\"wordcloud\", END)\n\n        # Achievement extraction flow\n        builder.add_edge(\"copy_editor\", \"achievements_extractor\")\n        builder.add_edge(\"achievements_extractor_tool_node\", \"after_achievements_parser\")\n\n        # Review evaluation flow\n        builder.add_edge(\"after_achievements_parser\", \"review_text_evaluator\")\n        builder.add_edge(\"review_text_evaluator_tool_node\", \"after_evaluation_parser\")\n        builder.add_edge(\"after_evaluation_parser\", END)\n\n    # -----------------------------------------------------------------------------\n\n    def _add_conditional_edges(self, builder: StateGraph) -&gt; None:\n        \"\"\"Add conditional edges for decision points.\n\n        Args:\n            builder: The StateGraph builder instance.\n        \"\"\"\n        # Achievement extraction decision point\n        builder.add_conditional_edges(\n            \"achievements_extractor\",\n            self.nodes.should_call_achievements_extractor_tools,\n            {\n                \"tools\": \"achievements_extractor_tool_node\",\n                \"no_tools\": END,\n            },\n        )\n\n        # Review evaluation decision point\n        builder.add_conditional_edges(\n            \"review_text_evaluator\",\n            self.nodes.should_call_review_text_evaluator_tools,\n            {\n                \"tools\": \"review_text_evaluator_tool_node\",\n                \"no_tools\": END,\n            },\n        )\n\n    # -----------------------------------------------------------------------------\n\n    def _generate_graph_visualization(self, graph: StateGraph) -&gt; None:\n        \"\"\"Generate visual representation for documentation.\n\n        Args:\n            graph: The compiled graph to visualize.\n        \"\"\"\n        try:\n            graph.get_graph().draw_mermaid_png(output_file_path=\"executor_graph.png\")\n            logger.debug(\"Generated graph visualization: executor_graph.png\")\n        except Exception as e:\n            logger.warning(\"Failed to generate graph visualization: {}\", e)\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/GraphBuilder/#metamorphosis.agents.self_reviewer.graph_builder.GraphBuilder.__init__","title":"<code>__init__(nodes)</code>","text":"<p>Initialize the graph builder.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>WorkflowNodes</code> <p>Initialized workflow nodes instance.</p> required Source code in <code>src/metamorphosis/agents/self_reviewer/graph_builder.py</code> <pre><code>def __init__(self, nodes: WorkflowNodes) -&gt; None:\n    \"\"\"Initialize the graph builder.\n\n    Args:\n        nodes: Initialized workflow nodes instance.\n    \"\"\"\n    self.nodes = nodes\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/GraphBuilder/#metamorphosis.agents.self_reviewer.graph_builder.GraphBuilder.build","title":"<code>build()</code>  <code>async</code>","text":"<p>Build and configure the LangGraph workflow for self-review processing.</p> <p>This method constructs the complete workflow graph that orchestrates the self-review processing pipeline. It initializes components, adds nodes, defines edges, and compiles the graph with memory support.</p> <p>Returns:</p> Name Type Description <code>StateGraph</code> <code>StateGraph</code> <p>Compiled graph ready for execution.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If graph construction or compilation fails.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/graph_builder.py</code> <pre><code>@validate_call\n@ensure(lambda result: result is not None, \"Graph must be built successfully\")\nasync def build(self) -&gt; StateGraph:\n    \"\"\"Build and configure the LangGraph workflow for self-review processing.\n\n    This method constructs the complete workflow graph that orchestrates the\n    self-review processing pipeline. It initializes components, adds nodes,\n    defines edges, and compiles the graph with memory support.\n\n    Returns:\n        StateGraph: Compiled graph ready for execution.\n\n    Raises:\n        Exception: If graph construction or compilation fails.\n    \"\"\"\n    logger.debug(\"Building LangGraph workflow\")\n\n    builder = StateGraph(GraphState)\n\n    # Add all processing nodes to the graph\n    self._add_nodes(builder)\n\n    # Define the workflow edges (execution flow)\n    self._add_edges(builder)\n\n    # Add conditional edges for decision points\n    self._add_conditional_edges(builder)\n\n    # Compile the graph with memory support\n    memory = InMemorySaver()\n    graph = builder.compile(checkpointer=memory)\n\n    # Generate visual representation for documentation\n    self._generate_graph_visualization(graph)\n\n    logger.debug(\"LangGraph workflow built successfully\")\n    return graph\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/GraphBuilder/#purpose","title":"Purpose","text":"<p>Builds the LangGraph by adding nodes, wiring edges, and enabling a simple in\u2011memory checkpoint.</p>"},{"location":"metamorphosis/agents/self_reviewer/GraphBuilder/#workflow-outline","title":"Workflow outline","text":"%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#e0f2ff', 'edgeLabelBackground':'#ffffffaa', 'clusterBkg':'#f7fbff', 'fontFamily':'Inter'}}}%% graph LR     START([START]) --&gt; CE[copy_editor]     CE --&gt; SUM[summarizer]     CE --&gt; WC[wordcloud]     CE --&gt; AE[achievements_extractor]     AE --&gt;|tools?| AET[achievements_extractor_tool_node]     AET --&gt; AEP[after_achievements_parser]     AEP --&gt; EV[review_text_evaluator]     EV --&gt;|tools?| EVT[review_text_evaluator_tool_node]     EVT --&gt; EVP[after_evaluation_parser]     SUM --&gt; END([END])     WC --&gt; END     EVP --&gt; END"},{"location":"metamorphosis/agents/self_reviewer/GraphState/","title":"GraphState","text":""},{"location":"metamorphosis/agents/self_reviewer/GraphState/#graphstate","title":"GraphState","text":"<p>               Bases: <code>TypedDict</code></p> <p>State definition for the LangGraph workflow.</p> <p>This TypedDict defines the structure of the state that flows through the LangGraph, containing all the data needed at each step of the workflow.</p> <p>Attributes:</p> Name Type Description <code>original_text</code> <code>str</code> <p>The original self-review input text from the employee.</p> <code>copy_edited_text</code> <code>Optional[str]</code> <p>The grammar and clarity improved version.</p> <code>summary</code> <code>Optional[str]</code> <p>The abstractive summary of the key insights.</p> <code>word_cloud_path</code> <code>Optional[str]</code> <p>Path to the generated word cloud image file.</p> <code>achievements</code> <code>Optional[AchievementsList]</code> <p>The key achievements extracted from the text by an agent accessing an achievements extraction langgraph tool.</p> <code>review_scorecard</code> <code>Optional[ReviewScorecard]</code> <p>The review scorecard generated from the text by an agent accessing a review scorecard generation langgraph tool.</p> <code>review_complete</code> <code>Optional[bool]</code> <p>A flag indicating if the achievements list has at least 3 items.</p> <code>messages</code> <code>Annotated[list, add_messages]</code> <p>List of messages for agent communication.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/state.py</code> <pre><code>class GraphState(TypedDict):\n    \"\"\"State definition for the LangGraph workflow.\n\n    This TypedDict defines the structure of the state that flows through the LangGraph,\n    containing all the data needed at each step of the workflow.\n\n    Attributes:\n        original_text: The original self-review input text from the employee.\n        copy_edited_text: The grammar and clarity improved version.\n        summary: The abstractive summary of the key insights.\n        word_cloud_path: Path to the generated word cloud image file.\n        achievements: The key achievements extracted from the text by an agent accessing an achievements extraction langgraph tool.\n        review_scorecard: The review scorecard generated from the text by an agent accessing a review scorecard generation langgraph tool.\n        review_complete: A flag indicating if the achievements list has at least 3 items.\n        messages: List of messages for agent communication.\n    \"\"\"\n\n    original_text: str\n    copy_edited_text: Optional[str]\n    summary: Optional[str]\n    word_cloud_path: Optional[str]\n    achievements: Optional[AchievementsList]\n    review_scorecard: Optional[ReviewScorecard]\n    messages: Annotated[list, add_messages]\n    review_complete: Optional[bool]\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/GraphState/#fields","title":"Fields","text":"<ul> <li><code>original_text</code>: input provided by the user</li> <li><code>copy_edited_text</code>: improved text from the copy editor</li> <li><code>summary</code>: the short summary</li> <li><code>word_cloud_path</code>: image path returned by the word cloud tool</li> <li><code>achievements</code>: structured achievements list</li> <li><code>review_scorecard</code>: structured evaluation</li> <li><code>messages</code>: conversation messages for agent/tool steps</li> <li><code>review_complete</code>: True when we have enough achievements</li> </ul>"},{"location":"metamorphosis/agents/self_reviewer/GraphState/#state-diagram","title":"State diagram","text":"%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#e0f2ff', 'edgeLabelBackground':'#ffffffaa', 'clusterBkg':'#f7fbff', 'fontFamily':'Inter'}}}%% stateDiagram-v2     [*] --&gt; Initialized     Initialized --&gt; CopyEditing: User submits text     CopyEditing --&gt; ParallelProcessing: Text edited      state ParallelProcessing {         [*] --&gt; Summarizing         [*] --&gt; WordCloudGen         Summarizing --&gt; SummaryComplete         WordCloudGen --&gt; WordCloudComplete         SummaryComplete --&gt; [*]         WordCloudComplete --&gt; [*]     }      ParallelProcessing --&gt; Achievements     Achievements --&gt; Evaluation     Evaluation --&gt; Complete     Complete --&gt; [*]"},{"location":"metamorphosis/agents/self_reviewer/MCPClientManager/","title":"MCPClientManager","text":""},{"location":"metamorphosis/agents/self_reviewer/MCPClientManager/#mcpclientmanager","title":"MCPClientManager","text":"<p>Manages MCP client connections and tool discovery.</p> <p>This class encapsulates all MCP client functionality including connection management, tool discovery, and tool access. It provides a clean interface for the workflow system to interact with MCP servers.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>Optional[MultiServerMCPClient]</code> <p>The underlying MultiServerMCPClient instance.</p> <code>tools</code> <code>List</code> <p>List of available tools from MCP servers.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/client.py</code> <pre><code>class MCPClientManager:\n    \"\"\"Manages MCP client connections and tool discovery.\n\n    This class encapsulates all MCP client functionality including connection\n    management, tool discovery, and tool access. It provides a clean interface\n    for the workflow system to interact with MCP servers.\n\n    Attributes:\n        client: The underlying MultiServerMCPClient instance.\n        tools: List of available tools from MCP servers.\n    \"\"\"\n\n    def __init__(self, config: Optional[dict] = None) -&gt; None:\n        \"\"\"Initialize the MCP client manager.\n\n        Args:\n            config: Optional configuration dictionary. If not provided,\n                   uses environment variables for configuration.\n        \"\"\"\n        self.config = config or {}\n        self.client: Optional[MultiServerMCPClient] = None\n        self.tools: List = []\n        self._initialized = False\n\n    @validate_call\n    @require(lambda: True, \"Environment should be properly configured\")\n    @ensure(lambda result: result is not None, \"MCP client must be created successfully\")\n    def _create_client(self) -&gt; MultiServerMCPClient:\n        \"\"\"Create MCP client with environment-based configuration.\n\n        Returns:\n            MultiServerMCPClient: Configured client instance.\n        \"\"\"\n        host = self.config.get(\"mcp_server_host\") or os.getenv(\"MCP_SERVER_HOST\", \"localhost\")\n        port = self.config.get(\"mcp_server_port\") or os.getenv(\"MCP_SERVER_PORT\", \"3333\")\n        url = f\"http://{host}:{port}/mcp\"\n\n        logger.debug(\"Configuring MCP client for URL: {}\", url)\n\n        return MultiServerMCPClient(\n            {\n                \"text_modifier_mcp_server\": {\n                    \"url\": url,\n                    \"transport\": \"streamable_http\",\n                }\n            }\n        )\n\n    @validate_call\n    @ensure(lambda self: len(self.tools) &gt; 0, \"Tools must be retrieved from MCP server\")\n    async def initialize(self) -&gt; None:\n        \"\"\"Initialize MCP client and retrieve available tools.\n\n        This method establishes the connection to MCP servers and retrieves\n        the list of available tools that can be used in the workflow.\n\n        Raises:\n            ConnectionError: If unable to connect to MCP servers.\n            Exception: If tool retrieval fails.\n        \"\"\"\n        if self._initialized:\n            return\n\n        logger.debug(\"Initializing MCP client manager\")\n\n        # Create the client\n        self.client = self._create_client()\n\n        # Retrieve available tools\n        self.tools = await self.client.get_tools()\n        tool_names = [tool.name for tool in self.tools]\n\n        # Postcondition: ensure tools were retrieved\n        if not self.tools or not isinstance(self.tools, list):\n            raise_postcondition_error(\n                \"No tools retrieved from MCP server\",\n                context={\"tools_count\": len(self.tools) if self.tools else 0},\n                operation=\"mcp_tool_initialization\",\n            )\n\n        logger.info(\"Available MCP tools: {}\", tool_names)\n        self._initialized = True\n\n    @validate_call\n    @require(\n        lambda tool_name: isinstance(tool_name, str) and len(tool_name) &gt; 0,\n        \"Tool name must be non-empty string\",\n    )\n    @ensure(lambda result: result is not None, \"Tool must be found\")\n    def get_tool(self, tool_name: str):\n        \"\"\"Get a tool by name from the available tools.\n\n        Args:\n            tool_name: Name of the tool to retrieve.\n\n        Returns:\n            Tool: The requested tool instance.\n\n        Raises:\n            ValueError: If tool is not found.\n            RuntimeError: If client is not initialized.\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"MCP client manager not initialized. Call initialize() first.\")\n\n        tool = next((t for t in self.tools if t.name == tool_name), None)\n        if not tool:\n            raise_mcp_tool_error(\n                f\"{tool_name} tool not found\", tool_name=tool_name, operation=\"tool_lookup\"\n            )\n        return tool\n\n    def list_available_tools(self) -&gt; List[str]:\n        \"\"\"Get list of available tool names.\n\n        Returns:\n            List[str]: Names of all available tools.\n\n        Raises:\n            RuntimeError: If client is not initialized.\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"MCP client manager not initialized. Call initialize() first.\")\n\n        return [tool.name for tool in self.tools]\n\n    async def close(self) -&gt; None:\n        \"\"\"Close the MCP client connection.\n\n        This method should be called when the client manager is no longer needed\n        to properly clean up resources.\n        \"\"\"\n        if self.client:\n            # Note: MultiServerMCPClient doesn't have a close method in current version\n            # but we include this for future compatibility\n            logger.debug(\"Closing MCP client connection\")\n            self.client = None\n        self._initialized = False\n\n    @property\n    def is_initialized(self) -&gt; bool:\n        \"\"\"Check if the client manager is initialized.\n\n        Returns:\n            bool: True if initialized, False otherwise.\n        \"\"\"\n        return self._initialized\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/MCPClientManager/#metamorphosis.agents.self_reviewer.client.MCPClientManager.is_initialized","title":"<code>is_initialized</code>  <code>property</code>","text":"<p>Check if the client manager is initialized.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if initialized, False otherwise.</p>"},{"location":"metamorphosis/agents/self_reviewer/MCPClientManager/#metamorphosis.agents.self_reviewer.client.MCPClientManager.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize the MCP client manager.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[dict]</code> <p>Optional configuration dictionary. If not provided,    uses environment variables for configuration.</p> <code>None</code> Source code in <code>src/metamorphosis/agents/self_reviewer/client.py</code> <pre><code>def __init__(self, config: Optional[dict] = None) -&gt; None:\n    \"\"\"Initialize the MCP client manager.\n\n    Args:\n        config: Optional configuration dictionary. If not provided,\n               uses environment variables for configuration.\n    \"\"\"\n    self.config = config or {}\n    self.client: Optional[MultiServerMCPClient] = None\n    self.tools: List = []\n    self._initialized = False\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/MCPClientManager/#metamorphosis.agents.self_reviewer.client.MCPClientManager.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the MCP client connection.</p> <p>This method should be called when the client manager is no longer needed to properly clean up resources.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/client.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the MCP client connection.\n\n    This method should be called when the client manager is no longer needed\n    to properly clean up resources.\n    \"\"\"\n    if self.client:\n        # Note: MultiServerMCPClient doesn't have a close method in current version\n        # but we include this for future compatibility\n        logger.debug(\"Closing MCP client connection\")\n        self.client = None\n    self._initialized = False\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/MCPClientManager/#metamorphosis.agents.self_reviewer.client.MCPClientManager.get_tool","title":"<code>get_tool(tool_name)</code>","text":"<p>Get a tool by name from the available tools.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Tool</code> <p>The requested tool instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tool is not found.</p> <code>RuntimeError</code> <p>If client is not initialized.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/client.py</code> <pre><code>@validate_call\n@require(\n    lambda tool_name: isinstance(tool_name, str) and len(tool_name) &gt; 0,\n    \"Tool name must be non-empty string\",\n)\n@ensure(lambda result: result is not None, \"Tool must be found\")\ndef get_tool(self, tool_name: str):\n    \"\"\"Get a tool by name from the available tools.\n\n    Args:\n        tool_name: Name of the tool to retrieve.\n\n    Returns:\n        Tool: The requested tool instance.\n\n    Raises:\n        ValueError: If tool is not found.\n        RuntimeError: If client is not initialized.\n    \"\"\"\n    if not self._initialized:\n        raise RuntimeError(\"MCP client manager not initialized. Call initialize() first.\")\n\n    tool = next((t for t in self.tools if t.name == tool_name), None)\n    if not tool:\n        raise_mcp_tool_error(\n            f\"{tool_name} tool not found\", tool_name=tool_name, operation=\"tool_lookup\"\n        )\n    return tool\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/MCPClientManager/#metamorphosis.agents.self_reviewer.client.MCPClientManager.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Initialize MCP client and retrieve available tools.</p> <p>This method establishes the connection to MCP servers and retrieves the list of available tools that can be used in the workflow.</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If unable to connect to MCP servers.</p> <code>Exception</code> <p>If tool retrieval fails.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/client.py</code> <pre><code>@validate_call\n@ensure(lambda self: len(self.tools) &gt; 0, \"Tools must be retrieved from MCP server\")\nasync def initialize(self) -&gt; None:\n    \"\"\"Initialize MCP client and retrieve available tools.\n\n    This method establishes the connection to MCP servers and retrieves\n    the list of available tools that can be used in the workflow.\n\n    Raises:\n        ConnectionError: If unable to connect to MCP servers.\n        Exception: If tool retrieval fails.\n    \"\"\"\n    if self._initialized:\n        return\n\n    logger.debug(\"Initializing MCP client manager\")\n\n    # Create the client\n    self.client = self._create_client()\n\n    # Retrieve available tools\n    self.tools = await self.client.get_tools()\n    tool_names = [tool.name for tool in self.tools]\n\n    # Postcondition: ensure tools were retrieved\n    if not self.tools or not isinstance(self.tools, list):\n        raise_postcondition_error(\n            \"No tools retrieved from MCP server\",\n            context={\"tools_count\": len(self.tools) if self.tools else 0},\n            operation=\"mcp_tool_initialization\",\n        )\n\n    logger.info(\"Available MCP tools: {}\", tool_names)\n    self._initialized = True\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/MCPClientManager/#metamorphosis.agents.self_reviewer.client.MCPClientManager.list_available_tools","title":"<code>list_available_tools()</code>","text":"<p>Get list of available tool names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Names of all available tools.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If client is not initialized.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/client.py</code> <pre><code>def list_available_tools(self) -&gt; List[str]:\n    \"\"\"Get list of available tool names.\n\n    Returns:\n        List[str]: Names of all available tools.\n\n    Raises:\n        RuntimeError: If client is not initialized.\n    \"\"\"\n    if not self._initialized:\n        raise RuntimeError(\"MCP client manager not initialized. Call initialize() first.\")\n\n    return [tool.name for tool in self.tools]\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/MCPClientManager/#purpose","title":"Purpose","text":"<p>Wraps <code>MultiServerMCPClient</code> so nodes can look up tools by name and call them.</p>"},{"location":"metamorphosis/agents/self_reviewer/MCPClientManager/#tool-lookup-flow","title":"Tool lookup flow","text":"%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#e0f2ff', 'edgeLabelBackground':'#ffffffaa', 'clusterBkg':'#f7fbff', 'fontFamily':'Inter'}}}%% sequenceDiagram     participant Node     participant MCP as MCPClientManager     participant Srv as MCP Server      Node-&gt;&gt;MCP: get_tool(\"copy_edit\")     MCP-&gt;&gt;Srv: get_tools()     Srv--&gt;&gt;MCP: [copy_edit, abstractive_summarize, word_cloud]     MCP--&gt;&gt;Node: Tool(copy_edit)     Node-&gt;&gt;Tool: ainvoke({text})     Tool--&gt;&gt;Node: result"},{"location":"metamorphosis/agents/self_reviewer/WorkflowExecutor/","title":"WorkflowExecutor","text":""},{"location":"metamorphosis/agents/self_reviewer/WorkflowExecutor/#workflowexecutor","title":"WorkflowExecutor","text":"<p>High-level interface for executing review workflows.</p> <p>This class provides a simple, clean interface for running self-review workflows. It handles all the complexity of component initialization, graph construction, and execution orchestration.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>Optional configuration dictionary.</p> <code>mcp_client</code> <code>Optional[MCPClientManager]</code> <p>The MCP client manager instance.</p> <code>nodes</code> <code>Optional[WorkflowNodes]</code> <p>The workflow nodes instance.</p> <code>graph_builder</code> <code>Optional[GraphBuilder]</code> <p>The graph builder instance.</p> <code>graph</code> <p>The compiled workflow graph.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/executor.py</code> <pre><code>class WorkflowExecutor:\n    \"\"\"High-level interface for executing review workflows.\n\n    This class provides a simple, clean interface for running self-review\n    workflows. It handles all the complexity of component initialization,\n    graph construction, and execution orchestration.\n\n    Attributes:\n        config: Optional configuration dictionary.\n        mcp_client: The MCP client manager instance.\n        nodes: The workflow nodes instance.\n        graph_builder: The graph builder instance.\n        graph: The compiled workflow graph.\n    \"\"\"\n\n    def __init__(self, config: Optional[dict] = None) -&gt; None:\n        \"\"\"Initialize the workflow executor.\n\n        Args:\n            config: Optional configuration dictionary for customizing behavior.\n        \"\"\"\n        self.config = config or {}\n        self.mcp_client: Optional[MCPClientManager] = None\n        self.nodes: Optional[WorkflowNodes] = None\n        self.graph_builder: Optional[GraphBuilder] = None\n        self.graph = None\n        self._initialized = False\n\n    @validate_call\n    async def initialize(self) -&gt; None:\n        \"\"\"Initialize all components and build the workflow graph.\n\n        This method sets up all the necessary components including the MCP client,\n        workflow nodes, graph builder, and compiles the final workflow graph.\n\n        Raises:\n            Exception: If initialization fails at any step.\n        \"\"\"\n        if self._initialized:\n            return\n\n        logger.info(\"Initializing WorkflowExecutor\")\n\n        # Initialize MCP client\n        self.mcp_client = MCPClientManager(self.config)\n        await self.mcp_client.initialize()\n\n        # Initialize workflow nodes\n        self.nodes = WorkflowNodes(self.mcp_client)\n\n        # Initialize graph builder\n        self.graph_builder = GraphBuilder(self.nodes)\n\n        # Build the workflow graph\n        self.graph = await self.graph_builder.build()\n\n        self._initialized = True\n        logger.info(\"WorkflowExecutor initialized successfully\")\n\n    @validate_call\n    @require(lambda self: self.graph is not None, \"Graph must not be None\")\n    @require(lambda review_text: len(review_text.strip()) &gt; 0, \"Review text must not be empty\")\n    @require(lambda thread_id: len(thread_id.strip()) &gt; 0, \"Thread ID must not be empty\")\n    async def run_workflow(\n        self,\n        review_text: Annotated[str, Field(min_length=1)],\n        thread_id: Annotated[str, Field(min_length=1)] = \"main\",\n    ) -&gt; dict | None:\n        \"\"\"Execute the LangGraph workflow for processing self-review text.\n\n        This is the main execution method that runs the complete self-review\n        processing pipeline.\n\n        Args:\n            review_text: The self-review text to process.\n            thread_id: Unique identifier for state persistence.\n\n        Returns:\n            dict | None: Complete processed results or None if execution fails.\n\n        Raises:\n            RuntimeError: If executor is not initialized.\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"WorkflowExecutor not initialized. Call initialize() first.\")\n\n        logger.info(\"Running workflow (thread_id={}, text_length={})\", thread_id, len(review_text))\n\n        try:\n            result = await self.graph.ainvoke(\n                {\"original_text\": review_text}, config={\"configurable\": {\"thread_id\": thread_id}}\n            )\n\n            self._validate_graph_result(result)\n            logger.info(\"Workflow execution completed successfully (thread_id={})\", thread_id)\n            return result\n\n        except Exception as e:\n            logger.error(\"Error running workflow (thread_id={}): {}\", thread_id, e)\n            return None\n\n    # -----------------------------------------------------------------------------\n\n    def _validate_graph_result(self, result: dict) -&gt; None:\n        \"\"\"Validate graph execution result structure.\n\n        Args:\n            result: The result to validate.\n\n        Raises:\n            ValueError: If result structure is invalid.\n        \"\"\"\n        if not isinstance(result, dict) or \"original_text\" not in result:\n            raise_postcondition_error(\n                \"Graph execution result validation failed\",\n                context={\n                    \"result_type\": type(result).__name__,\n                    \"has_original_text\": \"original_text\" in result\n                    if isinstance(result, dict)\n                    else False,\n                },\n                operation=\"graph_execution_validation\",\n            )\n\n    # -----------------------------------------------------------------------------\n\n    @validate_call\n    async def test_workflow(self) -&gt; dict | None:\n        \"\"\"Test function to validate the workflow with sample data.\n\n        This method loads a sample review from the data_engineer_review.md file\n        and processes it through the complete workflow to validate functionality.\n\n        Returns:\n            dict | None: Complete processed results from the workflow or None if failed.\n\n        Raises:\n            RuntimeError: If executor is not initialized.\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"WorkflowExecutor not initialized. Call initialize() first.\")\n\n        logger.info(\"Running workflow test with sample data\")\n\n        project_root = get_project_root()\n        review_path = project_root / \"sample_reviews\" / \"data_engineer_review.md\"\n        sample_text = read_text_file(review_path)\n\n        return await self.run_workflow(sample_text, thread_id=\"test\")\n\n    # -----------------------------------------------------------------------------\n\n    def get_graph_visualization(self) -&gt; str:\n        \"\"\"Get the path to the generated graph visualization.\n\n        Returns:\n            str: Path to the graph visualization PNG file.\n\n        Raises:\n            RuntimeError: If executor is not initialized.\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"WorkflowExecutor not initialized. Call initialize() first.\")\n\n        return \"self_reviewer_agents.png\"\n\n    # -----------------------------------------------------------------------------\n\n    def list_available_tools(self) -&gt; list[str]:\n        \"\"\"Get list of available MCP tools.\n\n        Returns:\n            list[str]: Names of all available MCP tools.\n\n        Raises:\n            RuntimeError: If executor is not initialized.\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"WorkflowExecutor not initialized. Call initialize() first.\")\n\n        return self.mcp_client.list_available_tools()\n\n    # -----------------------------------------------------------------------------\n\n    async def close(self) -&gt; None:\n        \"\"\"Clean up resources and close connections.\n\n        This method should be called when the executor is no longer needed\n        to properly clean up resources.\n        \"\"\"\n        if self.mcp_client:\n            await self.mcp_client.close()\n\n        self._initialized = False\n        logger.debug(\"WorkflowExecutor closed\")\n\n    # -----------------------------------------------------------------------------\n\n    @property\n    def is_initialized(self) -&gt; bool:\n        \"\"\"Check if the executor is initialized.\n\n        Returns:\n            bool: True if initialized, False otherwise.\n        \"\"\"\n        return self._initialized\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowExecutor/#metamorphosis.agents.self_reviewer.executor.WorkflowExecutor.is_initialized","title":"<code>is_initialized</code>  <code>property</code>","text":"<p>Check if the executor is initialized.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if initialized, False otherwise.</p>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowExecutor/#metamorphosis.agents.self_reviewer.executor.WorkflowExecutor.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize the workflow executor.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[dict]</code> <p>Optional configuration dictionary for customizing behavior.</p> <code>None</code> Source code in <code>src/metamorphosis/agents/self_reviewer/executor.py</code> <pre><code>def __init__(self, config: Optional[dict] = None) -&gt; None:\n    \"\"\"Initialize the workflow executor.\n\n    Args:\n        config: Optional configuration dictionary for customizing behavior.\n    \"\"\"\n    self.config = config or {}\n    self.mcp_client: Optional[MCPClientManager] = None\n    self.nodes: Optional[WorkflowNodes] = None\n    self.graph_builder: Optional[GraphBuilder] = None\n    self.graph = None\n    self._initialized = False\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowExecutor/#metamorphosis.agents.self_reviewer.executor.WorkflowExecutor.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Clean up resources and close connections.</p> <p>This method should be called when the executor is no longer needed to properly clean up resources.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/executor.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Clean up resources and close connections.\n\n    This method should be called when the executor is no longer needed\n    to properly clean up resources.\n    \"\"\"\n    if self.mcp_client:\n        await self.mcp_client.close()\n\n    self._initialized = False\n    logger.debug(\"WorkflowExecutor closed\")\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowExecutor/#metamorphosis.agents.self_reviewer.executor.WorkflowExecutor.get_graph_visualization","title":"<code>get_graph_visualization()</code>","text":"<p>Get the path to the generated graph visualization.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the graph visualization PNG file.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If executor is not initialized.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/executor.py</code> <pre><code>def get_graph_visualization(self) -&gt; str:\n    \"\"\"Get the path to the generated graph visualization.\n\n    Returns:\n        str: Path to the graph visualization PNG file.\n\n    Raises:\n        RuntimeError: If executor is not initialized.\n    \"\"\"\n    if not self._initialized:\n        raise RuntimeError(\"WorkflowExecutor not initialized. Call initialize() first.\")\n\n    return \"self_reviewer_agents.png\"\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowExecutor/#metamorphosis.agents.self_reviewer.executor.WorkflowExecutor.initialize","title":"<code>initialize()</code>  <code>async</code>","text":"<p>Initialize all components and build the workflow graph.</p> <p>This method sets up all the necessary components including the MCP client, workflow nodes, graph builder, and compiles the final workflow graph.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If initialization fails at any step.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/executor.py</code> <pre><code>@validate_call\nasync def initialize(self) -&gt; None:\n    \"\"\"Initialize all components and build the workflow graph.\n\n    This method sets up all the necessary components including the MCP client,\n    workflow nodes, graph builder, and compiles the final workflow graph.\n\n    Raises:\n        Exception: If initialization fails at any step.\n    \"\"\"\n    if self._initialized:\n        return\n\n    logger.info(\"Initializing WorkflowExecutor\")\n\n    # Initialize MCP client\n    self.mcp_client = MCPClientManager(self.config)\n    await self.mcp_client.initialize()\n\n    # Initialize workflow nodes\n    self.nodes = WorkflowNodes(self.mcp_client)\n\n    # Initialize graph builder\n    self.graph_builder = GraphBuilder(self.nodes)\n\n    # Build the workflow graph\n    self.graph = await self.graph_builder.build()\n\n    self._initialized = True\n    logger.info(\"WorkflowExecutor initialized successfully\")\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowExecutor/#metamorphosis.agents.self_reviewer.executor.WorkflowExecutor.list_available_tools","title":"<code>list_available_tools()</code>","text":"<p>Get list of available MCP tools.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Names of all available MCP tools.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If executor is not initialized.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/executor.py</code> <pre><code>def list_available_tools(self) -&gt; list[str]:\n    \"\"\"Get list of available MCP tools.\n\n    Returns:\n        list[str]: Names of all available MCP tools.\n\n    Raises:\n        RuntimeError: If executor is not initialized.\n    \"\"\"\n    if not self._initialized:\n        raise RuntimeError(\"WorkflowExecutor not initialized. Call initialize() first.\")\n\n    return self.mcp_client.list_available_tools()\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowExecutor/#metamorphosis.agents.self_reviewer.executor.WorkflowExecutor.run_workflow","title":"<code>run_workflow(review_text, thread_id='main')</code>  <code>async</code>","text":"<p>Execute the LangGraph workflow for processing self-review text.</p> <p>This is the main execution method that runs the complete self-review processing pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>review_text</code> <code>Annotated[str, Field(min_length=1)]</code> <p>The self-review text to process.</p> required <code>thread_id</code> <code>Annotated[str, Field(min_length=1)]</code> <p>Unique identifier for state persistence.</p> <code>'main'</code> <p>Returns:</p> Type Description <code>dict | None</code> <p>dict | None: Complete processed results or None if execution fails.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If executor is not initialized.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/executor.py</code> <pre><code>@validate_call\n@require(lambda self: self.graph is not None, \"Graph must not be None\")\n@require(lambda review_text: len(review_text.strip()) &gt; 0, \"Review text must not be empty\")\n@require(lambda thread_id: len(thread_id.strip()) &gt; 0, \"Thread ID must not be empty\")\nasync def run_workflow(\n    self,\n    review_text: Annotated[str, Field(min_length=1)],\n    thread_id: Annotated[str, Field(min_length=1)] = \"main\",\n) -&gt; dict | None:\n    \"\"\"Execute the LangGraph workflow for processing self-review text.\n\n    This is the main execution method that runs the complete self-review\n    processing pipeline.\n\n    Args:\n        review_text: The self-review text to process.\n        thread_id: Unique identifier for state persistence.\n\n    Returns:\n        dict | None: Complete processed results or None if execution fails.\n\n    Raises:\n        RuntimeError: If executor is not initialized.\n    \"\"\"\n    if not self._initialized:\n        raise RuntimeError(\"WorkflowExecutor not initialized. Call initialize() first.\")\n\n    logger.info(\"Running workflow (thread_id={}, text_length={})\", thread_id, len(review_text))\n\n    try:\n        result = await self.graph.ainvoke(\n            {\"original_text\": review_text}, config={\"configurable\": {\"thread_id\": thread_id}}\n        )\n\n        self._validate_graph_result(result)\n        logger.info(\"Workflow execution completed successfully (thread_id={})\", thread_id)\n        return result\n\n    except Exception as e:\n        logger.error(\"Error running workflow (thread_id={}): {}\", thread_id, e)\n        return None\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowExecutor/#metamorphosis.agents.self_reviewer.executor.WorkflowExecutor.test_workflow","title":"<code>test_workflow()</code>  <code>async</code>","text":"<p>Test function to validate the workflow with sample data.</p> <p>This method loads a sample review from the data_engineer_review.md file and processes it through the complete workflow to validate functionality.</p> <p>Returns:</p> Type Description <code>dict | None</code> <p>dict | None: Complete processed results from the workflow or None if failed.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If executor is not initialized.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/executor.py</code> <pre><code>@validate_call\nasync def test_workflow(self) -&gt; dict | None:\n    \"\"\"Test function to validate the workflow with sample data.\n\n    This method loads a sample review from the data_engineer_review.md file\n    and processes it through the complete workflow to validate functionality.\n\n    Returns:\n        dict | None: Complete processed results from the workflow or None if failed.\n\n    Raises:\n        RuntimeError: If executor is not initialized.\n    \"\"\"\n    if not self._initialized:\n        raise RuntimeError(\"WorkflowExecutor not initialized. Call initialize() first.\")\n\n    logger.info(\"Running workflow test with sample data\")\n\n    project_root = get_project_root()\n    review_path = project_root / \"sample_reviews\" / \"data_engineer_review.md\"\n    sample_text = read_text_file(review_path)\n\n    return await self.run_workflow(sample_text, thread_id=\"test\")\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowExecutor/#what-it-does","title":"What it does","text":"<p>The <code>WorkflowExecutor</code> is the friendly entry point. It initializes the MCP client, builds the LangGraph, and runs the workflow for a given piece of text and <code>thread_id</code>.</p>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowExecutor/#typical-use","title":"Typical use","text":"<pre><code>import asyncio\nfrom metamorphosis.agents.self_reviewer import WorkflowExecutor\n\nasync def main():\n    ex = WorkflowExecutor()\n    await ex.initialize()\n    result = await ex.run_workflow(\"Your self\u2011review text here\", thread_id=\"demo-1\")\n    print(result)\n\nasyncio.run(main())\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowExecutor/#small-class-diagram","title":"Small class diagram","text":"%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#e0f2ff', 'edgeLabelBackground':'#ffffffaa', 'clusterBkg':'#f7fbff', 'fontFamily':'Inter'}}}%% classDiagram     class WorkflowExecutor {         +initialize() async         +run_workflow(review_text, thread_id) async dict|None         +list_available_tools() list~str~         +get_graph_visualization() str         +close() async         +is_initialized: bool     }     WorkflowExecutor --&gt; MCPClientManager     WorkflowExecutor --&gt; GraphBuilder     WorkflowExecutor --&gt; WorkflowNodes"},{"location":"metamorphosis/agents/self_reviewer/WorkflowNodes/","title":"WorkflowNodes","text":""},{"location":"metamorphosis/agents/self_reviewer/WorkflowNodes/#workflownodes","title":"WorkflowNodes","text":"<p>Contains all workflow node implementations.</p> <p>This class encapsulates all the node functions used in the self-reviewer workflow, providing a clean interface for graph construction and execution.</p> <p>Attributes:</p> Name Type Description <code>mcp_client</code> <p>The MCP client manager for accessing tools.</p> <code>llm</code> <p>The base language model for agent operations.</p> <code>achievements_extractor_llm</code> <p>LLM bound with achievements extraction tools.</p> <code>review_text_evaluator_llm</code> <p>LLM bound with review evaluation tools.</p> <code>achievements_extractor_tool_node</code> <p>Tool node for achievements extraction.</p> <code>review_text_evaluator_tool_node</code> <p>Tool node for review evaluation.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/nodes.py</code> <pre><code>class WorkflowNodes:\n    \"\"\"Contains all workflow node implementations.\n\n    This class encapsulates all the node functions used in the self-reviewer\n    workflow, providing a clean interface for graph construction and execution.\n\n    Attributes:\n        mcp_client: The MCP client manager for accessing tools.\n        llm: The base language model for agent operations.\n        achievements_extractor_llm: LLM bound with achievements extraction tools.\n        review_text_evaluator_llm: LLM bound with review evaluation tools.\n        achievements_extractor_tool_node: Tool node for achievements extraction.\n        review_text_evaluator_tool_node: Tool node for review evaluation.\n    \"\"\"\n\n    def __init__(self, mcp_client: MCPClientManager) -&gt; None:\n        \"\"\"Initialize the workflow nodes.\n\n        Args:\n            mcp_client: Initialized MCP client manager.\n        \"\"\"\n        self.mcp_client = mcp_client\n\n        # Initialize LLM and tool configurations\n        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n        self.achievements_extractor_llm = self.llm.bind_tools([extract_achievements])\n        self.review_text_evaluator_llm = self.llm.bind_tools([evaluate_review_text])\n        self.achievements_extractor_tool_node = ToolNode([extract_achievements])\n        self.review_text_evaluator_tool_node = ToolNode([evaluate_review_text])\n\n        # Load prompt templates\n        project_root = get_project_root()\n        prompts_dir = project_root / \"prompts\"\n        self.achievements_extraction_system_prompt = read_text_file(\n            prompts_dir / \"achievements_extraction_system_prompt.md\"\n        )\n        self.evaluation_score_system_prompt = read_text_file(\n            prompts_dir / \"evaluation_score_system_prompt.md\"\n        )\n\n    # -----------------------------------------------------------------------------\n\n    @validate_call\n    @require(lambda state: \"original_text\" in state, \"State must contain original_text\")\n    @require(lambda state: isinstance(state[\"original_text\"], str), \"original_text must be string\")\n    @require(\n        lambda state: len(state[\"original_text\"].strip()) &gt; 0, \"original_text must not be empty\"\n    )\n    @ensure(lambda result: \"copy_edited_text\" in result, \"Result must contain copy_edited_text\")\n    @ensure(\n        lambda result: isinstance(result[\"copy_edited_text\"], str),\n        \"copy_edited_text must be string\",\n    )\n    async def copy_editor_node(\n        self, state: Annotated[dict, Field(description=\"Current workflow state\")]\n    ) -&gt; dict:\n        \"\"\"Copy editor node for grammar and clarity improvements.\n\n        This node processes the original self-review text to improve grammar,\n        spelling, punctuation, and overall clarity.\n\n        Args:\n            state: Current workflow state containing original_text.\n\n        Returns:\n            dict: Updated state with copy_edited_text field populated.\n\n        Raises:\n            ValueError: If copy_edit tool is not available or postcondition fails.\n            json.JSONDecodeError: If tool response is not valid JSON.\n        \"\"\"\n        original_text = state[\"original_text\"]\n        logger.info(\"copy_editor_node: processing text (length={})\", len(original_text))\n\n        copy_edit_tool = self.mcp_client.get_tool(\"copy_edit\")\n        result = await copy_edit_tool.ainvoke({\"text\": original_text})\n\n        result_data = json.loads(result)\n        copy_edited_text = result_data[\"copy_edited_text\"]\n\n        return {\"copy_edited_text\": copy_edited_text}\n\n    # -----------------------------------------------------------------------------\n\n    @validate_call\n    @require(lambda state: \"copy_edited_text\" in state, \"State must contain copy_edited_text\")\n    @require(\n        lambda state: isinstance(state[\"copy_edited_text\"], str), \"copy_edited_text must be string\"\n    )\n    @require(\n        lambda state: len(state[\"copy_edited_text\"].strip()) &gt; 0,\n        \"copy_edited_text must not be empty\",\n    )\n    @ensure(lambda result: \"summary\" in result, \"Result must contain summary\")\n    @ensure(lambda result: isinstance(result[\"summary\"], str), \"summary must be string\")\n    async def summarizer_node(\n        self, state: Annotated[dict, Field(description=\"Current workflow state\")]\n    ) -&gt; dict:\n        \"\"\"Summarizer node for abstractive text summarization.\n\n        This node creates a concise, abstractive summary of the copy-edited text,\n        extracting key insights and main points.\n\n        Args:\n            state: Current workflow state containing copy_edited_text.\n\n        Returns:\n            dict: Updated state with summary field populated.\n\n        Raises:\n            ValueError: If abstractive_summarize tool is not available or postcondition fails.\n            json.JSONDecodeError: If tool response is not valid JSON.\n        \"\"\"\n        copy_edited_text = state[\"copy_edited_text\"]\n        logger.info(\"summarizer_node: processing text (length={})\", len(copy_edited_text))\n\n        summarizer_tool = self.mcp_client.get_tool(\"abstractive_summarize\")\n        result = await summarizer_tool.ainvoke({\"text\": copy_edited_text})\n\n        result_data = json.loads(result)\n        summary = result_data[\"summarized_text\"]\n\n        return {\"summary\": summary}\n\n    # -----------------------------------------------------------------------------\n\n    @validate_call\n    @require(lambda state: \"copy_edited_text\" in state, \"State must contain copy_edited_text\")\n    @require(\n        lambda state: isinstance(state[\"copy_edited_text\"], str), \"copy_edited_text must be string\"\n    )\n    @require(\n        lambda state: len(state[\"copy_edited_text\"].strip()) &gt; 0,\n        \"copy_edited_text must not be empty\",\n    )\n    @ensure(lambda result: \"word_cloud_path\" in result, \"Result must contain word_cloud_path\")\n    @ensure(\n        lambda result: isinstance(result[\"word_cloud_path\"], str), \"word_cloud_path must be string\"\n    )\n    async def wordcloud_node(\n        self, state: Annotated[dict, Field(description=\"Current workflow state\")]\n    ) -&gt; dict:\n        \"\"\"Word cloud generation node for visual text analysis.\n\n        This node generates a visual word cloud from the copy-edited text, creating\n        a graphical representation where word frequency is shown through font size.\n\n        Args:\n            state: Current workflow state containing copy_edited_text.\n\n        Returns:\n            dict: Updated state with word_cloud_path field populated.\n\n        Raises:\n            ValueError: If word_cloud tool is not available or postcondition fails.\n        \"\"\"\n        copy_edited_text = state[\"copy_edited_text\"]\n        logger.info(\"wordcloud_node: processing text (length={})\", len(copy_edited_text))\n\n        wordcloud_tool = self.mcp_client.get_tool(\"word_cloud\")\n        result = await wordcloud_tool.ainvoke({\"text\": copy_edited_text})\n\n        # The word_cloud tool returns a string path directly, not JSON\n        wordcloud_path = result\n\n        return {\"word_cloud_path\": wordcloud_path}\n\n    # -----------------------------------------------------------------------------\n\n    @validate_call\n    @require(lambda state: \"copy_edited_text\" in state, \"State must contain copy_edited_text\")\n    @require(\n        lambda state: isinstance(state[\"copy_edited_text\"], str), \"copy_edited_text must be string\"\n    )\n    @require(\n        lambda state: len(state[\"copy_edited_text\"].strip()) &gt; 0,\n        \"copy_edited_text must not be empty\",\n    )\n    @ensure(lambda result: \"messages\" in result, \"Result must contain messages\")\n    @ensure(lambda result: isinstance(result[\"messages\"], list), \"messages must be list\")\n    async def achievements_extractor_node(\n        self, state: Annotated[dict, Field(description=\"Current workflow state\")]\n    ) -&gt; dict:\n        \"\"\"Achievements extractor node for extracting key achievements from the text.\n\n        This asynchronous node is responsible for orchestrating the extraction of key achievements\n        from the employee's copy-edited self-review text. It leverages a LangGraph agent, which\n        utilizes the `extract_achievements` tool, and is guided by a system prompt designed to\n        ensure accurate and relevant extraction. The node constructs a prompt with the current\n        conversation history and the input text, invokes the agent, and returns the updated\n        message list including the agent's response.\n\n        Args:\n            state: The current workflow state. Must contain:\n                - \"copy_edited_text\" (str): The self-review text to analyze.\n                - \"messages\" (list, optional): Conversation history for context.\n\n        Returns:\n            dict: A dictionary with a single key \"messages\", containing the updated list of\n                messages (including the agent's response with extracted achievements).\n\n        Raises:\n            KeyError: If \"copy_edited_text\" is missing from the state.\n            Exception: If the agent invocation fails or returns an unexpected result.\n        \"\"\"\n        copy_edited_text = state[\"copy_edited_text\"]\n        logger.info(\n            \"achievements_extractor_node: processing text (length={})\", len(copy_edited_text)\n        )\n\n        messages = state.get(\"messages\", [])\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", self.achievements_extraction_system_prompt),\n                MessagesPlaceholder(\"messages\"),\n                (\n                    \"human\",\n                    \"Extract key achievements from this text:\\\\n\\\\n{input_text}\\\\n\\\\nIf needed, call the tool.\",\n                ),\n            ]\n        )\n        rendered = prompt.invoke({\"messages\": messages, \"input_text\": copy_edited_text})\n        ai_response = await self.achievements_extractor_llm.ainvoke(rendered)\n\n        return {\"messages\": [ai_response]}\n\n    # -----------------------------------------------------------------------------\n\n    @validate_call\n    def _validate_messages(self, msgs: list) -&gt; None:\n        \"\"\"Validate that messages exist and contain tool messages.\n\n        Args:\n            msgs: List of messages to validate.\n\n        Raises:\n            ValueError: If validation fails.\n        \"\"\"\n        if not msgs:\n            raise_postcondition_error(\n                \"Post-tools node: no messages found\",\n                context={\"messages_count\": len(msgs) if msgs else 0},\n                operation=\"post_tools_validation\",\n            )\n\n    # -----------------------------------------------------------------------------\n\n    @validate_call\n    def _extract_tool_payload(self, tool_msg: ToolMessage) -&gt; dict:\n        \"\"\"Extract payload from tool message content.\n\n        Args:\n            tool_msg: The tool message to extract payload from.\n\n        Returns:\n            dict: The extracted payload.\n        \"\"\"\n        # Handle both dict and string content formats\n        if isinstance(tool_msg.content, (dict, list)):\n            return tool_msg.content\n\n        # Try to parse JSON if it's a string\n        try:\n            return json.loads(tool_msg.content)\n        except Exception:\n            return {\"raw\": tool_msg.content}\n\n    # -----------------------------------------------------------------------------\n\n    @validate_call\n    def _find_latest_tool_message(self, msgs: list) -&gt; ToolMessage:\n        \"\"\"Find the most recent ToolMessage from the messages.\n\n        Args:\n            msgs: List of messages to search.\n\n        Returns:\n            ToolMessage: The latest tool message.\n\n        Raises:\n            ValueError: If no tool messages found.\n        \"\"\"\n        tool_msgs = [m for m in reversed(msgs) if isinstance(m, ToolMessage)]\n        if not tool_msgs:\n            raise_postcondition_error(\n                \"Post-tools node: no tool messages found\",\n                context={\"tool_msgs_count\": len(tool_msgs) if tool_msgs else 0},\n                operation=\"post_tools_validation\",\n            )\n        return tool_msgs[0]\n\n    # -----------------------------------------------------------------------------\n\n    @require(lambda state: \"messages\" in state, \"State must contain messages\")\n    @require(lambda state: isinstance(state[\"messages\"], list), \"messages must be list\")\n    @ensure(lambda result: \"achievements\" in result, \"Result must contain achievements\")\n    @ensure(lambda result: \"review_complete\" in result, \"Result must contain review_complete\")\n    async def after_achievements_parser(self, state: GraphState) -&gt; dict:\n        \"\"\"Achievements extractor post-tools node for extracting key achievements from the text.\n\n        Called after ToolNode executes. It reads the latest ToolMessage,\n        extracts the tool result (AchievementsList), and writes it into the state.\n\n        Args:\n            state: Current workflow state containing messages.\n\n        Returns:\n            dict: Updated state with achievements and review_complete flag.\n        \"\"\"\n        logger.info(\"after_achievements_parser: parsing achievements\")\n        msgs = state.get(\"messages\", [])\n\n        self._validate_messages(msgs)\n        last_tool_msg = self._find_latest_tool_message(msgs)\n        payload = self._extract_tool_payload(last_tool_msg)\n\n        # We expect payload to match AchievementsList\n        achievements_obj = payload if isinstance(payload, dict) else {\"result\": payload}\n        achievements = AchievementsList(**achievements_obj)\n\n        summary = AIMessage(\n            content=f\"Received {len(achievements.items)} achievements from tool.\\\\n\"\n        )\n\n        review_complete = len(achievements.items) &gt;= 3\n        return {\n            \"messages\": [summary],\n            \"achievements\": achievements,\n            \"review_complete\": review_complete,\n        }\n\n    # -----------------------------------------------------------------------------\n\n    @validate_call\n    @require(lambda state: \"copy_edited_text\" in state, \"State must contain copy_edited_text\")\n    @require(\n        lambda state: isinstance(state[\"copy_edited_text\"], str), \"copy_edited_text must be string\"\n    )\n    @require(\n        lambda state: len(state[\"copy_edited_text\"].strip()) &gt; 0,\n        \"copy_edited_text must not be empty\",\n    )\n    @ensure(lambda result: \"messages\" in result, \"Result must contain messages\")\n    @ensure(lambda result: isinstance(result[\"messages\"], list), \"messages must be list\")\n    async def review_text_evaluator_node(\n        self, state: Annotated[dict, Field(description=\"Current workflow state\")]\n    ) -&gt; dict:\n        \"\"\"Review text evaluator node for evaluating the copy-edited text.\n\n        This node evaluates the review text using an agent accessing a review scorecard generation langgraph tool.\n\n        Args:\n            state: Current workflow state containing copy_edited_text.\n\n        Returns:\n            dict: Updated state with messages containing evaluation response.\n        \"\"\"\n        copy_edited_text = state[\"copy_edited_text\"]\n        logger.info(\n            \"review_text_evaluator_node: processing text (length={})\", len(copy_edited_text)\n        )\n\n        messages = state.get(\"messages\", [])\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", self.evaluation_score_system_prompt),\n                MessagesPlaceholder(\"messages\"),\n                (\n                    \"human\",\n                    \"Evaluate the review text and generate a review scorecard:\\\\n\\\\n{input_text}\\\\n\\\\nIf needed, call the tool.\",\n                ),\n            ]\n        )\n        rendered = prompt.invoke({\"messages\": messages, \"input_text\": copy_edited_text})\n        ai_response = await self.review_text_evaluator_llm.ainvoke(rendered)\n\n        return {\"messages\": [ai_response]}\n\n    # -----------------------------------------------------------------------------\n\n    @require(lambda state: \"messages\" in state, \"State must contain messages\")\n    @require(lambda state: isinstance(state[\"messages\"], list), \"messages must be list\")\n    @ensure(lambda result: \"review_scorecard\" in result, \"Result must contain review_scorecard\")\n    async def after_evaluation_parser(self, state: GraphState) -&gt; dict:\n        \"\"\"Review text evaluator post-tools node for evaluating the review text.\n\n        Called after ToolNode executes. It reads the latest ToolMessage,\n        extracts the tool result (ReviewScorecard), and writes it into the state.\n\n        Args:\n            state: Current workflow state containing messages.\n\n        Returns:\n            dict: Updated state with review_scorecard.\n        \"\"\"\n        logger.info(\"after_evaluation_parser: parsing review scorecard\")\n        msgs = state.get(\"messages\", [])\n\n        self._validate_messages(msgs)\n        last_tool_msg = self._find_latest_tool_message(msgs)\n        payload = self._extract_tool_payload(last_tool_msg)\n\n        # We expect payload to match ReviewScorecard\n        review_scorecard_obj = payload if isinstance(payload, dict) else {\"result\": payload}\n        review_scorecard = ReviewScorecard(**review_scorecard_obj)\n\n        summary = AIMessage(\n            content=f\"Received {review_scorecard_obj.get('overall')} review scorecard from tool.\\\\n\"\n        )\n\n        return {\n            \"messages\": [summary],\n            \"review_scorecard\": review_scorecard,\n        }\n\n    # -----------------------------------------------------------------------------\n\n    @require(lambda state: isinstance(state, dict), \"State must be a dictionary\")\n    async def should_call_achievements_extractor_tools(\n        self, state: GraphState\n    ) -&gt; Literal[\"no_tools\", \"tools\"]:\n        \"\"\"If the last AI message contains tool calls, go to tools; else no tools.\n\n        Args:\n            state: Current workflow state containing messages.\n\n        Returns:\n            Literal: \"tools\" if tool calls found, \"no_tools\" otherwise.\n        \"\"\"\n        msgs = state.get(\"messages\", [])\n        if not msgs:\n            return \"no_tools\"\n\n        last = msgs[-1]\n        if isinstance(last, AIMessage) and getattr(last, \"tool_calls\", None):\n            logger.info(\"should_call_achievements_extractor_tools: calling tools\")\n            return \"tools\"\n\n        logger.info(\"should_call_achievements_extractor_tools: no tools called\")\n        return \"no_tools\"\n\n    # -----------------------------------------------------------------------------\n\n    @require(lambda state: isinstance(state, dict), \"State must be a dictionary\")\n    async def should_call_review_text_evaluator_tools(\n        self, state: GraphState\n    ) -&gt; Literal[\"no_tools\", \"tools\"]:\n        \"\"\"If the last AI message contains tool calls, go to tools; else no tools.\n\n        Args:\n            state: Current workflow state containing messages.\n\n        Returns:\n            Literal: \"tools\" if tool calls found, \"no_tools\" otherwise.\n        \"\"\"\n        msgs = state.get(\"messages\", [])\n        if not msgs:\n            return \"no_tools\"\n\n        last = msgs[-1]\n        if isinstance(last, AIMessage) and getattr(last, \"tool_calls\", None):\n            logger.info(\"should_call_review_text_evaluator_tools: calling tools\")\n            return \"tools\"\n\n        logger.info(\"should_call_review_text_evaluator_tools: no tools called\")\n        return \"no_tools\"\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowNodes/#metamorphosis.agents.self_reviewer.nodes.WorkflowNodes.__init__","title":"<code>__init__(mcp_client)</code>","text":"<p>Initialize the workflow nodes.</p> <p>Parameters:</p> Name Type Description Default <code>mcp_client</code> <code>MCPClientManager</code> <p>Initialized MCP client manager.</p> required Source code in <code>src/metamorphosis/agents/self_reviewer/nodes.py</code> <pre><code>def __init__(self, mcp_client: MCPClientManager) -&gt; None:\n    \"\"\"Initialize the workflow nodes.\n\n    Args:\n        mcp_client: Initialized MCP client manager.\n    \"\"\"\n    self.mcp_client = mcp_client\n\n    # Initialize LLM and tool configurations\n    self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n    self.achievements_extractor_llm = self.llm.bind_tools([extract_achievements])\n    self.review_text_evaluator_llm = self.llm.bind_tools([evaluate_review_text])\n    self.achievements_extractor_tool_node = ToolNode([extract_achievements])\n    self.review_text_evaluator_tool_node = ToolNode([evaluate_review_text])\n\n    # Load prompt templates\n    project_root = get_project_root()\n    prompts_dir = project_root / \"prompts\"\n    self.achievements_extraction_system_prompt = read_text_file(\n        prompts_dir / \"achievements_extraction_system_prompt.md\"\n    )\n    self.evaluation_score_system_prompt = read_text_file(\n        prompts_dir / \"evaluation_score_system_prompt.md\"\n    )\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowNodes/#metamorphosis.agents.self_reviewer.nodes.WorkflowNodes.achievements_extractor_node","title":"<code>achievements_extractor_node(state)</code>  <code>async</code>","text":"<p>Achievements extractor node for extracting key achievements from the text.</p> <p>This asynchronous node is responsible for orchestrating the extraction of key achievements from the employee's copy-edited self-review text. It leverages a LangGraph agent, which utilizes the <code>extract_achievements</code> tool, and is guided by a system prompt designed to ensure accurate and relevant extraction. The node constructs a prompt with the current conversation history and the input text, invokes the agent, and returns the updated message list including the agent's response.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Annotated[dict, Field(description='Current workflow state')]</code> <p>The current workflow state. Must contain: - \"copy_edited_text\" (str): The self-review text to analyze. - \"messages\" (list, optional): Conversation history for context.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with a single key \"messages\", containing the updated list of messages (including the agent's response with extracted achievements).</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If \"copy_edited_text\" is missing from the state.</p> <code>Exception</code> <p>If the agent invocation fails or returns an unexpected result.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/nodes.py</code> <pre><code>@validate_call\n@require(lambda state: \"copy_edited_text\" in state, \"State must contain copy_edited_text\")\n@require(\n    lambda state: isinstance(state[\"copy_edited_text\"], str), \"copy_edited_text must be string\"\n)\n@require(\n    lambda state: len(state[\"copy_edited_text\"].strip()) &gt; 0,\n    \"copy_edited_text must not be empty\",\n)\n@ensure(lambda result: \"messages\" in result, \"Result must contain messages\")\n@ensure(lambda result: isinstance(result[\"messages\"], list), \"messages must be list\")\nasync def achievements_extractor_node(\n    self, state: Annotated[dict, Field(description=\"Current workflow state\")]\n) -&gt; dict:\n    \"\"\"Achievements extractor node for extracting key achievements from the text.\n\n    This asynchronous node is responsible for orchestrating the extraction of key achievements\n    from the employee's copy-edited self-review text. It leverages a LangGraph agent, which\n    utilizes the `extract_achievements` tool, and is guided by a system prompt designed to\n    ensure accurate and relevant extraction. The node constructs a prompt with the current\n    conversation history and the input text, invokes the agent, and returns the updated\n    message list including the agent's response.\n\n    Args:\n        state: The current workflow state. Must contain:\n            - \"copy_edited_text\" (str): The self-review text to analyze.\n            - \"messages\" (list, optional): Conversation history for context.\n\n    Returns:\n        dict: A dictionary with a single key \"messages\", containing the updated list of\n            messages (including the agent's response with extracted achievements).\n\n    Raises:\n        KeyError: If \"copy_edited_text\" is missing from the state.\n        Exception: If the agent invocation fails or returns an unexpected result.\n    \"\"\"\n    copy_edited_text = state[\"copy_edited_text\"]\n    logger.info(\n        \"achievements_extractor_node: processing text (length={})\", len(copy_edited_text)\n    )\n\n    messages = state.get(\"messages\", [])\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", self.achievements_extraction_system_prompt),\n            MessagesPlaceholder(\"messages\"),\n            (\n                \"human\",\n                \"Extract key achievements from this text:\\\\n\\\\n{input_text}\\\\n\\\\nIf needed, call the tool.\",\n            ),\n        ]\n    )\n    rendered = prompt.invoke({\"messages\": messages, \"input_text\": copy_edited_text})\n    ai_response = await self.achievements_extractor_llm.ainvoke(rendered)\n\n    return {\"messages\": [ai_response]}\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowNodes/#metamorphosis.agents.self_reviewer.nodes.WorkflowNodes.after_achievements_parser","title":"<code>after_achievements_parser(state)</code>  <code>async</code>","text":"<p>Achievements extractor post-tools node for extracting key achievements from the text.</p> <p>Called after ToolNode executes. It reads the latest ToolMessage, extracts the tool result (AchievementsList), and writes it into the state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>GraphState</code> <p>Current workflow state containing messages.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated state with achievements and review_complete flag.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/nodes.py</code> <pre><code>@require(lambda state: \"messages\" in state, \"State must contain messages\")\n@require(lambda state: isinstance(state[\"messages\"], list), \"messages must be list\")\n@ensure(lambda result: \"achievements\" in result, \"Result must contain achievements\")\n@ensure(lambda result: \"review_complete\" in result, \"Result must contain review_complete\")\nasync def after_achievements_parser(self, state: GraphState) -&gt; dict:\n    \"\"\"Achievements extractor post-tools node for extracting key achievements from the text.\n\n    Called after ToolNode executes. It reads the latest ToolMessage,\n    extracts the tool result (AchievementsList), and writes it into the state.\n\n    Args:\n        state: Current workflow state containing messages.\n\n    Returns:\n        dict: Updated state with achievements and review_complete flag.\n    \"\"\"\n    logger.info(\"after_achievements_parser: parsing achievements\")\n    msgs = state.get(\"messages\", [])\n\n    self._validate_messages(msgs)\n    last_tool_msg = self._find_latest_tool_message(msgs)\n    payload = self._extract_tool_payload(last_tool_msg)\n\n    # We expect payload to match AchievementsList\n    achievements_obj = payload if isinstance(payload, dict) else {\"result\": payload}\n    achievements = AchievementsList(**achievements_obj)\n\n    summary = AIMessage(\n        content=f\"Received {len(achievements.items)} achievements from tool.\\\\n\"\n    )\n\n    review_complete = len(achievements.items) &gt;= 3\n    return {\n        \"messages\": [summary],\n        \"achievements\": achievements,\n        \"review_complete\": review_complete,\n    }\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowNodes/#metamorphosis.agents.self_reviewer.nodes.WorkflowNodes.after_evaluation_parser","title":"<code>after_evaluation_parser(state)</code>  <code>async</code>","text":"<p>Review text evaluator post-tools node for evaluating the review text.</p> <p>Called after ToolNode executes. It reads the latest ToolMessage, extracts the tool result (ReviewScorecard), and writes it into the state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>GraphState</code> <p>Current workflow state containing messages.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated state with review_scorecard.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/nodes.py</code> <pre><code>@require(lambda state: \"messages\" in state, \"State must contain messages\")\n@require(lambda state: isinstance(state[\"messages\"], list), \"messages must be list\")\n@ensure(lambda result: \"review_scorecard\" in result, \"Result must contain review_scorecard\")\nasync def after_evaluation_parser(self, state: GraphState) -&gt; dict:\n    \"\"\"Review text evaluator post-tools node for evaluating the review text.\n\n    Called after ToolNode executes. It reads the latest ToolMessage,\n    extracts the tool result (ReviewScorecard), and writes it into the state.\n\n    Args:\n        state: Current workflow state containing messages.\n\n    Returns:\n        dict: Updated state with review_scorecard.\n    \"\"\"\n    logger.info(\"after_evaluation_parser: parsing review scorecard\")\n    msgs = state.get(\"messages\", [])\n\n    self._validate_messages(msgs)\n    last_tool_msg = self._find_latest_tool_message(msgs)\n    payload = self._extract_tool_payload(last_tool_msg)\n\n    # We expect payload to match ReviewScorecard\n    review_scorecard_obj = payload if isinstance(payload, dict) else {\"result\": payload}\n    review_scorecard = ReviewScorecard(**review_scorecard_obj)\n\n    summary = AIMessage(\n        content=f\"Received {review_scorecard_obj.get('overall')} review scorecard from tool.\\\\n\"\n    )\n\n    return {\n        \"messages\": [summary],\n        \"review_scorecard\": review_scorecard,\n    }\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowNodes/#metamorphosis.agents.self_reviewer.nodes.WorkflowNodes.copy_editor_node","title":"<code>copy_editor_node(state)</code>  <code>async</code>","text":"<p>Copy editor node for grammar and clarity improvements.</p> <p>This node processes the original self-review text to improve grammar, spelling, punctuation, and overall clarity.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Annotated[dict, Field(description='Current workflow state')]</code> <p>Current workflow state containing original_text.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated state with copy_edited_text field populated.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If copy_edit tool is not available or postcondition fails.</p> <code>JSONDecodeError</code> <p>If tool response is not valid JSON.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/nodes.py</code> <pre><code>@validate_call\n@require(lambda state: \"original_text\" in state, \"State must contain original_text\")\n@require(lambda state: isinstance(state[\"original_text\"], str), \"original_text must be string\")\n@require(\n    lambda state: len(state[\"original_text\"].strip()) &gt; 0, \"original_text must not be empty\"\n)\n@ensure(lambda result: \"copy_edited_text\" in result, \"Result must contain copy_edited_text\")\n@ensure(\n    lambda result: isinstance(result[\"copy_edited_text\"], str),\n    \"copy_edited_text must be string\",\n)\nasync def copy_editor_node(\n    self, state: Annotated[dict, Field(description=\"Current workflow state\")]\n) -&gt; dict:\n    \"\"\"Copy editor node for grammar and clarity improvements.\n\n    This node processes the original self-review text to improve grammar,\n    spelling, punctuation, and overall clarity.\n\n    Args:\n        state: Current workflow state containing original_text.\n\n    Returns:\n        dict: Updated state with copy_edited_text field populated.\n\n    Raises:\n        ValueError: If copy_edit tool is not available or postcondition fails.\n        json.JSONDecodeError: If tool response is not valid JSON.\n    \"\"\"\n    original_text = state[\"original_text\"]\n    logger.info(\"copy_editor_node: processing text (length={})\", len(original_text))\n\n    copy_edit_tool = self.mcp_client.get_tool(\"copy_edit\")\n    result = await copy_edit_tool.ainvoke({\"text\": original_text})\n\n    result_data = json.loads(result)\n    copy_edited_text = result_data[\"copy_edited_text\"]\n\n    return {\"copy_edited_text\": copy_edited_text}\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowNodes/#metamorphosis.agents.self_reviewer.nodes.WorkflowNodes.review_text_evaluator_node","title":"<code>review_text_evaluator_node(state)</code>  <code>async</code>","text":"<p>Review text evaluator node for evaluating the copy-edited text.</p> <p>This node evaluates the review text using an agent accessing a review scorecard generation langgraph tool.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Annotated[dict, Field(description='Current workflow state')]</code> <p>Current workflow state containing copy_edited_text.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated state with messages containing evaluation response.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/nodes.py</code> <pre><code>@validate_call\n@require(lambda state: \"copy_edited_text\" in state, \"State must contain copy_edited_text\")\n@require(\n    lambda state: isinstance(state[\"copy_edited_text\"], str), \"copy_edited_text must be string\"\n)\n@require(\n    lambda state: len(state[\"copy_edited_text\"].strip()) &gt; 0,\n    \"copy_edited_text must not be empty\",\n)\n@ensure(lambda result: \"messages\" in result, \"Result must contain messages\")\n@ensure(lambda result: isinstance(result[\"messages\"], list), \"messages must be list\")\nasync def review_text_evaluator_node(\n    self, state: Annotated[dict, Field(description=\"Current workflow state\")]\n) -&gt; dict:\n    \"\"\"Review text evaluator node for evaluating the copy-edited text.\n\n    This node evaluates the review text using an agent accessing a review scorecard generation langgraph tool.\n\n    Args:\n        state: Current workflow state containing copy_edited_text.\n\n    Returns:\n        dict: Updated state with messages containing evaluation response.\n    \"\"\"\n    copy_edited_text = state[\"copy_edited_text\"]\n    logger.info(\n        \"review_text_evaluator_node: processing text (length={})\", len(copy_edited_text)\n    )\n\n    messages = state.get(\"messages\", [])\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", self.evaluation_score_system_prompt),\n            MessagesPlaceholder(\"messages\"),\n            (\n                \"human\",\n                \"Evaluate the review text and generate a review scorecard:\\\\n\\\\n{input_text}\\\\n\\\\nIf needed, call the tool.\",\n            ),\n        ]\n    )\n    rendered = prompt.invoke({\"messages\": messages, \"input_text\": copy_edited_text})\n    ai_response = await self.review_text_evaluator_llm.ainvoke(rendered)\n\n    return {\"messages\": [ai_response]}\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowNodes/#metamorphosis.agents.self_reviewer.nodes.WorkflowNodes.should_call_achievements_extractor_tools","title":"<code>should_call_achievements_extractor_tools(state)</code>  <code>async</code>","text":"<p>If the last AI message contains tool calls, go to tools; else no tools.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>GraphState</code> <p>Current workflow state containing messages.</p> required <p>Returns:</p> Name Type Description <code>Literal</code> <code>Literal['no_tools', 'tools']</code> <p>\"tools\" if tool calls found, \"no_tools\" otherwise.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/nodes.py</code> <pre><code>@require(lambda state: isinstance(state, dict), \"State must be a dictionary\")\nasync def should_call_achievements_extractor_tools(\n    self, state: GraphState\n) -&gt; Literal[\"no_tools\", \"tools\"]:\n    \"\"\"If the last AI message contains tool calls, go to tools; else no tools.\n\n    Args:\n        state: Current workflow state containing messages.\n\n    Returns:\n        Literal: \"tools\" if tool calls found, \"no_tools\" otherwise.\n    \"\"\"\n    msgs = state.get(\"messages\", [])\n    if not msgs:\n        return \"no_tools\"\n\n    last = msgs[-1]\n    if isinstance(last, AIMessage) and getattr(last, \"tool_calls\", None):\n        logger.info(\"should_call_achievements_extractor_tools: calling tools\")\n        return \"tools\"\n\n    logger.info(\"should_call_achievements_extractor_tools: no tools called\")\n    return \"no_tools\"\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowNodes/#metamorphosis.agents.self_reviewer.nodes.WorkflowNodes.should_call_review_text_evaluator_tools","title":"<code>should_call_review_text_evaluator_tools(state)</code>  <code>async</code>","text":"<p>If the last AI message contains tool calls, go to tools; else no tools.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>GraphState</code> <p>Current workflow state containing messages.</p> required <p>Returns:</p> Name Type Description <code>Literal</code> <code>Literal['no_tools', 'tools']</code> <p>\"tools\" if tool calls found, \"no_tools\" otherwise.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/nodes.py</code> <pre><code>@require(lambda state: isinstance(state, dict), \"State must be a dictionary\")\nasync def should_call_review_text_evaluator_tools(\n    self, state: GraphState\n) -&gt; Literal[\"no_tools\", \"tools\"]:\n    \"\"\"If the last AI message contains tool calls, go to tools; else no tools.\n\n    Args:\n        state: Current workflow state containing messages.\n\n    Returns:\n        Literal: \"tools\" if tool calls found, \"no_tools\" otherwise.\n    \"\"\"\n    msgs = state.get(\"messages\", [])\n    if not msgs:\n        return \"no_tools\"\n\n    last = msgs[-1]\n    if isinstance(last, AIMessage) and getattr(last, \"tool_calls\", None):\n        logger.info(\"should_call_review_text_evaluator_tools: calling tools\")\n        return \"tools\"\n\n    logger.info(\"should_call_review_text_evaluator_tools: no tools called\")\n    return \"no_tools\"\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowNodes/#metamorphosis.agents.self_reviewer.nodes.WorkflowNodes.summarizer_node","title":"<code>summarizer_node(state)</code>  <code>async</code>","text":"<p>Summarizer node for abstractive text summarization.</p> <p>This node creates a concise, abstractive summary of the copy-edited text, extracting key insights and main points.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Annotated[dict, Field(description='Current workflow state')]</code> <p>Current workflow state containing copy_edited_text.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated state with summary field populated.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If abstractive_summarize tool is not available or postcondition fails.</p> <code>JSONDecodeError</code> <p>If tool response is not valid JSON.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/nodes.py</code> <pre><code>@validate_call\n@require(lambda state: \"copy_edited_text\" in state, \"State must contain copy_edited_text\")\n@require(\n    lambda state: isinstance(state[\"copy_edited_text\"], str), \"copy_edited_text must be string\"\n)\n@require(\n    lambda state: len(state[\"copy_edited_text\"].strip()) &gt; 0,\n    \"copy_edited_text must not be empty\",\n)\n@ensure(lambda result: \"summary\" in result, \"Result must contain summary\")\n@ensure(lambda result: isinstance(result[\"summary\"], str), \"summary must be string\")\nasync def summarizer_node(\n    self, state: Annotated[dict, Field(description=\"Current workflow state\")]\n) -&gt; dict:\n    \"\"\"Summarizer node for abstractive text summarization.\n\n    This node creates a concise, abstractive summary of the copy-edited text,\n    extracting key insights and main points.\n\n    Args:\n        state: Current workflow state containing copy_edited_text.\n\n    Returns:\n        dict: Updated state with summary field populated.\n\n    Raises:\n        ValueError: If abstractive_summarize tool is not available or postcondition fails.\n        json.JSONDecodeError: If tool response is not valid JSON.\n    \"\"\"\n    copy_edited_text = state[\"copy_edited_text\"]\n    logger.info(\"summarizer_node: processing text (length={})\", len(copy_edited_text))\n\n    summarizer_tool = self.mcp_client.get_tool(\"abstractive_summarize\")\n    result = await summarizer_tool.ainvoke({\"text\": copy_edited_text})\n\n    result_data = json.loads(result)\n    summary = result_data[\"summarized_text\"]\n\n    return {\"summary\": summary}\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowNodes/#metamorphosis.agents.self_reviewer.nodes.WorkflowNodes.wordcloud_node","title":"<code>wordcloud_node(state)</code>  <code>async</code>","text":"<p>Word cloud generation node for visual text analysis.</p> <p>This node generates a visual word cloud from the copy-edited text, creating a graphical representation where word frequency is shown through font size.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Annotated[dict, Field(description='Current workflow state')]</code> <p>Current workflow state containing copy_edited_text.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated state with word_cloud_path field populated.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If word_cloud tool is not available or postcondition fails.</p> Source code in <code>src/metamorphosis/agents/self_reviewer/nodes.py</code> <pre><code>@validate_call\n@require(lambda state: \"copy_edited_text\" in state, \"State must contain copy_edited_text\")\n@require(\n    lambda state: isinstance(state[\"copy_edited_text\"], str), \"copy_edited_text must be string\"\n)\n@require(\n    lambda state: len(state[\"copy_edited_text\"].strip()) &gt; 0,\n    \"copy_edited_text must not be empty\",\n)\n@ensure(lambda result: \"word_cloud_path\" in result, \"Result must contain word_cloud_path\")\n@ensure(\n    lambda result: isinstance(result[\"word_cloud_path\"], str), \"word_cloud_path must be string\"\n)\nasync def wordcloud_node(\n    self, state: Annotated[dict, Field(description=\"Current workflow state\")]\n) -&gt; dict:\n    \"\"\"Word cloud generation node for visual text analysis.\n\n    This node generates a visual word cloud from the copy-edited text, creating\n    a graphical representation where word frequency is shown through font size.\n\n    Args:\n        state: Current workflow state containing copy_edited_text.\n\n    Returns:\n        dict: Updated state with word_cloud_path field populated.\n\n    Raises:\n        ValueError: If word_cloud tool is not available or postcondition fails.\n    \"\"\"\n    copy_edited_text = state[\"copy_edited_text\"]\n    logger.info(\"wordcloud_node: processing text (length={})\", len(copy_edited_text))\n\n    wordcloud_tool = self.mcp_client.get_tool(\"word_cloud\")\n    result = await wordcloud_tool.ainvoke({\"text\": copy_edited_text})\n\n    # The word_cloud tool returns a string path directly, not JSON\n    wordcloud_path = result\n\n    return {\"word_cloud_path\": wordcloud_path}\n</code></pre>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowNodes/#what-each-node-does","title":"What each node does","text":"<ul> <li>copy_editor_node: calls the <code>copy_edit</code> MCP tool to tidy grammar and clarity.</li> <li>summarizer_node: calls <code>abstractive_summarize</code> to produce a short summary.</li> <li>wordcloud_node: calls <code>word_cloud</code> to generate an image path.</li> <li>achievements_extractor_node: uses an agent with the <code>extract_achievements</code> tool.</li> <li>after_achievements_parser: pulls <code>AchievementsList</code> out of the last tool message.</li> <li>review_text_evaluator_node: uses an agent with <code>evaluate_review_text</code>.</li> <li>after_evaluation_parser: pulls a <code>ReviewScorecard</code> from the last tool message.</li> </ul>"},{"location":"metamorphosis/agents/self_reviewer/WorkflowNodes/#node-sketch","title":"Node sketch","text":"%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#e0f2ff', 'edgeLabelBackground':'#ffffffaa', 'clusterBkg':'#f7fbff', 'fontFamily':'Inter'}}}%% classDiagram     class WorkflowNodes {         +copy_editor_node(state) async dict         +summarizer_node(state) async dict         +wordcloud_node(state) async dict         +achievements_extractor_node(state) async dict         +after_achievements_parser(state) async dict         +review_text_evaluator_node(state) async dict         +after_evaluation_parser(state) async dict         +should_call_achievements_extractor_tools(state) async \"tools|no_tools\"         +should_call_review_text_evaluator_tools(state) async \"tools|no_tools\"     }"},{"location":"metamorphosis/mcp/","title":"MCP (Model Context Protocol) Package","text":"<p>The <code>metamorphosis.mcp</code> package provides Model Context Protocol integration for text processing capabilities. It implements a standardized interface for LLM-backed text utilities with type-safe structured outputs and comprehensive error handling.</p>"},{"location":"metamorphosis/mcp/#package-overview","title":"Package Overview","text":"classDiagram     class TextModifiers {         +summarizer_llm: ChatOpenAI         +copy_editor_llm: ChatOpenAI         +key_achievements_llm: ChatOpenAI         +review_text_evaluator_llm: ChatOpenAI         +__init__() None         +summarize(text: str, max_words: int) SummarizedText         +rationalize_text(text: str) CopyEditedText         +extract_achievements(text: str) AchievementsList         +evaluate_review_text(text: str) ReviewScorecard         +get_model_info(method: str) dict         +_log_model_details_table(method: str) None     }      class ToolsServer {         +copy_edit(text: str) CopyEditedText         +create_word_cloud(text: str) str         +abstractive_summarize(text: str, max_words: int) SummarizedText         +_get_modifiers() TextModifiers     }      class SummarizedText {         +summarized_text: str         +size: int         +unit: str     }      class CopyEditedText {         +copy_edited_text: str         +size: int         +is_edited: bool     }      class AchievementsList {         +items: List[Achievement]         +size: int         +unit: str     }      class ReviewScorecard {         +metrics: List[MetricScore]         +overall: int         +verdict: Verdict         +notes: List[str]         +radar_labels: List[str]         +radar_values: List[int]     }      TextModifiers --&gt; SummarizedText : creates     TextModifiers --&gt; CopyEditedText : creates     TextModifiers --&gt; AchievementsList : creates     TextModifiers --&gt; ReviewScorecard : creates     ToolsServer --&gt; TextModifiers : uses     ToolsServer ..&gt; SummarizedText : returns     ToolsServer ..&gt; CopyEditedText : returns"},{"location":"metamorphosis/mcp/#core-components","title":"Core Components","text":""},{"location":"metamorphosis/mcp/#textmodifiers-class","title":"TextModifiers Class","text":"<p>The <code>TextModifiers</code> class is the heart of the MCP package, providing four main text processing capabilities:</p>"},{"location":"metamorphosis/mcp/#1-text-summarization","title":"1. Text Summarization","text":"<ul> <li>Purpose: Generate abstractive summaries of employee review text</li> <li>Model: Configurable LLM (default: gpt-4o)</li> <li>Features: Configurable word limits, structured output, token estimation</li> </ul>"},{"location":"metamorphosis/mcp/#2-text-rationalization-copy-editing","title":"2. Text Rationalization (Copy Editing)","text":"<ul> <li>Purpose: Improve grammar, spelling, and professional tone</li> <li>Features: Preserves meaning and structure, tracks modifications</li> <li>Guarantees: No content addition/removal, maintains author voice</li> </ul>"},{"location":"metamorphosis/mcp/#3-achievement-extraction","title":"3. Achievement Extraction","text":"<ul> <li>Purpose: Extract and structure key accomplishments from reviews</li> <li>Output: Up to 10 ranked achievements with metadata</li> <li>Metadata: Impact areas, metrics, timeframes, collaborators</li> </ul>"},{"location":"metamorphosis/mcp/#4-review-quality-evaluation","title":"4. Review Quality Evaluation","text":"<ul> <li>Purpose: Assess writing quality across six dimensions</li> <li>Scoring: 0-100 scale with weighted overall score</li> <li>Output: Detailed metrics, suggestions, radar chart data</li> </ul>"},{"location":"metamorphosis/mcp/#mcp-tools-server","title":"MCP Tools Server","text":"<p>The <code>tools_server.py</code> module implements MCP-compliant tool functions that can be called by external agents and workflows:</p> <ul> <li>Standardized Interface: MCP protocol compliance</li> <li>Error Handling: Comprehensive exception management</li> <li>Caching: LRU-cached TextModifiers instances</li> <li>Logging: Detailed operation tracking</li> </ul>"},{"location":"metamorphosis/mcp/#module-documentation","title":"Module Documentation","text":""},{"location":"metamorphosis/mcp/#core-modules","title":"Core Modules","text":"Module Description Key Components <code>text_modifiers.py</code> Main text processing class <code>TextModifiers</code> <code>tools_server.py</code> MCP server implementation Tool functions <code>__init__.py</code> Package initialization Exports"},{"location":"metamorphosis/mcp/#key-classes","title":"Key Classes","text":"Class Purpose Documentation <code>TextModifiers</code> Core text processing utilities View Details \u2192"},{"location":"metamorphosis/mcp/#tool-functions","title":"Tool Functions","text":"Function Purpose Documentation <code>copy_edit</code> MCP tool for text rationalization View Details \u2192 <code>abstractive_summarize</code> MCP tool for summarization View Details \u2192 <code>create_word_cloud</code> MCP tool for visualization View Details \u2192"},{"location":"metamorphosis/mcp/#processing-pipeline","title":"Processing Pipeline","text":""},{"location":"metamorphosis/mcp/#text-processing-flow","title":"Text Processing Flow","text":"sequenceDiagram     participant Client     participant ToolsServer     participant TextModifiers     participant LLM     participant ModelRegistry      Client-&gt;&gt;ToolsServer: copy_edit(text)     ToolsServer-&gt;&gt;ToolsServer: _get_modifiers()     ToolsServer-&gt;&gt;TextModifiers: rationalize_text(text)     TextModifiers-&gt;&gt;ModelRegistry: get copy_editor_llm     ModelRegistry-&gt;&gt;TextModifiers: ChatOpenAI instance     TextModifiers-&gt;&gt;TextModifiers: _log_model_details_table()     TextModifiers-&gt;&gt;LLM: invoke with prompt     LLM-&gt;&gt;TextModifiers: structured response     TextModifiers-&gt;&gt;TextModifiers: validate output     TextModifiers-&gt;&gt;ToolsServer: CopyEditedText     ToolsServer-&gt;&gt;Client: validated result"},{"location":"metamorphosis/mcp/#error-handling-flow","title":"Error Handling Flow","text":"sequenceDiagram     participant Client     participant ToolsServer     participant TextModifiers     participant Exception      Client-&gt;&gt;ToolsServer: tool_function(invalid_input)     ToolsServer-&gt;&gt;TextModifiers: process(invalid_input)     TextModifiers-&gt;&gt;Exception: ValidationError     Exception-&gt;&gt;TextModifiers: error context     TextModifiers-&gt;&gt;ToolsServer: PostconditionError     ToolsServer-&gt;&gt;ToolsServer: log error     ToolsServer-&gt;&gt;Client: structured error response"},{"location":"metamorphosis/mcp/#configuration","title":"Configuration","text":""},{"location":"metamorphosis/mcp/#model-configuration","title":"Model Configuration","text":"<p>Each processing method uses a dedicated LLM configuration:</p> <pre><code>models:\n  summarizer:\n    model: \"gpt-4o\"\n    temperature: 0.0\n    max_tokens: 2000\n    timeout: 120\n\n  copy_editor:\n    model: \"gpt-4o\"\n    temperature: 0.0\n    max_tokens: 4000\n    timeout: 180\n\n  key_achievements:\n    model: \"gpt-4o\"  # Non-reasoning model for structured output\n    temperature: 0.0\n    max_tokens: 4000\n    timeout: 300\n\n  review_text_evaluator:\n    model: \"gpt-5\"  # Reasoning model for complex evaluation\n    temperature: 0.0\n    max_tokens: 20000\n    timeout: 300\n</code></pre>"},{"location":"metamorphosis/mcp/#prompt-templates","title":"Prompt Templates","text":"<p>The system uses external prompt templates for maintainability:</p> <pre><code>prompts/\n\u251c\u2500\u2500 summarizer_system_prompt.md\n\u251c\u2500\u2500 summarizer_user_prompt.md\n\u251c\u2500\u2500 text_rationalization_system_prompt.md\n\u251c\u2500\u2500 text_rationalization_user_prompt.md\n\u251c\u2500\u2500 key_achievements_system_prompt.md\n\u251c\u2500\u2500 key_achievements_user_prompt.md\n\u251c\u2500\u2500 text_evaluator_system_prompt.md\n\u2514\u2500\u2500 text_evaluator_user_prompt.md\n</code></pre>"},{"location":"metamorphosis/mcp/#usage-examples","title":"Usage Examples","text":""},{"location":"metamorphosis/mcp/#basic-text-processing","title":"Basic Text Processing","text":"<pre><code>from metamorphosis.mcp.text_modifiers import TextModifiers\n\n# Initialize the processor\nmodifier = TextModifiers()\n\n# Summarize text\nsummary = modifier.summarize(\n    text=\"Long employee review text...\",\n    max_words=100\n)\nprint(f\"Summary: {summary.summarized_text}\")\nprint(f\"Size: {summary.size} {summary.unit}\")\n\n# Copy edit text\nedited = modifier.rationalize_text(\n    text=\"Text with grammar errors and typos...\"\n)\nprint(f\"Edited: {edited.copy_edited_text}\")\nprint(f\"Was modified: {edited.is_edited}\")\n</code></pre>"},{"location":"metamorphosis/mcp/#achievement-extraction","title":"Achievement Extraction","text":"<pre><code># Extract achievements\nachievements = modifier.extract_achievements(\n    text=\"I led the migration project that reduced latency by 50%...\"\n)\n\nfor achievement in achievements.items:\n    print(f\"Title: {achievement.title}\")\n    print(f\"Impact: {achievement.impact_area}\")\n    print(f\"Metrics: {achievement.metric_strings}\")\n    print(f\"Timeframe: {achievement.timeframe}\")\n    print(\"---\")\n</code></pre>"},{"location":"metamorphosis/mcp/#quality-evaluation","title":"Quality Evaluation","text":"<pre><code># Evaluate review quality\nscorecard = modifier.evaluate_review_text(\n    text=\"Employee review text to evaluate...\"\n)\n\nprint(f\"Overall Score: {scorecard.overall}/100\")\nprint(f\"Verdict: {scorecard.verdict}\")\n\nfor metric in scorecard.metrics:\n    print(f\"{metric.name}: {metric.score}/100\")\n    print(f\"Rationale: {metric.rationale}\")\n    print(f\"Suggestion: {metric.suggestion}\")\n    print(\"---\")\n</code></pre>"},{"location":"metamorphosis/mcp/#mcp-tool-usage","title":"MCP Tool Usage","text":"<pre><code>from metamorphosis.mcp.tools_server import copy_edit, abstractive_summarize\n\n# Use MCP tools directly\nedited_result = copy_edit(\"Text to edit...\")\nsummary_result = abstractive_summarize(\"Text to summarize...\", max_words=50)\n</code></pre>"},{"location":"metamorphosis/mcp/#advanced-features","title":"Advanced Features","text":""},{"location":"metamorphosis/mcp/#model-information-introspection","title":"Model Information Introspection","text":"<pre><code># Get model configuration for debugging\nmodel_info = modifier.get_model_info(\"summarize\")\nprint(f\"Model: {model_info['model']}\")\nprint(f\"Temperature: {model_info['temperature']}\")\nprint(f\"Max Tokens: {model_info['max_tokens']}\")\n</code></pre>"},{"location":"metamorphosis/mcp/#custom-prompt-templates","title":"Custom Prompt Templates","text":"<pre><code># The system automatically loads prompts from the prompts/ directory\n# Modify the .md files to customize behavior\n</code></pre>"},{"location":"metamorphosis/mcp/#logging-and-debugging","title":"Logging and Debugging","text":"<pre><code>import logging\nfrom loguru import logger\n\n# Enable detailed logging\nlogger.add(\"mcp_debug.log\", level=\"DEBUG\")\n\n# The system will log:\n# - Model configurations\n# - Processing steps\n# - Token usage\n# - Error details\n</code></pre>"},{"location":"metamorphosis/mcp/#performance-optimization","title":"Performance Optimization","text":""},{"location":"metamorphosis/mcp/#caching-strategy","title":"Caching Strategy","text":"<ul> <li>LLM Client Caching: Singleton model registry prevents duplicate initialization</li> <li>Prompt Template Caching: Templates loaded once at startup</li> <li>LRU Cache: Tools server uses <code>@lru_cache</code> for TextModifiers instances</li> </ul>"},{"location":"metamorphosis/mcp/#token-management","title":"Token Management","text":"<ul> <li>Efficient Prompting: Optimized prompt templates minimize token usage</li> <li>Structured Output: Direct JSON generation reduces parsing overhead</li> <li>Batch Processing: Process multiple items efficiently</li> </ul>"},{"location":"metamorphosis/mcp/#error-recovery","title":"Error Recovery","text":"<ul> <li>Graceful Degradation: Fallback strategies for API failures</li> <li>Retry Logic: Automatic retry for transient errors</li> <li>Circuit Breaker: Prevent cascade failures</li> </ul>"},{"location":"metamorphosis/mcp/#testing","title":"Testing","text":""},{"location":"metamorphosis/mcp/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom metamorphosis.mcp.text_modifiers import TextModifiers\nfrom metamorphosis.datamodel import SummarizedText\n\ndef test_summarization():\n    modifier = TextModifiers()\n    result = modifier.summarize(\n        text=\"Long text to summarize...\",\n        max_words=50\n    )\n    assert isinstance(result, SummarizedText)\n    assert len(result.summarized_text) &gt; 0\n    assert result.size &gt; 0\n\ndef test_copy_editing():\n    modifier = TextModifiers()\n    result = modifier.rationalize_text(\n        text=\"Text with errers and typos.\"\n    )\n    assert isinstance(result, CopyEditedText)\n    assert \"errors\" in result.copy_edited_text.lower()\n</code></pre>"},{"location":"metamorphosis/mcp/#integration-tests","title":"Integration Tests","text":"<pre><code>def test_mcp_tools_integration():\n    from metamorphosis.mcp.tools_server import copy_edit\n\n    result = copy_edit(\"Test text with errors.\")\n    assert result.copy_edited_text\n    assert isinstance(result.is_edited, bool)\n</code></pre>"},{"location":"metamorphosis/mcp/#troubleshooting","title":"Troubleshooting","text":""},{"location":"metamorphosis/mcp/#common-issues","title":"Common Issues","text":"<ol> <li>LLM API Errors:</li> <li>Check OpenAI API key validity</li> <li>Monitor rate limits and quotas</li> <li> <p>Verify network connectivity</p> </li> <li> <p>Prompt Template Issues:</p> </li> <li>Ensure all prompt files exist in <code>prompts/</code> directory</li> <li>Check file encoding (UTF-8)</li> <li> <p>Validate Jinja2 template syntax</p> </li> <li> <p>Memory Issues:</p> </li> <li>Monitor token usage with large texts</li> <li>Implement text chunking for very long inputs</li> <li>Use streaming for real-time processing</li> </ol>"},{"location":"metamorphosis/mcp/#debug-mode","title":"Debug Mode","text":"<pre><code># Enable comprehensive debugging\nfrom metamorphosis.mcp.text_modifiers import TextModifiers\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\nmodifier = TextModifiers()\n\n# All operations will now log detailed information\n</code></pre>"},{"location":"metamorphosis/mcp/#migration-guide","title":"Migration Guide","text":""},{"location":"metamorphosis/mcp/#from-legacy-versions","title":"From Legacy Versions","text":"<ul> <li>Update import statements to use new module structure</li> <li>Replace manual LLM initialization with ModelRegistry</li> <li>Update exception handling for new hierarchy</li> <li>Migrate to Pydantic v2 models</li> </ul>"},{"location":"metamorphosis/mcp/#see-also","title":"See Also","text":"<ul> <li>Agents Package - LangGraph workflow integration</li> <li>Data Models - Pydantic schema definitions</li> <li>Examples - Usage examples and tutorials</li> <li>Model Registry - LLM client management</li> </ul> <p>This documentation is automatically generated from the source code and maintained in sync with the codebase.</p>"},{"location":"metamorphosis/mcp/TextModifiers/","title":"TextModifiers Class","text":"<p>LLM-backed text processing utilities with structured outputs.</p> <p>This class provides text processing capabilities including summarization and copy editing using OpenAI's language models. All methods return structured Pydantic models for type safety and validation.</p> <p>The class uses prompt templates loaded from external files and leverages LangChain's structured output capabilities for reliable parsing.</p> Source code in <code>src/metamorphosis/mcp/text_modifiers.py</code> <pre><code>class TextModifiers:\n    \"\"\"LLM-backed text processing utilities with structured outputs.\n\n    This class provides text processing capabilities including summarization\n    and copy editing using OpenAI's language models. All methods return\n    structured Pydantic models for type safety and validation.\n\n    The class uses prompt templates loaded from external files and leverages\n    LangChain's structured output capabilities for reliable parsing.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the TextModifiers with LLM chains and prompt templates.\n\n        Raises:\n            ConfigurationError: If required environment variables are missing.\n            FileOperationError: If prompt files cannot be loaded.\n        \"\"\"\n        logger.debug(\"Initializing TextModifiers\")\n\n        # Acquire LLM clients from the central registry\n        registry = get_model_registry()\n        self.summarizer_llm = registry.summarizer_llm\n        self.copy_editor_llm = registry.copy_editor_llm\n        self.key_achievements_llm = registry.key_achievements_llm\n        self.review_text_evaluator_llm = registry.review_text_evaluator_llm\n\n        # Load prompt templates from files using utility functions\n        project_root = get_project_root()\n        prompts_dir = project_root / \"prompts\"\n        logger.debug(\"Using prompts directory: {}\", prompts_dir)\n\n        # Load as-is; no VOICE placeholder expected in the template\n        self.summarizer_system_prompt = read_text_file(prompts_dir / \"summarizer_system_prompt.md\")\n        self.summarizer_user_prompt = read_text_file(prompts_dir / \"summarizer_user_prompt.md\")\n\n        self.key_achievements_system_prompt = read_text_file(\n            prompts_dir / \"key_achievements_system_prompt.md\"\n        )\n        self.key_achievements_user_prompt = read_text_file(\n            prompts_dir / \"key_achievements_user_prompt.md\"\n        )\n\n        self.text_rationalization_system_prompt = read_text_file(\n            prompts_dir / \"text_rationalization_system_prompt.md\"\n        )\n        self.text_rationalization_user_prompt = read_text_file(\n            prompts_dir / \"text_rationalization_user_prompt.md\"\n        )\n\n        self.text_evaluator_system_prompt = read_text_file(\n            prompts_dir / \"text_evaluator_system_prompt.md\"\n        )\n        self.text_evaluator_user_prompt = read_text_file(\n            prompts_dir / \"text_evaluator_user_prompt.md\"\n        )\n\n        # Compose prompts with input placeholders for clarity.\n\n        logger.debug(\"TextModifiers initialized successfully\")\n\n    @validate_call\n    def summarize(\n        self,\n        *,\n        text: Annotated[str, Field(min_length=1)],\n        max_words: Annotated[int, Field(gt=0)] = 300,\n    ) -&gt; SummarizedText:\n        \"\"\"Summarize text using the configured LLM chain.\n\n        Args:\n            text: The input text to summarize.\n            max_words: Maximum number of words in the summary.\n\n        Returns:\n            SummarizedText: Structured summary with the summarized text.\n\n        Raises:\n            PostconditionError: If the output validation fails.\n        \"\"\"\n        logger.debug(\"summarize: processing text (length={}, max_words={})\", len(text), max_words)\n        # Log the model details as a simple table for traceability and debugging.\n        self._log_model_details_table(\"summarize\")\n\n        messages = [\n            (\"system\", self.summarizer_system_prompt),\n            (\"user\", self.summarizer_user_prompt.format(review=text)),\n        ]\n        prompt = ChatPromptTemplate.from_messages(messages)\n        logger.debug(\"summarizer_llm: {}\", self.summarizer_llm)\n        summarizer = prompt | self.summarizer_llm.with_structured_output(SummarizedText)\n\n        try:\n            result = summarizer.invoke({})\n        except Exception as e:\n            logger.error(\"summarize: LLM invocation failed - {}\", str(e))\n            raise PostconditionError(\n                \"Summarization LLM invocation failed\", operation=\"summarize_llm_invocation\"\n            ) from e\n\n        # Postcondition (O(1)): ensure structured output is valid\n        if not isinstance(result, SummarizedText) or not result.summarized_text:\n            raise_postcondition_error(\n                \"Summarization output validation failed\",\n                context={\n                    \"result_type\": type(result).__name__,\n                    \"has_text\": bool(getattr(result, \"summarized_text\", None)),\n                },\n                operation=\"summarize_validation\",\n            )\n\n        logger.debug(\n            \"summarize: completed successfully (output_length={})\", len(result.summarized_text)\n        )\n        return result\n\n    @validate_call\n    def rationalize_text(self, *, text: Annotated[str, Field(min_length=1)]) -&gt; CopyEditedText:\n        \"\"\"Rationalize text by correcting grammar, spelling, and formatting errors.\n\n        This method performs text rationalization using an LLM-based copy editor that makes\n        minor, localized corrections to improve the professional quality of text while\n        preserving the original meaning, structure, and content. The rationalization process\n        focuses on fixing:\n\n        - Spelling errors and typos (e.g., \"teh\" \u2192 \"the\", \"recieved\" \u2192 \"received\")\n        - Grammar issues (subject-verb agreement, tense consistency, articles)\n        - Punctuation normalization (quotes, commas, dashes, parentheses)\n        - Capitalization consistency (proper nouns, product names, teams)\n        - Whitespace and formatting standardization\n        - Casual shorthand replacement with formal equivalents (e.g., \"w/\" \u2192 \"with\")\n\n        The method is specifically designed for employee self-reviews and similar business\n        documents that may contain informal language, typos, or inconsistent formatting\n        from being pasted from drafts or messaging platforms.\n\n        **Preservation Guarantees:**\n        - Original paragraph structure and ordering are maintained exactly\n        - Numerical values and units remain identical (only formatting may be normalized)\n        - Bullet points, headings, and section order are preserved\n        - Author's voice and intent (first/third person) are retained\n        - No content is added, removed, or significantly rewritten\n\n        **Style Transformations:**\n        - Converts informal shorthand to professional language when unambiguous\n        - Normalizes product/tool names for consistency\n        - Standardizes punctuation and spacing around units and symbols\n        - Removes casual interjections while preserving meaning\n        - Corrects double negatives in reduction statements (e.g., \"reduced by -38%\" \u2192 \"reduced by 38%\")  # noqa: E501\n\n        Args:\n            text: The input text to rationalize. Must be non-empty string containing\n                the content to be copy-edited. Typically employee self-review text\n                or similar business documents that need professional polish.\n\n        Returns:\n            CopyEditedText: A structured response containing:\n                - copy_edited_text: The rationalized text with corrections applied\n                - size: Estimated token count of the rationalized text\n                - is_edited: Boolean indicating whether any changes were made\n\n        Raises:\n            ValidationError: If the input text is empty or invalid.\n            PostconditionError: If the LLM output validation fails or the structured\n                response cannot be parsed correctly.\n            ConfigurationError: If the copy editor LLM is not properly configured.\n\n        Example:\n            &gt;&gt;&gt; modifier = TextModifiers()\n            &gt;&gt;&gt; result = modifier.rationalize_text(\n            ...     text=\"I migrated teh system w/ better performance. \"\n            ...          \"Latency dropped by -38% after optimizations.\"\n            ... )\n            &gt;&gt;&gt; print(result.copy_edited_text)\n            \"I migrated the system with better performance. \"\n            \"Latency dropped by 38% after optimizations.\"\n            &gt;&gt;&gt; print(result.is_edited)\n            True\n\n        Note:\n            This method uses the text_rationalization_system_prompt.md and\n            text_rationalization_user_prompt.md templates to guide the LLM's\n            behavior. The rationalization is performed by the copy_editor_llm\n            configured in the model registry.\n        \"\"\"\n        logger.debug(\"rationalize_text: processing text (length={})\", len(text))\n        # Log the model details as a simple table for traceability and debugging.\n        self._log_model_details_table(\"rationalize_text\")\n\n        # Construct the prompt using the text rationalization templates\n        messages = [\n            (\"system\", self.text_rationalization_system_prompt),\n            (\"user\", self.text_rationalization_user_prompt.format(EMPLOYEE_REVIEW_TEXT=text)),\n        ]\n        prompt = ChatPromptTemplate.from_messages(messages)\n\n        # Create the rationalization chain with structured output\n        rationalizer = prompt | self.copy_editor_llm.with_structured_output(CopyEditedText)\n\n        # Invoke the chain with exception handling\n        try:\n            result = rationalizer.invoke({})\n        except Exception as e:\n            logger.error(\"rationalize_text: LLM invocation failed - {}\", str(e))\n            raise PostconditionError(\n                \"Text rationalization LLM invocation failed\",\n                operation=\"rationalize_text_llm_invocation\",\n            ) from e\n\n        # Postcondition (O(1)): ensure structured output is valid\n        if not isinstance(result, CopyEditedText) or not result.copy_edited_text:\n            raise_postcondition_error(\n                \"Text rationalization output validation failed\",\n                context={\n                    \"result_type\": type(result).__name__,\n                    \"has_text\": bool(getattr(result, \"copy_edited_text\", None)),\n                },\n                operation=\"rationalize_text_validation\",\n            )\n\n        logger.debug(\n            \"rationalize_text: completed successfully (output_length={}, edited={})\",\n            len(result.copy_edited_text),\n            result.is_edited,\n        )\n        return result\n\n    # =========================================================================\n\n    @validate_call\n    def extract_achievements(\n        self, *, text: Annotated[str, Field(min_length=1)]\n    ) -&gt; AchievementsList:\n        \"\"\"Extract key achievements from employee self-review text.\n\n        This method analyzes employee self-review text and extracts up to 10 key achievements\n        with structured metadata including impact areas, metrics, timeframes, and\n        collaboration details. The extraction focuses on identifying concrete outcomes\n        and business impact rather than activities or tasks.\n\n        The method uses an LLM-based achievement extractor that follows strict guidelines\n        to identify and rank achievements by:\n        - Business/customer impact (revenue, cost, risk, user outcomes)\n        - Reliability/quality/security improvements (SLO/MTTR/incidents/defects)\n        - Breadth of ownership (Cross-team/Org-wide &gt; TechLead &gt; IC)\n        - Adoption/usage and external validation\n        - Recency (tie-breaker)\n\n        **Achievement Structure:**\n        Each extracted achievement includes:\n        - **title**: Concise, outcome-oriented label (\u226412 words)\n        - **outcome**: Impact/result description (\u226440 words)\n        - **impact_area**: Categorized impact type (reliability, performance, security, etc.)\n        - **metric_strings**: Verbatim numbers/units from the review text\n        - **timeframe**: Explicit time period if stated (e.g., \"Q2 2025\", \"H1\")\n        - **ownership_scope**: Leadership level if explicit (IC, TechLead, Manager, etc.)\n        - **collaborators**: Named people/teams if mentioned\n\n        **Quality Guarantees:**\n        - No invented metrics, dates, or collaborators - only explicit information\n        - Numbers and units copied exactly as they appear in the review\n        - Achievements are deduplicated and ranked by impact\n        - Maximum of 10 achievements returned, fewer if insufficient quality achievements exist\n\n        Args:\n            text: The employee self-review text to analyze. Must be non-empty string\n                containing the review content from which to extract achievements.\n                Typically contains mixed content (tasks, outcomes, anecdotes) that\n                needs to be parsed for concrete accomplishments.\n\n        Returns:\n            AchievementsList: A structured response containing:\n                - items: List of up to 10 Achievement objects ranked by impact\n                - size: Token estimate of concatenated titles and outcomes\n                - unit: Always \"tokens\"\n\n        Raises:\n            ValidationError: If the input text is empty or invalid.\n            PostconditionError: If the LLM output validation fails or the structured\n                response cannot be parsed correctly.\n            ConfigurationError: If the key achievements LLM is not properly configured.\n\n        Example:\n            &gt;&gt;&gt; modifier = TextModifiers()\n            &gt;&gt;&gt; result = modifier.extract_achievements(\n            ...     text=\"I reduced checkout p95 latency from 480ms to 190ms in H1 2025 \"\n            ...          \"by redesigning the caching layer with the Payments and SRE teams. \"\n            ...          \"This improved conversion rates during peak traffic periods.\"\n            ... )\n            &gt;&gt;&gt; print(result.items[0].title)\n            \"Cut checkout p95 latency\"\n            &gt;&gt;&gt; print(result.items[0].impact_area)\n            \"performance\"\n            &gt;&gt;&gt; print(result.items[0].metric_strings)\n            [\"480ms\", \"190ms\"]\n\n        Note:\n            This method uses the key_achievements_system_prompt.md and\n            key_achievements_user_prompt.md templates to guide the LLM's behavior.\n            The extraction is performed by the key_achievements_llm configured\n            in the model registry.\n        \"\"\"\n        logger.debug(\"extract_achievements: processing text (length={})\", len(text))\n        # Log the model details as a simple table for traceability and debugging.\n        self._log_model_details_table(\"extract_achievements\")\n\n        # Construct the prompt using the key achievements templates\n        messages = [\n            (\"system\", self.key_achievements_system_prompt),\n            (\"user\", self.key_achievements_user_prompt.format(EMPLOYEE_REVIEW_TEXT=text)),\n        ]\n        prompt = ChatPromptTemplate.from_messages(messages)\n\n        # Create the achievement extraction chain with structured output\n        extractor = prompt | self.key_achievements_llm.with_structured_output(AchievementsList)\n\n        # Invoke the chain with exception handling\n        try:\n            result = extractor.invoke({})\n        except Exception as e:\n            logger.error(\"extract_achievements: LLM invocation failed - {}\", str(e))\n            raise PostconditionError(\n                \"Achievement extraction LLM invocation failed\",\n                operation=\"extract_achievements_llm_invocation\",\n            ) from e\n\n        # Postcondition (O(1)): ensure structured output is valid\n        if not isinstance(result, AchievementsList):\n            raise_postcondition_error(\n                \"Achievement extraction output validation failed\",\n                context={\n                    \"result_type\": type(result).__name__,\n                    \"has_items\": bool(getattr(result, \"items\", None)),\n                },\n                operation=\"extract_achievements_validation\",\n            )\n\n        logger.debug(\n            \"extract_achievements: completed successfully (items_count={}, total_size={})\",\n            len(result.items),\n            result.size,\n        )\n        return result\n\n    # =========================================================================\n\n    @validate_call\n    def evaluate_review_text(self, *, text: Annotated[str, Field(min_length=1)]) -&gt; ReviewScorecard:\n        \"\"\"Evaluate the writing quality of employee self-review text.\n\n        This method analyzes employee self-review text and provides a comprehensive\n        assessment of writing quality across six key dimensions. The evaluation\n        focuses on the quality of the writing itself, not job performance, to help\n        HR partners and engineering leaders quickly assess review quality.\n\n        The method uses an LLM-based evaluator that scores the review on:\n        - **OutcomeOverActivity** (25%): Emphasis on concrete outcomes vs. task lists\n        - **QuantitativeSpecificity** (25%): Use of metrics, numbers, and baselines\n        - **ClarityCoherence** (15%): Logical flow and readability\n        - **Conciseness** (15%): Efficient, non-redundant expression\n        - **OwnershipLeadership** (10%): Clear ownership and leadership signals\n        - **Collaboration** (10%): Evidence of cross-team work and partnerships\n\n        **Scoring System:**\n        Each dimension is scored 0-100 using anchor levels (20/40/60/80/95) with\n        specific rubrics. The overall score is a weighted average that determines\n        the verdict: excellent (\u226585), strong (70-84), mixed (50-69), weak (&lt;50).\n\n        **Output Structure:**\n        - Individual scores and rationales for each dimension\n        - Specific, actionable improvement suggestions\n        - Weighted overall score and verdict classification\n        - Optional flags for common issues (e.g., no_numbers_detected, short_review)\n        - Radar chart data for visualization\n\n        Args:\n            text: The employee self-review text to evaluate. Must be non-empty string\n                containing the review content to be assessed for writing quality.\n                Typically contains mixed content about achievements, projects, and\n                activities that needs quality assessment.\n\n        Returns:\n            ReviewScorecard: A structured assessment containing:\n                - metrics: List of 6 MetricScore objects with scores, rationales, suggestions\n                - overall: Weighted average score (0-100)\n                - verdict: Quality classification (excellent/strong/mixed/weak)\n                - notes: Optional flags for specific issues detected\n                - radar_labels: Metric names for chart visualization\n                - radar_values: Corresponding scores for chart visualization\n\n        Raises:\n            ValidationError: If the input text is empty or invalid.\n            PostconditionError: If the LLM output validation fails or the structured\n                response cannot be parsed correctly.\n            ConfigurationError: If the review text evaluator LLM is not properly configured.\n\n        Example:\n            &gt;&gt;&gt; modifier = TextModifiers()\n            &gt;&gt;&gt; result = modifier.evaluate_review_text(\n            ...     text=\"I reduced latency from 480ms to 190ms by optimizing the cache. \"\n            ...          \"This improved user experience and reduced server costs by 15%.\"\n            ... )\n            &gt;&gt;&gt; print(result.overall)\n            75\n            &gt;&gt;&gt; print(result.verdict)\n            \"strong\"\n            &gt;&gt;&gt; print(result.metrics[0].name)\n            \"OutcomeOverActivity\"\n\n        Note:\n            This method uses the text_evaluator_system_prompt.md and\n            text_evaluator_user_prompt.md templates to guide the LLM's behavior.\n            The evaluation is performed by the review_text_evaluator_llm configured\n            in the model registry.\n        \"\"\"\n        logger.debug(\"evaluate_review_text: processing text (length={})\", len(text))\n        # Log the model details as a simple table for traceability and debugging.\n        self._log_model_details_table(\"evaluate_review_text\")\n\n        # Construct the prompt using the text evaluator templates\n        messages = [\n            (\"system\", self.text_evaluator_system_prompt),\n            (\"user\", self.text_evaluator_user_prompt.format(EMPLOYEE_REVIEW_TEXT=text)),\n        ]\n        prompt = ChatPromptTemplate.from_messages(messages)\n\n        # Create the evaluation chain with structured output\n        evaluator = prompt | self.review_text_evaluator_llm.with_structured_output(ReviewScorecard)\n\n        # Invoke the chain with exception handling\n        try:\n            result = evaluator.invoke({})\n        except Exception as e:\n            logger.error(\"evaluate_review_text: LLM invocation failed - {}\", str(e))\n            raise PostconditionError(\n                \"Review text evaluation LLM invocation failed\",\n                operation=\"evaluate_review_text_llm_invocation\",\n            ) from e\n\n        # Postcondition (O(1)): ensure structured output is valid\n        if (\n            not isinstance(result, ReviewScorecard)\n            or not result.metrics\n            or len(result.metrics) != 6\n        ):\n            raise_postcondition_error(\n                \"Review text evaluation output validation failed\",\n                context={\n                    \"result_type\": type(result).__name__,\n                    \"has_metrics\": bool(getattr(result, \"metrics\", None)),\n                    \"metrics_count\": len(getattr(result, \"metrics\", [])),\n                },\n                operation=\"evaluate_review_text_validation\",\n            )\n\n        logger.debug(\n            \"evaluate_review_text: completed successfully (overall_score={}, verdict={})\",\n            result.overall,\n            result.verdict,\n        )\n        return result\n\n    # =========================================================================\n\n    def get_model_info(self, method: str) -&gt; dict[str, Any] | None:\n        \"\"\"Get model configuration information for a specific method.\n\n        Args:\n            method: The method name (e.g., \"summarize\", \"rationalize_text\", \n                \"extract_achievements\", \"evaluate_review_text\").\n\n        Returns:\n            Dictionary containing model configuration or None if not found.\n        \"\"\"\n        method_to_llm = {\n            \"summarize\": self.summarizer_llm,\n            \"rationalize_text\": self.copy_editor_llm,\n            \"extract_achievements\": self.key_achievements_llm,\n            \"evaluate_review_text\": self.review_text_evaluator_llm,\n        }\n\n        llm = method_to_llm.get(method)\n        if not llm:\n            return None\n\n        # Extract configuration from the LLM instance\n        model_info = {\n            \"model\": getattr(llm, \"model_name\", \"N/A\"),\n            \"temperature\": getattr(llm, \"temperature\", \"N/A\"),\n            \"max_tokens\": getattr(llm, \"max_tokens\", \"N/A\"),\n            \"timeout\": getattr(llm, \"request_timeout\", \"N/A\"),\n        }\n\n        # Add optional parameters if they exist\n        for attr in (\"top_p\", \"frequency_penalty\", \"presence_penalty\"):\n            value = getattr(llm, attr, None)\n            if value is not None:\n                model_info[attr] = value\n\n        return model_info\n\n    def _log_model_details_table(self, method: str) -&gt; None:\n        \"\"\"Log the LLM model details as a table for the given TextModifiers method.\n\n        Args:\n            method: The name of the method (e.g., \"summarize\", \"rationalize_text\").\n        \"\"\"\n        # Defensive: ensure the model config is present and has expected keys\n        model_info = self.get_model_info(method)\n        if not model_info:\n            logger.warning(\"No model info found for method '{}'\", method)\n            return\n\n        # Prepare table rows\n        rows = [\n            (\"Model\", model_info.get(\"model\", \"N/A\")),\n            (\"Temperature\", model_info.get(\"temperature\", \"N/A\")),\n            (\"Max Tokens\", model_info.get(\"max_tokens\", \"N/A\")),\n            (\"Timeout\", model_info.get(\"timeout\", \"N/A\")),\n        ]\n        # Optional fields\n        for key in (\"top_p\", \"frequency_penalty\", \"presence_penalty\"):\n            if key in model_info:\n                rows.append((key.replace(\"_\", \" \").title(), model_info[key]))\n\n        # Format as table\n        col_width = max(len(str(k)) for k, _ in rows) + 2\n        table_lines = [\n            f\"{'Parameter'.ljust(col_width)}| Value\",\n            f\"{'-' * (col_width)}|{'-' * 20}\",\n        ]\n        for k, v in rows:\n            table_lines.append(f\"{str(k).ljust(col_width)}| {v}\")\n\n        logger.info(\"LLM Model Details for '{}':\\n{}\", method, \"\\n\".join(table_lines))\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#metamorphosis.mcp.text_modifiers.TextModifiers.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the TextModifiers with LLM chains and prompt templates.</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If required environment variables are missing.</p> <code>FileOperationError</code> <p>If prompt files cannot be loaded.</p> Source code in <code>src/metamorphosis/mcp/text_modifiers.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the TextModifiers with LLM chains and prompt templates.\n\n    Raises:\n        ConfigurationError: If required environment variables are missing.\n        FileOperationError: If prompt files cannot be loaded.\n    \"\"\"\n    logger.debug(\"Initializing TextModifiers\")\n\n    # Acquire LLM clients from the central registry\n    registry = get_model_registry()\n    self.summarizer_llm = registry.summarizer_llm\n    self.copy_editor_llm = registry.copy_editor_llm\n    self.key_achievements_llm = registry.key_achievements_llm\n    self.review_text_evaluator_llm = registry.review_text_evaluator_llm\n\n    # Load prompt templates from files using utility functions\n    project_root = get_project_root()\n    prompts_dir = project_root / \"prompts\"\n    logger.debug(\"Using prompts directory: {}\", prompts_dir)\n\n    # Load as-is; no VOICE placeholder expected in the template\n    self.summarizer_system_prompt = read_text_file(prompts_dir / \"summarizer_system_prompt.md\")\n    self.summarizer_user_prompt = read_text_file(prompts_dir / \"summarizer_user_prompt.md\")\n\n    self.key_achievements_system_prompt = read_text_file(\n        prompts_dir / \"key_achievements_system_prompt.md\"\n    )\n    self.key_achievements_user_prompt = read_text_file(\n        prompts_dir / \"key_achievements_user_prompt.md\"\n    )\n\n    self.text_rationalization_system_prompt = read_text_file(\n        prompts_dir / \"text_rationalization_system_prompt.md\"\n    )\n    self.text_rationalization_user_prompt = read_text_file(\n        prompts_dir / \"text_rationalization_user_prompt.md\"\n    )\n\n    self.text_evaluator_system_prompt = read_text_file(\n        prompts_dir / \"text_evaluator_system_prompt.md\"\n    )\n    self.text_evaluator_user_prompt = read_text_file(\n        prompts_dir / \"text_evaluator_user_prompt.md\"\n    )\n\n    # Compose prompts with input placeholders for clarity.\n\n    logger.debug(\"TextModifiers initialized successfully\")\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#metamorphosis.mcp.text_modifiers.TextModifiers.evaluate_review_text","title":"<code>evaluate_review_text(*, text)</code>","text":"<p>Evaluate the writing quality of employee self-review text.</p> <p>This method analyzes employee self-review text and provides a comprehensive assessment of writing quality across six key dimensions. The evaluation focuses on the quality of the writing itself, not job performance, to help HR partners and engineering leaders quickly assess review quality.</p> <p>The method uses an LLM-based evaluator that scores the review on: - OutcomeOverActivity (25%): Emphasis on concrete outcomes vs. task lists - QuantitativeSpecificity (25%): Use of metrics, numbers, and baselines - ClarityCoherence (15%): Logical flow and readability - Conciseness (15%): Efficient, non-redundant expression - OwnershipLeadership (10%): Clear ownership and leadership signals - Collaboration (10%): Evidence of cross-team work and partnerships</p> <p>Scoring System: Each dimension is scored 0-100 using anchor levels (20/40/60/80/95) with specific rubrics. The overall score is a weighted average that determines the verdict: excellent (\u226585), strong (70-84), mixed (50-69), weak (&lt;50).</p> <p>Output Structure: - Individual scores and rationales for each dimension - Specific, actionable improvement suggestions - Weighted overall score and verdict classification - Optional flags for common issues (e.g., no_numbers_detected, short_review) - Radar chart data for visualization</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Annotated[str, Field(min_length=1)]</code> <p>The employee self-review text to evaluate. Must be non-empty string containing the review content to be assessed for writing quality. Typically contains mixed content about achievements, projects, and activities that needs quality assessment.</p> required <p>Returns:</p> Name Type Description <code>ReviewScorecard</code> <code>ReviewScorecard</code> <p>A structured assessment containing: - metrics: List of 6 MetricScore objects with scores, rationales, suggestions - overall: Weighted average score (0-100) - verdict: Quality classification (excellent/strong/mixed/weak) - notes: Optional flags for specific issues detected - radar_labels: Metric names for chart visualization - radar_values: Corresponding scores for chart visualization</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the input text is empty or invalid.</p> <code>PostconditionError</code> <p>If the LLM output validation fails or the structured response cannot be parsed correctly.</p> <code>ConfigurationError</code> <p>If the review text evaluator LLM is not properly configured.</p> Example <p>modifier = TextModifiers() result = modifier.evaluate_review_text( ...     text=\"I reduced latency from 480ms to 190ms by optimizing the cache. \" ...          \"This improved user experience and reduced server costs by 15%.\" ... ) print(result.overall) 75 print(result.verdict) \"strong\" print(result.metrics[0].name) \"OutcomeOverActivity\"</p> Note <p>This method uses the text_evaluator_system_prompt.md and text_evaluator_user_prompt.md templates to guide the LLM's behavior. The evaluation is performed by the review_text_evaluator_llm configured in the model registry.</p> Source code in <code>src/metamorphosis/mcp/text_modifiers.py</code> <pre><code>@validate_call\ndef evaluate_review_text(self, *, text: Annotated[str, Field(min_length=1)]) -&gt; ReviewScorecard:\n    \"\"\"Evaluate the writing quality of employee self-review text.\n\n    This method analyzes employee self-review text and provides a comprehensive\n    assessment of writing quality across six key dimensions. The evaluation\n    focuses on the quality of the writing itself, not job performance, to help\n    HR partners and engineering leaders quickly assess review quality.\n\n    The method uses an LLM-based evaluator that scores the review on:\n    - **OutcomeOverActivity** (25%): Emphasis on concrete outcomes vs. task lists\n    - **QuantitativeSpecificity** (25%): Use of metrics, numbers, and baselines\n    - **ClarityCoherence** (15%): Logical flow and readability\n    - **Conciseness** (15%): Efficient, non-redundant expression\n    - **OwnershipLeadership** (10%): Clear ownership and leadership signals\n    - **Collaboration** (10%): Evidence of cross-team work and partnerships\n\n    **Scoring System:**\n    Each dimension is scored 0-100 using anchor levels (20/40/60/80/95) with\n    specific rubrics. The overall score is a weighted average that determines\n    the verdict: excellent (\u226585), strong (70-84), mixed (50-69), weak (&lt;50).\n\n    **Output Structure:**\n    - Individual scores and rationales for each dimension\n    - Specific, actionable improvement suggestions\n    - Weighted overall score and verdict classification\n    - Optional flags for common issues (e.g., no_numbers_detected, short_review)\n    - Radar chart data for visualization\n\n    Args:\n        text: The employee self-review text to evaluate. Must be non-empty string\n            containing the review content to be assessed for writing quality.\n            Typically contains mixed content about achievements, projects, and\n            activities that needs quality assessment.\n\n    Returns:\n        ReviewScorecard: A structured assessment containing:\n            - metrics: List of 6 MetricScore objects with scores, rationales, suggestions\n            - overall: Weighted average score (0-100)\n            - verdict: Quality classification (excellent/strong/mixed/weak)\n            - notes: Optional flags for specific issues detected\n            - radar_labels: Metric names for chart visualization\n            - radar_values: Corresponding scores for chart visualization\n\n    Raises:\n        ValidationError: If the input text is empty or invalid.\n        PostconditionError: If the LLM output validation fails or the structured\n            response cannot be parsed correctly.\n        ConfigurationError: If the review text evaluator LLM is not properly configured.\n\n    Example:\n        &gt;&gt;&gt; modifier = TextModifiers()\n        &gt;&gt;&gt; result = modifier.evaluate_review_text(\n        ...     text=\"I reduced latency from 480ms to 190ms by optimizing the cache. \"\n        ...          \"This improved user experience and reduced server costs by 15%.\"\n        ... )\n        &gt;&gt;&gt; print(result.overall)\n        75\n        &gt;&gt;&gt; print(result.verdict)\n        \"strong\"\n        &gt;&gt;&gt; print(result.metrics[0].name)\n        \"OutcomeOverActivity\"\n\n    Note:\n        This method uses the text_evaluator_system_prompt.md and\n        text_evaluator_user_prompt.md templates to guide the LLM's behavior.\n        The evaluation is performed by the review_text_evaluator_llm configured\n        in the model registry.\n    \"\"\"\n    logger.debug(\"evaluate_review_text: processing text (length={})\", len(text))\n    # Log the model details as a simple table for traceability and debugging.\n    self._log_model_details_table(\"evaluate_review_text\")\n\n    # Construct the prompt using the text evaluator templates\n    messages = [\n        (\"system\", self.text_evaluator_system_prompt),\n        (\"user\", self.text_evaluator_user_prompt.format(EMPLOYEE_REVIEW_TEXT=text)),\n    ]\n    prompt = ChatPromptTemplate.from_messages(messages)\n\n    # Create the evaluation chain with structured output\n    evaluator = prompt | self.review_text_evaluator_llm.with_structured_output(ReviewScorecard)\n\n    # Invoke the chain with exception handling\n    try:\n        result = evaluator.invoke({})\n    except Exception as e:\n        logger.error(\"evaluate_review_text: LLM invocation failed - {}\", str(e))\n        raise PostconditionError(\n            \"Review text evaluation LLM invocation failed\",\n            operation=\"evaluate_review_text_llm_invocation\",\n        ) from e\n\n    # Postcondition (O(1)): ensure structured output is valid\n    if (\n        not isinstance(result, ReviewScorecard)\n        or not result.metrics\n        or len(result.metrics) != 6\n    ):\n        raise_postcondition_error(\n            \"Review text evaluation output validation failed\",\n            context={\n                \"result_type\": type(result).__name__,\n                \"has_metrics\": bool(getattr(result, \"metrics\", None)),\n                \"metrics_count\": len(getattr(result, \"metrics\", [])),\n            },\n            operation=\"evaluate_review_text_validation\",\n        )\n\n    logger.debug(\n        \"evaluate_review_text: completed successfully (overall_score={}, verdict={})\",\n        result.overall,\n        result.verdict,\n    )\n    return result\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#metamorphosis.mcp.text_modifiers.TextModifiers.extract_achievements","title":"<code>extract_achievements(*, text)</code>","text":"<p>Extract key achievements from employee self-review text.</p> <p>This method analyzes employee self-review text and extracts up to 10 key achievements with structured metadata including impact areas, metrics, timeframes, and collaboration details. The extraction focuses on identifying concrete outcomes and business impact rather than activities or tasks.</p> <p>The method uses an LLM-based achievement extractor that follows strict guidelines to identify and rank achievements by: - Business/customer impact (revenue, cost, risk, user outcomes) - Reliability/quality/security improvements (SLO/MTTR/incidents/defects) - Breadth of ownership (Cross-team/Org-wide &gt; TechLead &gt; IC) - Adoption/usage and external validation - Recency (tie-breaker)</p> <p>Achievement Structure: Each extracted achievement includes: - title: Concise, outcome-oriented label (\u226412 words) - outcome: Impact/result description (\u226440 words) - impact_area: Categorized impact type (reliability, performance, security, etc.) - metric_strings: Verbatim numbers/units from the review text - timeframe: Explicit time period if stated (e.g., \"Q2 2025\", \"H1\") - ownership_scope: Leadership level if explicit (IC, TechLead, Manager, etc.) - collaborators: Named people/teams if mentioned</p> <p>Quality Guarantees: - No invented metrics, dates, or collaborators - only explicit information - Numbers and units copied exactly as they appear in the review - Achievements are deduplicated and ranked by impact - Maximum of 10 achievements returned, fewer if insufficient quality achievements exist</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Annotated[str, Field(min_length=1)]</code> <p>The employee self-review text to analyze. Must be non-empty string containing the review content from which to extract achievements. Typically contains mixed content (tasks, outcomes, anecdotes) that needs to be parsed for concrete accomplishments.</p> required <p>Returns:</p> Name Type Description <code>AchievementsList</code> <code>AchievementsList</code> <p>A structured response containing: - items: List of up to 10 Achievement objects ranked by impact - size: Token estimate of concatenated titles and outcomes - unit: Always \"tokens\"</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the input text is empty or invalid.</p> <code>PostconditionError</code> <p>If the LLM output validation fails or the structured response cannot be parsed correctly.</p> <code>ConfigurationError</code> <p>If the key achievements LLM is not properly configured.</p> Example <p>modifier = TextModifiers() result = modifier.extract_achievements( ...     text=\"I reduced checkout p95 latency from 480ms to 190ms in H1 2025 \" ...          \"by redesigning the caching layer with the Payments and SRE teams. \" ...          \"This improved conversion rates during peak traffic periods.\" ... ) print(result.items[0].title) \"Cut checkout p95 latency\" print(result.items[0].impact_area) \"performance\" print(result.items[0].metric_strings) [\"480ms\", \"190ms\"]</p> Note <p>This method uses the key_achievements_system_prompt.md and key_achievements_user_prompt.md templates to guide the LLM's behavior. The extraction is performed by the key_achievements_llm configured in the model registry.</p> Source code in <code>src/metamorphosis/mcp/text_modifiers.py</code> <pre><code>@validate_call\ndef extract_achievements(\n    self, *, text: Annotated[str, Field(min_length=1)]\n) -&gt; AchievementsList:\n    \"\"\"Extract key achievements from employee self-review text.\n\n    This method analyzes employee self-review text and extracts up to 10 key achievements\n    with structured metadata including impact areas, metrics, timeframes, and\n    collaboration details. The extraction focuses on identifying concrete outcomes\n    and business impact rather than activities or tasks.\n\n    The method uses an LLM-based achievement extractor that follows strict guidelines\n    to identify and rank achievements by:\n    - Business/customer impact (revenue, cost, risk, user outcomes)\n    - Reliability/quality/security improvements (SLO/MTTR/incidents/defects)\n    - Breadth of ownership (Cross-team/Org-wide &gt; TechLead &gt; IC)\n    - Adoption/usage and external validation\n    - Recency (tie-breaker)\n\n    **Achievement Structure:**\n    Each extracted achievement includes:\n    - **title**: Concise, outcome-oriented label (\u226412 words)\n    - **outcome**: Impact/result description (\u226440 words)\n    - **impact_area**: Categorized impact type (reliability, performance, security, etc.)\n    - **metric_strings**: Verbatim numbers/units from the review text\n    - **timeframe**: Explicit time period if stated (e.g., \"Q2 2025\", \"H1\")\n    - **ownership_scope**: Leadership level if explicit (IC, TechLead, Manager, etc.)\n    - **collaborators**: Named people/teams if mentioned\n\n    **Quality Guarantees:**\n    - No invented metrics, dates, or collaborators - only explicit information\n    - Numbers and units copied exactly as they appear in the review\n    - Achievements are deduplicated and ranked by impact\n    - Maximum of 10 achievements returned, fewer if insufficient quality achievements exist\n\n    Args:\n        text: The employee self-review text to analyze. Must be non-empty string\n            containing the review content from which to extract achievements.\n            Typically contains mixed content (tasks, outcomes, anecdotes) that\n            needs to be parsed for concrete accomplishments.\n\n    Returns:\n        AchievementsList: A structured response containing:\n            - items: List of up to 10 Achievement objects ranked by impact\n            - size: Token estimate of concatenated titles and outcomes\n            - unit: Always \"tokens\"\n\n    Raises:\n        ValidationError: If the input text is empty or invalid.\n        PostconditionError: If the LLM output validation fails or the structured\n            response cannot be parsed correctly.\n        ConfigurationError: If the key achievements LLM is not properly configured.\n\n    Example:\n        &gt;&gt;&gt; modifier = TextModifiers()\n        &gt;&gt;&gt; result = modifier.extract_achievements(\n        ...     text=\"I reduced checkout p95 latency from 480ms to 190ms in H1 2025 \"\n        ...          \"by redesigning the caching layer with the Payments and SRE teams. \"\n        ...          \"This improved conversion rates during peak traffic periods.\"\n        ... )\n        &gt;&gt;&gt; print(result.items[0].title)\n        \"Cut checkout p95 latency\"\n        &gt;&gt;&gt; print(result.items[0].impact_area)\n        \"performance\"\n        &gt;&gt;&gt; print(result.items[0].metric_strings)\n        [\"480ms\", \"190ms\"]\n\n    Note:\n        This method uses the key_achievements_system_prompt.md and\n        key_achievements_user_prompt.md templates to guide the LLM's behavior.\n        The extraction is performed by the key_achievements_llm configured\n        in the model registry.\n    \"\"\"\n    logger.debug(\"extract_achievements: processing text (length={})\", len(text))\n    # Log the model details as a simple table for traceability and debugging.\n    self._log_model_details_table(\"extract_achievements\")\n\n    # Construct the prompt using the key achievements templates\n    messages = [\n        (\"system\", self.key_achievements_system_prompt),\n        (\"user\", self.key_achievements_user_prompt.format(EMPLOYEE_REVIEW_TEXT=text)),\n    ]\n    prompt = ChatPromptTemplate.from_messages(messages)\n\n    # Create the achievement extraction chain with structured output\n    extractor = prompt | self.key_achievements_llm.with_structured_output(AchievementsList)\n\n    # Invoke the chain with exception handling\n    try:\n        result = extractor.invoke({})\n    except Exception as e:\n        logger.error(\"extract_achievements: LLM invocation failed - {}\", str(e))\n        raise PostconditionError(\n            \"Achievement extraction LLM invocation failed\",\n            operation=\"extract_achievements_llm_invocation\",\n        ) from e\n\n    # Postcondition (O(1)): ensure structured output is valid\n    if not isinstance(result, AchievementsList):\n        raise_postcondition_error(\n            \"Achievement extraction output validation failed\",\n            context={\n                \"result_type\": type(result).__name__,\n                \"has_items\": bool(getattr(result, \"items\", None)),\n            },\n            operation=\"extract_achievements_validation\",\n        )\n\n    logger.debug(\n        \"extract_achievements: completed successfully (items_count={}, total_size={})\",\n        len(result.items),\n        result.size,\n    )\n    return result\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#metamorphosis.mcp.text_modifiers.TextModifiers.get_model_info","title":"<code>get_model_info(method)</code>","text":"<p>Get model configuration information for a specific method.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method name (e.g., \"summarize\", \"rationalize_text\",  \"extract_achievements\", \"evaluate_review_text\").</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>Dictionary containing model configuration or None if not found.</p> Source code in <code>src/metamorphosis/mcp/text_modifiers.py</code> <pre><code>def get_model_info(self, method: str) -&gt; dict[str, Any] | None:\n    \"\"\"Get model configuration information for a specific method.\n\n    Args:\n        method: The method name (e.g., \"summarize\", \"rationalize_text\", \n            \"extract_achievements\", \"evaluate_review_text\").\n\n    Returns:\n        Dictionary containing model configuration or None if not found.\n    \"\"\"\n    method_to_llm = {\n        \"summarize\": self.summarizer_llm,\n        \"rationalize_text\": self.copy_editor_llm,\n        \"extract_achievements\": self.key_achievements_llm,\n        \"evaluate_review_text\": self.review_text_evaluator_llm,\n    }\n\n    llm = method_to_llm.get(method)\n    if not llm:\n        return None\n\n    # Extract configuration from the LLM instance\n    model_info = {\n        \"model\": getattr(llm, \"model_name\", \"N/A\"),\n        \"temperature\": getattr(llm, \"temperature\", \"N/A\"),\n        \"max_tokens\": getattr(llm, \"max_tokens\", \"N/A\"),\n        \"timeout\": getattr(llm, \"request_timeout\", \"N/A\"),\n    }\n\n    # Add optional parameters if they exist\n    for attr in (\"top_p\", \"frequency_penalty\", \"presence_penalty\"):\n        value = getattr(llm, attr, None)\n        if value is not None:\n            model_info[attr] = value\n\n    return model_info\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#metamorphosis.mcp.text_modifiers.TextModifiers.rationalize_text","title":"<code>rationalize_text(*, text)</code>","text":"<p>Rationalize text by correcting grammar, spelling, and formatting errors.</p> <p>This method performs text rationalization using an LLM-based copy editor that makes minor, localized corrections to improve the professional quality of text while preserving the original meaning, structure, and content. The rationalization process focuses on fixing:</p> <ul> <li>Spelling errors and typos (e.g., \"teh\" \u2192 \"the\", \"recieved\" \u2192 \"received\")</li> <li>Grammar issues (subject-verb agreement, tense consistency, articles)</li> <li>Punctuation normalization (quotes, commas, dashes, parentheses)</li> <li>Capitalization consistency (proper nouns, product names, teams)</li> <li>Whitespace and formatting standardization</li> <li>Casual shorthand replacement with formal equivalents (e.g., \"w/\" \u2192 \"with\")</li> </ul> <p>The method is specifically designed for employee self-reviews and similar business documents that may contain informal language, typos, or inconsistent formatting from being pasted from drafts or messaging platforms.</p> <p>Preservation Guarantees: - Original paragraph structure and ordering are maintained exactly - Numerical values and units remain identical (only formatting may be normalized) - Bullet points, headings, and section order are preserved - Author's voice and intent (first/third person) are retained - No content is added, removed, or significantly rewritten</p> <p>Style Transformations: - Converts informal shorthand to professional language when unambiguous - Normalizes product/tool names for consistency - Standardizes punctuation and spacing around units and symbols - Removes casual interjections while preserving meaning - Corrects double negatives in reduction statements (e.g., \"reduced by -38%\" \u2192 \"reduced by 38%\")  # noqa: E501</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Annotated[str, Field(min_length=1)]</code> <p>The input text to rationalize. Must be non-empty string containing the content to be copy-edited. Typically employee self-review text or similar business documents that need professional polish.</p> required <p>Returns:</p> Name Type Description <code>CopyEditedText</code> <code>CopyEditedText</code> <p>A structured response containing: - copy_edited_text: The rationalized text with corrections applied - size: Estimated token count of the rationalized text - is_edited: Boolean indicating whether any changes were made</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the input text is empty or invalid.</p> <code>PostconditionError</code> <p>If the LLM output validation fails or the structured response cannot be parsed correctly.</p> <code>ConfigurationError</code> <p>If the copy editor LLM is not properly configured.</p> Example <p>modifier = TextModifiers() result = modifier.rationalize_text( ...     text=\"I migrated teh system w/ better performance. \" ...          \"Latency dropped by -38% after optimizations.\" ... ) print(result.copy_edited_text) \"I migrated the system with better performance. \" \"Latency dropped by 38% after optimizations.\" print(result.is_edited) True</p> Note <p>This method uses the text_rationalization_system_prompt.md and text_rationalization_user_prompt.md templates to guide the LLM's behavior. The rationalization is performed by the copy_editor_llm configured in the model registry.</p> Source code in <code>src/metamorphosis/mcp/text_modifiers.py</code> <pre><code>@validate_call\ndef rationalize_text(self, *, text: Annotated[str, Field(min_length=1)]) -&gt; CopyEditedText:\n    \"\"\"Rationalize text by correcting grammar, spelling, and formatting errors.\n\n    This method performs text rationalization using an LLM-based copy editor that makes\n    minor, localized corrections to improve the professional quality of text while\n    preserving the original meaning, structure, and content. The rationalization process\n    focuses on fixing:\n\n    - Spelling errors and typos (e.g., \"teh\" \u2192 \"the\", \"recieved\" \u2192 \"received\")\n    - Grammar issues (subject-verb agreement, tense consistency, articles)\n    - Punctuation normalization (quotes, commas, dashes, parentheses)\n    - Capitalization consistency (proper nouns, product names, teams)\n    - Whitespace and formatting standardization\n    - Casual shorthand replacement with formal equivalents (e.g., \"w/\" \u2192 \"with\")\n\n    The method is specifically designed for employee self-reviews and similar business\n    documents that may contain informal language, typos, or inconsistent formatting\n    from being pasted from drafts or messaging platforms.\n\n    **Preservation Guarantees:**\n    - Original paragraph structure and ordering are maintained exactly\n    - Numerical values and units remain identical (only formatting may be normalized)\n    - Bullet points, headings, and section order are preserved\n    - Author's voice and intent (first/third person) are retained\n    - No content is added, removed, or significantly rewritten\n\n    **Style Transformations:**\n    - Converts informal shorthand to professional language when unambiguous\n    - Normalizes product/tool names for consistency\n    - Standardizes punctuation and spacing around units and symbols\n    - Removes casual interjections while preserving meaning\n    - Corrects double negatives in reduction statements (e.g., \"reduced by -38%\" \u2192 \"reduced by 38%\")  # noqa: E501\n\n    Args:\n        text: The input text to rationalize. Must be non-empty string containing\n            the content to be copy-edited. Typically employee self-review text\n            or similar business documents that need professional polish.\n\n    Returns:\n        CopyEditedText: A structured response containing:\n            - copy_edited_text: The rationalized text with corrections applied\n            - size: Estimated token count of the rationalized text\n            - is_edited: Boolean indicating whether any changes were made\n\n    Raises:\n        ValidationError: If the input text is empty or invalid.\n        PostconditionError: If the LLM output validation fails or the structured\n            response cannot be parsed correctly.\n        ConfigurationError: If the copy editor LLM is not properly configured.\n\n    Example:\n        &gt;&gt;&gt; modifier = TextModifiers()\n        &gt;&gt;&gt; result = modifier.rationalize_text(\n        ...     text=\"I migrated teh system w/ better performance. \"\n        ...          \"Latency dropped by -38% after optimizations.\"\n        ... )\n        &gt;&gt;&gt; print(result.copy_edited_text)\n        \"I migrated the system with better performance. \"\n        \"Latency dropped by 38% after optimizations.\"\n        &gt;&gt;&gt; print(result.is_edited)\n        True\n\n    Note:\n        This method uses the text_rationalization_system_prompt.md and\n        text_rationalization_user_prompt.md templates to guide the LLM's\n        behavior. The rationalization is performed by the copy_editor_llm\n        configured in the model registry.\n    \"\"\"\n    logger.debug(\"rationalize_text: processing text (length={})\", len(text))\n    # Log the model details as a simple table for traceability and debugging.\n    self._log_model_details_table(\"rationalize_text\")\n\n    # Construct the prompt using the text rationalization templates\n    messages = [\n        (\"system\", self.text_rationalization_system_prompt),\n        (\"user\", self.text_rationalization_user_prompt.format(EMPLOYEE_REVIEW_TEXT=text)),\n    ]\n    prompt = ChatPromptTemplate.from_messages(messages)\n\n    # Create the rationalization chain with structured output\n    rationalizer = prompt | self.copy_editor_llm.with_structured_output(CopyEditedText)\n\n    # Invoke the chain with exception handling\n    try:\n        result = rationalizer.invoke({})\n    except Exception as e:\n        logger.error(\"rationalize_text: LLM invocation failed - {}\", str(e))\n        raise PostconditionError(\n            \"Text rationalization LLM invocation failed\",\n            operation=\"rationalize_text_llm_invocation\",\n        ) from e\n\n    # Postcondition (O(1)): ensure structured output is valid\n    if not isinstance(result, CopyEditedText) or not result.copy_edited_text:\n        raise_postcondition_error(\n            \"Text rationalization output validation failed\",\n            context={\n                \"result_type\": type(result).__name__,\n                \"has_text\": bool(getattr(result, \"copy_edited_text\", None)),\n            },\n            operation=\"rationalize_text_validation\",\n        )\n\n    logger.debug(\n        \"rationalize_text: completed successfully (output_length={}, edited={})\",\n        len(result.copy_edited_text),\n        result.is_edited,\n    )\n    return result\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#metamorphosis.mcp.text_modifiers.TextModifiers.summarize","title":"<code>summarize(*, text, max_words=300)</code>","text":"<p>Summarize text using the configured LLM chain.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Annotated[str, Field(min_length=1)]</code> <p>The input text to summarize.</p> required <code>max_words</code> <code>Annotated[int, Field(gt=0)]</code> <p>Maximum number of words in the summary.</p> <code>300</code> <p>Returns:</p> Name Type Description <code>SummarizedText</code> <code>SummarizedText</code> <p>Structured summary with the summarized text.</p> <p>Raises:</p> Type Description <code>PostconditionError</code> <p>If the output validation fails.</p> Source code in <code>src/metamorphosis/mcp/text_modifiers.py</code> <pre><code>@validate_call\ndef summarize(\n    self,\n    *,\n    text: Annotated[str, Field(min_length=1)],\n    max_words: Annotated[int, Field(gt=0)] = 300,\n) -&gt; SummarizedText:\n    \"\"\"Summarize text using the configured LLM chain.\n\n    Args:\n        text: The input text to summarize.\n        max_words: Maximum number of words in the summary.\n\n    Returns:\n        SummarizedText: Structured summary with the summarized text.\n\n    Raises:\n        PostconditionError: If the output validation fails.\n    \"\"\"\n    logger.debug(\"summarize: processing text (length={}, max_words={})\", len(text), max_words)\n    # Log the model details as a simple table for traceability and debugging.\n    self._log_model_details_table(\"summarize\")\n\n    messages = [\n        (\"system\", self.summarizer_system_prompt),\n        (\"user\", self.summarizer_user_prompt.format(review=text)),\n    ]\n    prompt = ChatPromptTemplate.from_messages(messages)\n    logger.debug(\"summarizer_llm: {}\", self.summarizer_llm)\n    summarizer = prompt | self.summarizer_llm.with_structured_output(SummarizedText)\n\n    try:\n        result = summarizer.invoke({})\n    except Exception as e:\n        logger.error(\"summarize: LLM invocation failed - {}\", str(e))\n        raise PostconditionError(\n            \"Summarization LLM invocation failed\", operation=\"summarize_llm_invocation\"\n        ) from e\n\n    # Postcondition (O(1)): ensure structured output is valid\n    if not isinstance(result, SummarizedText) or not result.summarized_text:\n        raise_postcondition_error(\n            \"Summarization output validation failed\",\n            context={\n                \"result_type\": type(result).__name__,\n                \"has_text\": bool(getattr(result, \"summarized_text\", None)),\n            },\n            operation=\"summarize_validation\",\n        )\n\n    logger.debug(\n        \"summarize: completed successfully (output_length={})\", len(result.summarized_text)\n    )\n    return result\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#overview","title":"Overview","text":"<p>The <code>TextModifiers</code> class is the core component of the MCP package, providing comprehensive LLM-backed text processing utilities. It implements four main capabilities: summarization, copy editing (text rationalization), achievement extraction, and review quality evaluation.</p>"},{"location":"metamorphosis/mcp/TextModifiers/#class-architecture","title":"Class Architecture","text":"classDiagram     class TextModifiers {         -summarizer_llm: ChatOpenAI         -copy_editor_llm: ChatOpenAI         -key_achievements_llm: ChatOpenAI         -review_text_evaluator_llm: ChatOpenAI         -summarizer_system_prompt: str         -summarizer_user_prompt: str         -text_rationalization_system_prompt: str         -text_rationalization_user_prompt: str         -key_achievements_system_prompt: str         -key_achievements_user_prompt: str         -text_evaluator_system_prompt: str         -text_evaluator_user_prompt: str         +__init__()         +summarize(text, max_words) SummarizedText         +rationalize_text(text) CopyEditedText         +extract_achievements(text) AchievementsList         +evaluate_review_text(text) ReviewScorecard         +get_model_info(method) dict         -_log_model_details_table(method)     }      class ModelRegistry {         +summarizer_llm: ChatOpenAI         +copy_editor_llm: ChatOpenAI         +key_achievements_llm: ChatOpenAI         +review_text_evaluator_llm: ChatOpenAI     }      class SummarizedText {         +summarized_text: str         +size: int         +unit: str     }      class CopyEditedText {         +copy_edited_text: str         +size: int         +is_edited: bool     }      class AchievementsList {         +items: List[Achievement]         +size: int         +unit: str     }      class ReviewScorecard {         +metrics: List[MetricScore]         +overall: int         +verdict: Verdict         +notes: List[str]         +radar_labels: List[str]         +radar_values: List[int]     }      TextModifiers --&gt; ModelRegistry : uses     TextModifiers --&gt; SummarizedText : creates     TextModifiers --&gt; CopyEditedText : creates     TextModifiers --&gt; AchievementsList : creates     TextModifiers --&gt; ReviewScorecard : creates"},{"location":"metamorphosis/mcp/TextModifiers/#initialization","title":"Initialization","text":""},{"location":"metamorphosis/mcp/TextModifiers/#constructor","title":"Constructor","text":"<pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the TextModifiers with LLM chains and prompt templates.\"\"\"\n</code></pre> <p>The constructor performs several key initialization steps:</p> <ol> <li>LLM Client Acquisition: Retrieves configured LLM clients from the ModelRegistry</li> <li>Prompt Template Loading: Loads external prompt templates from the <code>prompts/</code> directory</li> <li>Validation: Ensures all required components are properly initialized</li> </ol>"},{"location":"metamorphosis/mcp/TextModifiers/#llm-clients","title":"LLM Clients","text":"<p>The class initializes four specialized LLM clients:</p> <ul> <li><code>summarizer_llm</code>: For text summarization (default: gpt-4o)</li> <li><code>copy_editor_llm</code>: For text rationalization (default: gpt-4o)</li> <li><code>key_achievements_llm</code>: For achievement extraction (default: gpt-4o)</li> <li><code>review_text_evaluator_llm</code>: For quality evaluation (default: gpt-5)</li> </ul>"},{"location":"metamorphosis/mcp/TextModifiers/#prompt-templates","title":"Prompt Templates","text":"<p>Prompt templates are loaded from external Markdown files for maintainability:</p> <pre><code># System and user prompts for each capability\nself.summarizer_system_prompt = read_text_file(prompts_dir / \"summarizer_system_prompt.md\")\nself.summarizer_user_prompt = read_text_file(prompts_dir / \"summarizer_user_prompt.md\")\n# ... (similar for other capabilities)\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#core-methods","title":"Core Methods","text":""},{"location":"metamorphosis/mcp/TextModifiers/#1-text-summarization","title":"1. Text Summarization","text":"<pre><code>@validate_call\ndef summarize(\n    self,\n    *,\n    text: Annotated[str, Field(min_length=1)],\n    max_words: Annotated[int, Field(gt=0)] = 300,\n) -&gt; SummarizedText:\n</code></pre> <p>Purpose: Generate abstractive summaries of employee review text.</p> <p>Features: - Configurable word limits (default: 300 words) - Preserves key information and context - Structured output with metadata - Token estimation for billing/monitoring</p> <p>Example Usage: <pre><code>modifier = TextModifiers()\nresult = modifier.summarize(\n    text=\"Long employee review discussing multiple projects and achievements...\",\n    max_words=100\n)\nprint(f\"Summary: {result.summarized_text}\")\nprint(f\"Token estimate: {result.size} {result.unit}\")\n</code></pre></p> <p>Processing Flow: 1. Input validation using Pydantic decorators 2. Model details logging for traceability 3. LLM invocation with structured output parsing 4. Post-condition validation 5. Result packaging in <code>SummarizedText</code> model</p>"},{"location":"metamorphosis/mcp/TextModifiers/#2-text-rationalization-copy-editing","title":"2. Text Rationalization (Copy Editing)","text":"<pre><code>@validate_call\ndef rationalize_text(self, *, text: Annotated[str, Field(min_length=1)]) -&gt; CopyEditedText:\n</code></pre> <p>Purpose: Improve grammar, spelling, and professional tone while preserving meaning.</p> <p>Key Features: - Grammar &amp; Spelling: Fixes errors and typos - Professional Tone: Converts informal language to professional style - Structure Preservation: Maintains original paragraph structure - Content Integrity: No addition or removal of factual content - Voice Preservation: Retains author's perspective (first/third person)</p> <p>Transformation Examples: - <code>\"teh system\"</code> \u2192 <code>\"the system\"</code> - <code>\"w/ better performance\"</code> \u2192 <code>\"with better performance\"</code> - <code>\"reduced by -38%\"</code> \u2192 <code>\"reduced by 38%\"</code></p> <p>Example Usage: <pre><code>result = modifier.rationalize_text(\n    text=\"I migrated teh system w/ better performance. Latency dropped by -38%.\"\n)\nprint(f\"Edited: {result.copy_edited_text}\")\nprint(f\"Was modified: {result.is_edited}\")\n</code></pre></p>"},{"location":"metamorphosis/mcp/TextModifiers/#3-achievement-extraction","title":"3. Achievement Extraction","text":"<pre><code>@validate_call\ndef extract_achievements(\n    self, *, text: Annotated[str, Field(min_length=1)]\n) -&gt; AchievementsList:\n</code></pre> <p>Purpose: Extract and structure key accomplishments from employee reviews.</p> <p>Key Features: - Ranking: Achievements ranked by business impact - Structured Metadata: Impact areas, metrics, timeframes, collaborators - Quality Filtering: Only defensible achievements included - Limit: Up to 10 achievements (configurable in prompt)</p> <p>Achievement Structure: <pre><code>class Achievement:\n    title: str  # Concise outcome-oriented label (\u226412 words)\n    outcome: str  # Impact/result description (\u226440 words)\n    impact_area: ImpactArea  # Categorized impact type\n    metric_strings: List[str]  # Verbatim numbers from text\n    timeframe: Optional[str]  # Explicit time period if stated\n    ownership_scope: Optional[OwnershipScope]  # Leadership level\n    collaborators: List[str]  # Named people/teams\n</code></pre></p> <p>Example Usage: <pre><code>result = modifier.extract_achievements(\n    text=\"I reduced checkout p95 latency from 480ms to 190ms in H1 2025 by redesigning the caching layer with the Payments and SRE teams.\"\n)\n\nfor achievement in result.items:\n    print(f\"Title: {achievement.title}\")\n    print(f\"Impact Area: {achievement.impact_area}\")\n    print(f\"Metrics: {achievement.metric_strings}\")\n    print(f\"Timeframe: {achievement.timeframe}\")\n    print(f\"Collaborators: {achievement.collaborators}\")\n</code></pre></p>"},{"location":"metamorphosis/mcp/TextModifiers/#4-review-quality-evaluation","title":"4. Review Quality Evaluation","text":"<pre><code>@validate_call\ndef evaluate_review_text(\n    self, *, text: Annotated[str, Field(min_length=1)]\n) -&gt; ReviewScorecard:\n</code></pre> <p>Purpose: Assess writing quality across six key dimensions.</p> <p>Evaluation Dimensions (with weights): 1. OutcomeOverActivity (25%): Emphasis on concrete outcomes vs. task lists 2. QuantitativeSpecificity (25%): Use of metrics, numbers, and baselines 3. ClarityCoherence (15%): Logical flow and readability 4. Conciseness (15%): Efficient, non-redundant expression 5. OwnershipLeadership (10%): Clear ownership and leadership signals 6. Collaboration (10%): Evidence of cross-team work</p> <p>Scoring System: - Each dimension scored 0-100 using anchor levels (20/40/60/80/95) - Overall score is weighted average - Verdict classification: excellent (\u226585), strong (70-84), mixed (50-69), weak (&lt;50)</p> <p>Example Usage: <pre><code>result = modifier.evaluate_review_text(\n    text=\"I reduced latency from 480ms to 190ms by optimizing the cache. This improved user experience and reduced server costs by 15%.\"\n)\n\nprint(f\"Overall Score: {result.overall}/100\")\nprint(f\"Verdict: {result.verdict}\")\n\nfor metric in result.metrics:\n    print(f\"{metric.name}: {metric.score}/100\")\n    print(f\"Rationale: {metric.rationale}\")\n    print(f\"Suggestion: {metric.suggestion}\")\n</code></pre></p>"},{"location":"metamorphosis/mcp/TextModifiers/#utility-methods","title":"Utility Methods","text":""},{"location":"metamorphosis/mcp/TextModifiers/#model-information-introspection","title":"Model Information Introspection","text":"<pre><code>def get_model_info(self, method: str) -&gt; dict[str, Any] | None:\n</code></pre> <p>Purpose: Retrieve model configuration for debugging and monitoring.</p> <p>Parameters: - <code>method</code>: The method name (\"summarize\", \"rationalize_text\", \"extract_achievements\", \"evaluate_review_text\")</p> <p>Returns: Dictionary containing model configuration or None if method not found.</p> <p>Example Usage: <pre><code>info = modifier.get_model_info(\"summarize\")\nprint(f\"Model: {info['model']}\")\nprint(f\"Temperature: {info['temperature']}\")\nprint(f\"Max Tokens: {info['max_tokens']}\")\nprint(f\"Timeout: {info['timeout']}\")\n</code></pre></p>"},{"location":"metamorphosis/mcp/TextModifiers/#model-details-logging","title":"Model Details Logging","text":"<pre><code>def _log_model_details_table(self, method: str) -&gt; None:\n</code></pre> <p>Purpose: Log LLM model details in a formatted table for traceability.</p> <p>This private method is automatically called by all processing methods to provide detailed logging of the model configuration being used. This is crucial for debugging and monitoring in production environments.</p> <p>Example Log Output: <pre><code>LLM Model Details for 'summarize':\nParameter       | Value\n----------------|--------------------\nModel           | gpt-4o\nTemperature     | 0.0\nMax Tokens      | 2000\nTimeout         | 120\n</code></pre></p>"},{"location":"metamorphosis/mcp/TextModifiers/#design-patterns","title":"Design Patterns","text":""},{"location":"metamorphosis/mcp/TextModifiers/#validation-first-design","title":"Validation-First Design","text":"<p>All public methods use Pydantic's <code>@validate_call</code> decorator for automatic input validation:</p> <pre><code>@validate_call\ndef summarize(\n    self,\n    *,  # Keyword-only arguments for clarity\n    text: Annotated[str, Field(min_length=1)],  # Must be non-empty\n    max_words: Annotated[int, Field(gt=0)] = 300,  # Must be positive\n) -&gt; SummarizedText:\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#fail-fast-error-handling","title":"Fail-Fast Error Handling","text":"<p>The class implements comprehensive error handling with specific exception types:</p> <pre><code>try:\n    result = summarizer.invoke({})\nexcept Exception as e:\n    logger.error(\"summarize: LLM invocation failed - {}\", str(e))\n    raise PostconditionError(\n        \"Summarization LLM invocation failed\",\n        operation=\"summarize_llm_invocation\"\n    ) from e\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#structured-output-pattern","title":"Structured Output Pattern","text":"<p>All methods return strongly-typed Pydantic models for type safety:</p> <pre><code># Instead of returning raw strings or dictionaries\nreturn SummarizedText(\n    summarized_text=processed_text,\n    size=token_estimate,\n    unit=\"tokens\"\n)\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#template-driven-prompting","title":"Template-Driven Prompting","text":"<p>Prompt templates are externalized for maintainability and version control:</p> <pre><code>messages = [\n    (\"system\", self.summarizer_system_prompt),\n    (\"user\", self.summarizer_user_prompt.format(review=text)),\n]\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#error-handling","title":"Error Handling","text":""},{"location":"metamorphosis/mcp/TextModifiers/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>The class raises specific exceptions for different error conditions:</p> <ul> <li><code>ValidationError</code>: Invalid input parameters</li> <li><code>PostconditionError</code>: LLM invocation or output validation failures</li> <li><code>ConfigurationError</code>: Missing or invalid configuration</li> <li><code>FileOperationError</code>: Prompt template loading issues</li> </ul>"},{"location":"metamorphosis/mcp/TextModifiers/#error-context","title":"Error Context","text":"<p>All exceptions include rich context for debugging:</p> <pre><code>raise_postcondition_error(\n    \"Summarization output validation failed\",\n    context={\n        \"result_type\": type(result).__name__,\n        \"has_text\": bool(getattr(result, 'summarized_text', None))\n    },\n    operation=\"summarize_validation\"\n)\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#performance-considerations","title":"Performance Considerations","text":""},{"location":"metamorphosis/mcp/TextModifiers/#llm-client-reuse","title":"LLM Client Reuse","text":"<p>LLM clients are initialized once and reused across all method calls, avoiding expensive re-initialization.</p>"},{"location":"metamorphosis/mcp/TextModifiers/#prompt-template-caching","title":"Prompt Template Caching","text":"<p>Prompt templates are loaded once during initialization and cached in memory.</p>"},{"location":"metamorphosis/mcp/TextModifiers/#token-optimization","title":"Token Optimization","text":"<ul> <li>Efficient prompt design minimizes token usage</li> <li>Structured output reduces parsing overhead</li> <li>Streaming support for real-time applications</li> </ul>"},{"location":"metamorphosis/mcp/TextModifiers/#memory-management","title":"Memory Management","text":"<ul> <li>Large text inputs are processed efficiently</li> <li>Results include size estimates for monitoring</li> <li>Proper cleanup of temporary objects</li> </ul>"},{"location":"metamorphosis/mcp/TextModifiers/#testing","title":"Testing","text":""},{"location":"metamorphosis/mcp/TextModifiers/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom metamorphosis.mcp.text_modifiers import TextModifiers\nfrom metamorphosis.datamodel import SummarizedText, CopyEditedText\n\nclass TestTextModifiers:\n    def setup_method(self):\n        self.modifier = TextModifiers()\n\n    def test_summarization(self):\n        result = self.modifier.summarize(\n            text=\"Long employee review text...\",\n            max_words=50\n        )\n        assert isinstance(result, SummarizedText)\n        assert len(result.summarized_text) &gt; 0\n        assert result.size &gt; 0\n\n    def test_copy_editing(self):\n        result = self.modifier.rationalize_text(\n            text=\"Text with errers and typos.\"\n        )\n        assert isinstance(result, CopyEditedText)\n        assert \"errors\" in result.copy_edited_text.lower()\n        assert result.is_edited is True\n\n    def test_model_info_retrieval(self):\n        info = self.modifier.get_model_info(\"summarize\")\n        assert info is not None\n        assert \"model\" in info\n        assert \"temperature\" in info\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#integration-tests","title":"Integration Tests","text":"<pre><code>def test_end_to_end_processing(self):\n    modifier = TextModifiers()\n\n    # Test full pipeline\n    text = \"I led a project that improved system performance by 40%...\"\n\n    summary = modifier.summarize(text=text, max_words=50)\n    edited = modifier.rationalize_text(text=text)\n    achievements = modifier.extract_achievements(text=text)\n    evaluation = modifier.evaluate_review_text(text=text)\n\n    assert all([summary, edited, achievements, evaluation])\n</code></pre>"},{"location":"metamorphosis/mcp/TextModifiers/#migration-guide","title":"Migration Guide","text":""},{"location":"metamorphosis/mcp/TextModifiers/#from-v0x-to-v10","title":"From v0.x to v1.0","text":"<ol> <li> <p>Update Imports:    <pre><code># Old\nfrom metamorphosis.text_utils import TextProcessor\n\n# New\nfrom metamorphosis.mcp.text_modifiers import TextModifiers\n</code></pre></p> </li> <li> <p>Update Method Names:    <pre><code># Old\nprocessor.copy_edit(text)\n\n# New\nmodifier.rationalize_text(text=text)\n</code></pre></p> </li> <li> <p>Update Return Types:    <pre><code># Old - returned plain strings/dicts\nresult = processor.summarize(text)\nsummary_text = result[\"summary\"]\n\n# New - returns Pydantic models\nresult = modifier.summarize(text=text)\nsummary_text = result.summarized_text\n</code></pre></p> </li> </ol>"},{"location":"metamorphosis/mcp/TextModifiers/#see-also","title":"See Also","text":"<ul> <li>MCP Package Overview - Package architecture and design</li> <li>Tools Server - MCP server implementation</li> <li>Data Models - Pydantic model definitions</li> <li>Model Registry - LLM client management</li> <li>Examples - Usage examples and tutorials</li> </ul> <p>This class documentation is automatically generated from the source code and maintained in sync with the implementation.</p>"},{"location":"metamorphosis/ui/","title":"Index","text":""},{"location":"metamorphosis/ui/#ui-package","title":"UI Package","text":"<p>This section documents the Streamlit UI entrypoint.</p> <p>Streamlit User Interface for Self-Review Processing</p> <p>This module provides a comprehensive web-based user interface for interacting with the self-reviewer agent workflow. It implements real-time streaming capabilities using Server-Sent Events (SSE) to provide live updates during text processing.</p> <p>Key Features: - Real-time streaming of LangGraph workflow execution - Interactive text input and processing controls - Live progress monitoring and result display - Debug information and event inspection - Session state management for conversation persistence - Responsive UI with dynamic content updates</p> <p>Architecture: - Streamlit frontend with session state management - SSE client for real-time communication with FastAPI backend - Event-driven UI updates during workflow execution - Robust error handling and connection management - Multi-column layout for optimal information display</p> <p>The interface serves as a monitoring and interaction tool for the LangGraph workflow, providing both user-friendly controls and detailed debugging capabilities for developers and power users.</p>"},{"location":"metamorphosis/ui/#metamorphosis.ui.streamlit_ui.count_visual_lines","title":"<code>count_visual_lines(text, chars_per_line=80)</code>","text":"<p>Approximate how many lines the text will take in the textarea, given an average chars_per_line before wrapping.</p> Source code in <code>src/metamorphosis/ui/streamlit_ui.py</code> <pre><code>def count_visual_lines(text: str, chars_per_line: int = 80) -&gt; int:\n    \"\"\"\n    Approximate how many lines the text will take in the textarea,\n    given an average chars_per_line before wrapping.\n    \"\"\"\n    if not text:\n        return 1\n    return sum(math.ceil(len(line) / chars_per_line) for line in text.split(\"\\n\"))\n</code></pre>"},{"location":"metamorphosis/ui/#metamorphosis.ui.streamlit_ui.create_html_achievements_table","title":"<code>create_html_achievements_table(achievements_list)</code>","text":"<p>Create an HTML table with proper text wrapping for achievements.</p> <p>Parameters:</p> Name Type Description Default <code>achievements_list</code> <code>AchievementsList</code> <p>The AchievementsList object containing extracted achievements.</p> required <p>Returns:</p> Type Description <code>str</code> <p>HTML string for the achievements table.</p> Source code in <code>src/metamorphosis/ui/streamlit_ui.py</code> <pre><code>def create_html_achievements_table(achievements_list: AchievementsList) -&gt; str:\n    \"\"\"\n    Create an HTML table with proper text wrapping for achievements.\n\n    Args:\n        achievements_list: The AchievementsList object containing extracted achievements.\n\n    Returns:\n        HTML string for the achievements table.\n    \"\"\"\n    # Start building the HTML table\n    html = \"\"\"\n    &lt;style&gt;\n    .achievements-table {\n        width: 100%;\n        border-collapse: collapse;\n        margin: 10px 0;\n        font-family: Arial, sans-serif;\n    }\n    .achievements-table th {\n        background-color: #f0f2f6;\n        color: #262730;\n        font-weight: bold;\n        padding: 12px 8px;\n        text-align: left;\n        border: 1px solid #e6e9ef;\n        white-space: nowrap;\n    }\n    .achievements-table td {\n        padding: 12px 8px;\n        border: 1px solid #e6e9ef;\n        vertical-align: top;\n        word-wrap: break-word;\n        word-break: break-word;\n        white-space: pre-wrap;\n        max-width: 200px;\n    }\n    .achievements-table tr:nth-child(even) {\n        background-color: #fafafa;\n    }\n    .achievements-table tr:hover {\n        background-color: #f0f2f6;\n    }\n    .title-cell {\n        font-weight: bold;\n        color: #1f77b4;\n    }\n    .impact-cell {\n        text-align: center;\n        font-weight: bold;\n    }\n    .metrics-cell {\n        font-style: italic;\n        color: #2ca02c;\n    }\n    .details-cell {\n        font-size: 0.9em;\n        color: #666;\n    }\n    &lt;/style&gt;\n\n    &lt;table class=\"achievements-table\"&gt;\n        &lt;thead&gt;\n            &lt;tr&gt;\n                &lt;th&gt;\ud83c\udfc6 Title&lt;/th&gt;\n                &lt;th&gt;\ud83d\udccb Outcome&lt;/th&gt;\n                &lt;th&gt;\ud83c\udfaf Impact Area&lt;/th&gt;\n                &lt;th&gt;\ud83d\udcca Metrics&lt;/th&gt;\n                &lt;th&gt;\u2139\ufe0f Details&lt;/th&gt;\n            &lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;\n    \"\"\"\n\n    # Add rows for each achievement\n    for i, achievement in enumerate(achievements_list.items, 1):\n        # Format metrics as a comma-separated string\n        metrics_text = \", \".join(achievement.metric_strings) if achievement.metric_strings else \"\u2014\"\n\n        # Format additional details (timeframe, scope, collaborators)\n        details_parts = []\n        if achievement.timeframe:\n            details_parts.append(f\"\u23f0 {achievement.timeframe}\")\n        if achievement.ownership_scope:\n            details_parts.append(f\"\ud83d\udc64 {achievement.ownership_scope}\")\n        if achievement.collaborators:\n            collabs = \", \".join(achievement.collaborators[:2])  # Show first 2 collaborators\n            if len(achievement.collaborators) &gt; 2:\n                collabs += f\" +{len(achievement.collaborators) - 2}\"\n            details_parts.append(f\"\ud83e\udd1d {collabs}\")\n\n        details_text = \"\\n\".join(details_parts) if details_parts else \"\u2014\"\n\n        # Color-code impact areas\n        impact_colors = {\n            \"reliability\": \"#ff6b6b\",\n            \"performance\": \"#4ecdc4\", \n            \"security\": \"#45b7d1\",\n            \"cost\": \"#96ceb4\",\n            \"revenue\": \"#feca57\",\n            \"customer\": \"#ff9ff3\",\n            \"delivery_speed\": \"#54a0ff\",\n            \"quality\": \"#5f27cd\",\n            \"compliance\": \"#00d2d3\",\n            \"team\": \"#ff9f43\",\n        }\n        impact_color = impact_colors.get(achievement.impact_area, \"#666\")\n\n        # Add the row\n        html += f\"\"\"\n            &lt;tr&gt;\n                &lt;td class=\"title-cell\"&gt;{i}. {achievement.title}&lt;/td&gt;\n                &lt;td&gt;{achievement.outcome}&lt;/td&gt;\n                &lt;td class=\"impact-cell\" style=\"color: {impact_color};\"&gt;{achievement.impact_area.replace('_', ' ').title()}&lt;/td&gt;\n                &lt;td class=\"metrics-cell\"&gt;{metrics_text}&lt;/td&gt;\n                &lt;td class=\"details-cell\"&gt;{details_text}&lt;/td&gt;\n            &lt;/tr&gt;\n        \"\"\"\n\n    # Close the table\n    html += \"\"\"\n        &lt;/tbody&gt;\n    &lt;/table&gt;\n    \"\"\"\n    return html\n</code></pre>"},{"location":"metamorphosis/ui/#metamorphosis.ui.streamlit_ui.create_html_metrics_table","title":"<code>create_html_metrics_table(review_scorecard)</code>","text":"<p>Create an HTML table with proper text wrapping for review metrics.</p> <p>Parameters:</p> Name Type Description Default <code>review_scorecard</code> <code>ReviewScorecard</code> <p>The ReviewScorecard object containing evaluation results.</p> required <p>Returns:</p> Type Description <code>str</code> <p>HTML string for the metrics table.</p> Source code in <code>src/metamorphosis/ui/streamlit_ui.py</code> <pre><code>def create_html_metrics_table(review_scorecard: ReviewScorecard) -&gt; str:\n    \"\"\"\n    Create an HTML table with proper text wrapping for review metrics.\n\n    Args:\n        review_scorecard: The ReviewScorecard object containing evaluation results.\n\n    Returns:\n        HTML string for the metrics table.\n    \"\"\"\n    # Define weights for display\n    weights = {\n        \"OutcomeOverActivity\": \"25%\",\n        \"QuantitativeSpecificity\": \"25%\",\n        \"ClarityCoherence\": \"15%\",\n        \"Conciseness\": \"15%\",\n        \"OwnershipLeadership\": \"10%\",\n        \"Collaboration\": \"10%\",\n    }\n\n    # Color coding based on score ranges\n    def get_score_color(score: int) -&gt; str:\n        if score &gt;= 85:\n            return \"#2ecc71\"  # bright green\n        elif score &gt;= 70:\n            return \"#27ae60\"  # green\n        elif score &gt;= 50:\n            return \"#f39c12\"  # yellow\n        else:\n            return \"#e74c3c\"  # red\n\n    # Start building the HTML table\n    html = \"\"\"\n    &lt;style&gt;\n    .metrics-table {\n        width: 100%;\n        border-collapse: collapse;\n        margin: 10px 0;\n        font-family: Arial, sans-serif;\n    }\n    .metrics-table th {\n        background-color: #f0f2f6;\n        color: #262730;\n        font-weight: bold;\n        padding: 12px 8px;\n        text-align: left;\n        border: 1px solid #e6e9ef;\n        white-space: nowrap;\n    }\n    .metrics-table td {\n        padding: 12px 8px;\n        border: 1px solid #e6e9ef;\n        vertical-align: top;\n        word-wrap: break-word;\n        word-break: break-word;\n        white-space: pre-wrap;\n        max-width: 200px;\n    }\n    .metrics-table tr:nth-child(even) {\n        background-color: #fafafa;\n    }\n    .metrics-table tr:hover {\n        background-color: #f0f2f6;\n    }\n    .metric-name-cell {\n        font-weight: bold;\n        color: #1f77b4;\n        width: 20%;\n    }\n    .score-cell {\n        text-align: center;\n        font-weight: bold;\n        width: 10%;\n    }\n    .rationale-cell {\n        width: 35%;\n    }\n    .suggestion-cell {\n        font-style: italic;\n        color: #2ca02c;\n        width: 35%;\n    }\n    &lt;/style&gt;\n\n    &lt;table class=\"metrics-table\"&gt;\n        &lt;thead&gt;\n            &lt;tr&gt;\n                &lt;th&gt;\ud83d\udcca Metric&lt;/th&gt;\n                &lt;th&gt;\ud83c\udfaf Score&lt;/th&gt;\n                &lt;th&gt;\ud83d\udcad Rationale&lt;/th&gt;\n                &lt;th&gt;\ud83d\udca1 Suggestion&lt;/th&gt;\n            &lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;\n    \"\"\"\n\n    # Add rows for each metric\n    for metric in review_scorecard.metrics:\n        weight = weights.get(metric.name, \"\")\n        metric_name = f\"{metric.name}\\n({weight})\"\n\n        score_color = get_score_color(metric.score)\n\n        # Add the row\n        html += f\"\"\"\n            &lt;tr&gt;\n                &lt;td class=\"metric-name-cell\"&gt;{metric_name}&lt;/td&gt;\n                &lt;td class=\"score-cell\" style=\"color: {score_color};\"&gt;{metric.score}/100&lt;/td&gt;\n                &lt;td class=\"rationale-cell\"&gt;{metric.rationale}&lt;/td&gt;\n                &lt;td class=\"suggestion-cell\"&gt;{metric.suggestion}&lt;/td&gt;\n            &lt;/tr&gt;\n        \"\"\"\n\n    # Close the table\n    html += \"\"\"\n        &lt;/tbody&gt;\n    &lt;/table&gt;\n    \"\"\"\n\n    return html\n</code></pre>"},{"location":"metamorphosis/ui/#metamorphosis.ui.streamlit_ui.display_achievements_table","title":"<code>display_achievements_table(achievements_list)</code>","text":"<p>Display achievements using HTML table with proper text wrapping.</p> <p>Parameters:</p> Name Type Description Default <code>achievements_list</code> <code>AchievementsList</code> <p>The AchievementsList object containing extracted achievements.</p> required Source code in <code>src/metamorphosis/ui/streamlit_ui.py</code> <pre><code>def display_achievements_table(achievements_list: AchievementsList):\n    \"\"\"\n    Display achievements using HTML table with proper text wrapping.\n\n    Args:\n        achievements_list: The AchievementsList object containing extracted achievements.\n    \"\"\"\n    # Create header with summary information\n    st.markdown(f\"\"\"\n    ### \ud83c\udfc6 Extracted Key Achievements\n    **{len(achievements_list.items)} items** \u2022 **~{achievements_list.size} tokens**\n    \"\"\")\n\n    # Create HTML table with proper text wrapping\n    html_table = create_html_achievements_table(achievements_list)\n    html_doc = f\"\"\"\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n        &lt;meta charset=\"utf-8\" /&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        {html_table}\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n\n    st.components.v1.html(html_doc, height=500, scrolling=True)\n</code></pre>"},{"location":"metamorphosis/ui/#metamorphosis.ui.streamlit_ui.display_metrics_table","title":"<code>display_metrics_table(review_scorecard)</code>","text":"<p>Display metrics using HTML table with proper text wrapping.</p> <p>Parameters:</p> Name Type Description Default <code>review_scorecard</code> <code>ReviewScorecard</code> <p>The ReviewScorecard object containing evaluation results.</p> required Source code in <code>src/metamorphosis/ui/streamlit_ui.py</code> <pre><code>def display_metrics_table(review_scorecard: ReviewScorecard):\n    \"\"\"\n    Display metrics using HTML table with proper text wrapping.\n\n    Args:\n        review_scorecard: The ReviewScorecard object containing evaluation results.\n    \"\"\"\n    # Create header with summary information\n    st.markdown(f\"\"\"\n    ### \ud83d\udcca Review Quality Evaluation\n    **Overall Score: {review_scorecard.overall}/100** \u2022 **Verdict: {review_scorecard.verdict.title()}**\n    \"\"\")\n\n    # Create HTML table with proper text wrapping\n    html_table = create_html_metrics_table(review_scorecard)\n    html_doc = f\"\"\"\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n        &lt;meta charset=\"utf-8\" /&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        {html_table}\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n\n    st.components.v1.html(html_doc, height=500, scrolling=True)\n</code></pre>"},{"location":"metamorphosis/ui/#metamorphosis.ui.streamlit_ui.extract_values_from_event","title":"<code>extract_values_from_event(ev)</code>","text":"<p>Extracts the actual state values from various LangGraph event formats.</p> <p>LangGraph events can have different structures depending on the configuration, streaming mode, and server implementation. This function provides robust handling for multiple event formats to ensure compatibility across different setups.</p> <p>The function implements a hierarchical checking strategy that looks for state data in common locations, falling back to pattern matching for custom formats. This approach ensures maximum compatibility while maintaining performance.</p> <p>Supported Event Formats: - Standard LangGraph: {\"values\": {...}} or {\"data\": {\"values\": {...}}} - State wrapper: {\"state\": {...}} or {\"data\": {\"state\": {...}}} - Direct format: {\"original_text\": \"...\", \"copy_edited_text\": \"...\", ...} - Custom wrappers: Various nested structures</p> <p>Parameters:</p> Name Type Description Default <code>ev</code> <code>Dict[str, Any]</code> <p>Raw event dictionary from the SSE stream</p> required <p>Returns:</p> Type Description <code>Dict[str, Any] | None</code> <p>Dict[str, Any] | None: Extracted state dictionary containing workflow data, or None if no valid state structure is found</p> Note <p>The function checks multiple common patterns to be robust against different LangGraph event formats and server configurations. It uses set intersection for efficient key matching and type checking for safe dictionary access.</p> Source code in <code>src/metamorphosis/ui/streamlit_ui.py</code> <pre><code>def extract_values_from_event(ev: Dict[str, Any]) -&gt; Dict[str, Any] | None:\n    \"\"\"\n    Extracts the actual state values from various LangGraph event formats.\n\n    LangGraph events can have different structures depending on the configuration,\n    streaming mode, and server implementation. This function provides robust handling\n    for multiple event formats to ensure compatibility across different setups.\n\n    The function implements a hierarchical checking strategy that looks for state\n    data in common locations, falling back to pattern matching for custom formats.\n    This approach ensures maximum compatibility while maintaining performance.\n\n    Supported Event Formats:\n    - Standard LangGraph: {\"values\": {...}} or {\"data\": {\"values\": {...}}}\n    - State wrapper: {\"state\": {...}} or {\"data\": {\"state\": {...}}}\n    - Direct format: {\"original_text\": \"...\", \"copy_edited_text\": \"...\", ...}\n    - Custom wrappers: Various nested structures\n\n    Args:\n        ev (Dict[str, Any]): Raw event dictionary from the SSE stream\n\n    Returns:\n        Dict[str, Any] | None: Extracted state dictionary containing workflow data,\n            or None if no valid state structure is found\n\n    Note:\n        The function checks multiple common patterns to be robust against\n        different LangGraph event formats and server configurations. It uses\n        set intersection for efficient key matching and type checking for\n        safe dictionary access.\n    \"\"\"\n    # Validate input is a dictionary to prevent type errors\n    if not isinstance(ev, dict):\n        return None\n\n    # Pattern A: Standard LangGraph wrapper formats\n    # These are the most common formats used by LangGraph in different modes\n\n    # Check if state is wrapped in \"values\" field (common in \"values\" mode)\n    if isinstance(ev.get(\"values\"), dict):\n        return ev[\"values\"]\n    # Check if state is wrapped in \"data.values\" nested structure (some server configs)\n    if isinstance(ev.get(\"data\"), dict) and isinstance(ev[\"data\"].get(\"values\"), dict):\n        return ev[\"data\"][\"values\"]\n    # Check if state is wrapped in \"state\" field (alternative naming)\n    if isinstance(ev.get(\"state\"), dict):\n        return ev[\"state\"]\n    # Check if state is wrapped in \"data.state\" nested structure (nested alternative)\n    if isinstance(ev.get(\"data\"), dict) and isinstance(ev[\"data\"].get(\"state\"), dict):\n        return ev[\"data\"][\"state\"]\n\n    # Pattern B: Custom server format - state is at TOP LEVEL\n    # This handles cases where the server sends the state directly without wrapping\n    # Define expected keys that indicate this is a GraphState object\n    expected_keys = {\"original_text\", \n    \"copy_edited_text\", \n    \"summary\", \n    \"word_cloud_path\", \n    \"achievements\", \n    \"review_scorecard\", \n    \"review_complete\"}\n    # If any of these expected keys exist, treat the whole event as the current state\n    # Using set intersection for efficient key checking\n    if expected_keys.intersection(ev.keys()):\n        return ev  # treat the whole event as the current state\n\n    # No valid state found - return None to indicate no state data in this event\n        return None\n</code></pre>"},{"location":"metamorphosis/ui/#metamorphosis.ui.streamlit_ui.patch_state","title":"<code>patch_state(dst, delta)</code>","text":"<p>Performs a shallow merge of two dictionaries for 'updates' mode.</p> <p>This function is used when the server sends delta updates instead of full state snapshots. It merges the delta changes into the existing destination dictionary, creating a new dictionary to avoid mutating the original. This is essential for handling incremental updates from the LangGraph streaming mode.</p> <p>The function handles None inputs gracefully and ensures that the destination dictionary is never modified in place, which is important for Streamlit's session state management.</p> <p>Parameters:</p> Name Type Description Default <code>dst</code> <code>Dict[str, Any]</code> <p>Destination dictionary to merge into (will be copied to avoid mutation)</p> required <code>delta</code> <code>Dict[str, Any]</code> <p>Dictionary containing updates to apply</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: New dictionary with merged state</p> Example <p>patch_state({\"a\": 1, \"b\": 2}, {\"b\": 3, \"c\": 4}) {\"a\": 1, \"b\": 3, \"c\": 4}</p> Note <ul> <li>Both dst and delta can be None, which will be handled gracefully</li> <li>The function creates a shallow copy, so nested dictionaries are not deep-copied</li> <li>This is intentional for performance and matches the expected behavior for state updates</li> </ul> Source code in <code>src/metamorphosis/ui/streamlit_ui.py</code> <pre><code>def patch_state(dst: Dict[str, Any], delta: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Performs a shallow merge of two dictionaries for 'updates' mode.\n\n    This function is used when the server sends delta updates instead of full state snapshots.\n    It merges the delta changes into the existing destination dictionary, creating a new\n    dictionary to avoid mutating the original. This is essential for handling incremental\n    updates from the LangGraph streaming mode.\n\n    The function handles None inputs gracefully and ensures that the destination dictionary\n    is never modified in place, which is important for Streamlit's session state management.\n\n    Args:\n        dst (Dict[str, Any]): Destination dictionary to merge into (will be copied to avoid mutation)\n        delta (Dict[str, Any]): Dictionary containing updates to apply\n\n    Returns:\n        Dict[str, Any]: New dictionary with merged state\n\n    Example:\n        &gt;&gt;&gt; patch_state({\"a\": 1, \"b\": 2}, {\"b\": 3, \"c\": 4})\n        {\"a\": 1, \"b\": 3, \"c\": 4}\n\n    Note:\n        - Both dst and delta can be None, which will be handled gracefully\n        - The function creates a shallow copy, so nested dictionaries are not deep-copied\n        - This is intentional for performance and matches the expected behavior for state updates\n    \"\"\"\n    # Create a copy to avoid mutating the original destination\n    # Handle None case by providing empty dict as default\n    dst = dict(dst or {})\n    # Apply each key-value pair from the delta\n    # Handle None delta case by providing empty dict as default\n    for k, v in (delta or {}).items():\n        dst[k] = v\n    return dst\n</code></pre>"},{"location":"metamorphosis/ui/#metamorphosis.ui.streamlit_ui.populate_tabs","title":"<code>populate_tabs(tabs, graph_completed, current, review_validation_container)</code>","text":"<p>Populate the tabbed interface with content based on graph execution status.</p> <p>This function handles the content for all tabs, showing appropriate content based on whether the graph execution has completed and what data is available.</p> <p>Parameters:</p> Name Type Description Default <code>tabs</code> <p>The Streamlit tabs object</p> required <code>graph_completed</code> <code>bool</code> <p>Whether the graph execution has completed</p> required <code>current</code> <code>dict</code> <p>Current session state data</p> required <code>review_validation_container</code> <p>Container for review validation messages</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The review text from the first tab</p> Source code in <code>src/metamorphosis/ui/streamlit_ui.py</code> <pre><code>def populate_tabs(tabs, graph_completed: bool, current: dict, review_validation_container) -&gt; str:\n    \"\"\"\n    Populate the tabbed interface with content based on graph execution status.\n\n    This function handles the content for all tabs, showing appropriate content\n    based on whether the graph execution has completed and what data is available.\n\n    Args:\n        tabs: The Streamlit tabs object\n        graph_completed (bool): Whether the graph execution has completed\n        current (dict): Current session state data\n        review_validation_container: Container for review validation messages\n\n    Returns:\n        str: The review text from the first tab\n    \"\"\"\n    # =============================================================================\n    # TAB 1: REVIEW TEXT INPUT\n    # =============================================================================\n    with tabs[0]:\n        #Create radio button for mode selection\n        mode = st.radio(\n            \"Choose mode:\",\n            [\"\ud83d\udcdd Edit\", \"\ud83d\udc41\ufe0f View\"],\n            horizontal=True\n        )\n        review_text = st.session_state.current_review_text\n        if mode == \"\ud83d\udcdd Edit\":           \n            st.subheader(\"\ud83d\udcdd Enter Your Review Text in Markdown Format\")\n            review_text = st_ace(\n                value=st.session_state.current_review_text,\n                language='markdown',\n                theme='monokai',\n                key='markdown_editor',\n                height=min(max(100, count_visual_lines(st.session_state.current_review_text) * 20 + 60), 800),\n                auto_update=True,\n                wrap=True,\n                font_size=14\n            )\n        else:\n            st.subheader(\"\ud83d\udcdd Review Text Preview\")\n            st.markdown(st.session_state.current_review_text, unsafe_allow_html=True)\n\n        # Validate input and show feedback\n        is_valid, validation_message = validate_review_text(review_text)\n        if not is_valid:\n            review_validation_container.warning(f\"\u26a0\ufe0f {validation_message}\")\n\n        # Handle review text changes - automatic state management\n        # When content changes, we need to reset the session to prevent mixing old and new data\n        if review_text != st.session_state.current_review_text:\n            st.session_state.current_review_text = review_text\n            # Generate new thread ID for clean separation between different content\n            st.session_state.thread_id = str(uuid.uuid4())\n            # Clear previous state to prevent data contamination from old content\n            st.session_state.state = {}\n            # Clear previous events for clean debugging of new content\n            st.session_state.events = []\n            # Clear previous progress steps to prevent accumulation of old steps\n            st.session_state.progress_steps = []\n\n    # =============================================================================\n    # TAB 2: COPY-EDITED TEXT\n    # =============================================================================\n    with tabs[1]:\n        if graph_completed and current.get(\"copy_edited_text\"):\n            st.subheader(\"\ud83d\udcdd Rationalized Text\")\n            st.markdown(current[\"copy_edited_text\"], unsafe_allow_html=True)\n        else:\n            st.info(\"\u23f3 Copy-edited text will appear here after graph execution completes.\")\n            if not graph_completed:\n                st.caption(\"\ud83d\udca1 Complete the review text input and click 'Start &amp; Stream' to begin processing.\")\n\n    # =============================================================================\n    # TAB 3: SUMMARY\n    # =============================================================================\n    with tabs[2]:\n        if graph_completed and current.get(\"summary\"):\n            st.subheader(\"\ud83d\udccb Summary\")\n            st.markdown(safe_markdown(current[\"summary\"]), unsafe_allow_html=True)\n        else:\n            st.info(\"\u23f3 Summary will appear here after graph execution completes.\")\n            if not graph_completed:\n                st.caption(\"\ud83d\udca1 Complete the review text input and click 'Start &amp; Stream' to begin processing.\")\n\n    # =============================================================================\n    # TAB 4: WORD CLOUD\n    # =============================================================================\n    with tabs[3]:\n        if graph_completed and current.get(\"word_cloud_path\"):\n            st.subheader(\"\ud83d\uddbc\ufe0f Word Cloud\")\n            try:\n                import os\n                if os.path.exists(current[\"word_cloud_path\"]):\n                    st.image(\n                        current[\"word_cloud_path\"],\n                        caption=\"Final Generated Word Cloud\",\n                        width='stretch',\n                    )\n                else:\n                    st.warning(f\"\u26a0\ufe0f Word cloud image not found at final path: {current['word_cloud_path']}\")\n            except Exception as e:\n                st.error(f\"\u274c Error displaying final word cloud: {e}\")\n        else:\n            st.info(\"\u23f3 Word cloud will appear here after graph execution completes.\")\n            if not graph_completed:\n                st.caption(\"\ud83d\udca1 Complete the review text input and click 'Start &amp; Stream' to begin processing.\")\n\n    # =============================================================================\n    # TAB 5: ACHIEVEMENTS\n    # =============================================================================\n    with tabs[4]:\n        if graph_completed and current.get(\"achievements\"):\n            st.subheader(\"\ud83c\udfc6 Achievements\")\n            try:\n                achievements_data = current[\"achievements\"]\n                achievements = None\n\n                # Handle both dict and string representations of achievements\n                if isinstance(achievements_data, dict):\n                    achievements = AchievementsList(**achievements_data)\n                else:\n                    st.write(\"\u26a0\ufe0f Achievements data not parse-able\")\n                    st.write(achievements_data)\n\n                # Only display the achievements if we successfully parsed them\n                if achievements is not None:\n                    # Render the summary panel as HTML\n                    summary_panel = create_summary_panel(achievements)\n                    render_rich(summary_panel)\n\n                    display_achievements_table(achievements)\n\n            except Exception as e:\n                st.error(f\"\u274c Error displaying final achievements: {e}\")\n        else:\n            st.info(\"\u23f3 Achievements will appear here after graph execution completes.\")\n            if not graph_completed:\n                st.caption(\"\ud83d\udca1 Complete the review text input and click 'Start &amp; Stream' to begin processing.\")\n\n    # =============================================================================\n    # TAB 6: REVIEW SCORECARD\n    # =============================================================================\n    with tabs[5]:\n        if graph_completed and current.get(\"review_scorecard\"):\n            st.subheader(\"\ud83d\udcca Review Scorecard\")\n            try:\n                review_scorecard_data = current[\"review_scorecard\"]\n                review_scorecard = None\n\n                # Handle both dict and string representations of review scorecard\n                if isinstance(review_scorecard_data, dict):\n                    review_scorecard = ReviewScorecard(**review_scorecard_data)\n                else:\n                    st.write(\"\u26a0\ufe0f Review scorecard data not parse-able\")\n                    st.write(review_scorecard_data)\n\n                # Only display the review scorecard if we successfully parsed it\n                if review_scorecard is not None:\n                    # Render the radar plot (this should be a Plotly figure)\n                    st.plotly_chart(create_radar_plot(review_scorecard.model_dump()))\n\n                    # Render the radar chart info as HTML\n                    radar_info = create_radar_chart_info(review_scorecard)\n                    render_rich(radar_info)\n\n                    # Render the evaluation summary panel as HTML\n                    eval_summary_panel = create_summary_panel_evaluation(review_scorecard)\n                    render_rich(eval_summary_panel)\n\n                    # Display the metrics table using HTML table with text wrapping\n                    display_metrics_table(review_scorecard)\n\n            except Exception as e:\n                st.error(f\"\u274c Error displaying final review scorecard: {e}\")\n        else:\n            st.info(\"\u23f3 Review scorecard will appear here after graph execution completes.\")\n            if not graph_completed:\n                st.caption(\"\ud83d\udca1 Complete the review text input and click 'Start &amp; Stream' to begin processing.\")\n\n    return review_text\n</code></pre>"},{"location":"metamorphosis/ui/#metamorphosis.ui.streamlit_ui.render_rich","title":"<code>render_rich(rich_renderable, *, char_width=100, line_height_px=20, padding_px=24, min_height=120, max_height=800, scrolling=True)</code>","text":"<p>Render a Rich Panel/Table (or any renderable) into Streamlit and auto-pick a height.</p> <p>Strategy: render to text to count lines -&gt; derive pixel height -&gt; render HTML at that height.</p> Source code in <code>src/metamorphosis/ui/streamlit_ui.py</code> <pre><code>def render_rich(\n    rich_renderable: object,\n    *,\n    char_width: int = 100,      # approximate characters per line (affects wrapping)\n    line_height_px: int = 20,   # monospace-ish line height\n    padding_px: int = 24,       # top+bottom padding\n    min_height: int = 120,\n    max_height: int = 800,\n    scrolling: bool = True,    \n):\n    \"\"\"\n    Render a Rich Panel/Table (or any renderable) into Streamlit and auto-pick a height.\n\n    Strategy: render to text to count lines -&gt; derive pixel height -&gt; render HTML at that height.\n    \"\"\"\n    # 1) Render once to measure (text)\n    measure_console = Console(record=True, width=char_width)\n    measure_console.print(rich_renderable)\n    text = measure_console.export_text(clear=False)\n\n    # 2) Count lines (includes wrapped lines because of width=char_width)\n    line_count = text.count(\"\\n\") + 1\n\n    # 3) Convert to pixels (very close for monospace; tweak constants to taste)\n    measured_height = line_count * line_height_px + padding_px\n    height = max(min_height, min(int(measured_height), max_height))\n\n    # 4) Export HTML from the same buffer so the look matches the measurement\n    html = measure_console.export_html(inline_styles=True)\n\n    # 5) Embed\n    st.components.v1.html(html, height=height, scrolling=scrolling)\n</code></pre>"},{"location":"metamorphosis/ui/#metamorphosis.ui.streamlit_ui.safe_markdown","title":"<code>safe_markdown(text)</code>","text":"<p>Replace $ with $, so that the markdown rendering is not broken. This is a workaround to avoid the markdown rendering breaking when $ is present in the text.</p> Source code in <code>src/metamorphosis/ui/streamlit_ui.py</code> <pre><code>def safe_markdown(text: str):\n    \"\"\"\n    Replace $ with \\\\$, so that the markdown rendering is not broken.\n    This is a workaround to avoid the markdown rendering breaking when $ is present in the text.\n    \"\"\"\n    return text.replace(\"$\", \"\\\\$\")\n</code></pre>"},{"location":"metamorphosis/ui/#metamorphosis.ui.streamlit_ui.sse_events","title":"<code>sse_events(url, data)</code>","text":"<p>Minimal Server-Sent Events (SSE) client using the requests library.</p> <p>This function establishes an HTTP connection to the server and yields decoded JSON payloads from lines that start with 'data:'. It's the core function that drives the server-side LangGraph execution because the /stream endpoint calls graph.astream(...).</p> <p>The function implements a robust SSE client that handles various edge cases and provides real-time streaming of LangGraph workflow execution. It's designed to be resilient to network issues and malformed data while maintaining a stable connection.</p> The server sends data in the format: <p>data: {\"message\": \"Hello\"} data: {\"message\": \"World\"}</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The SSE endpoint URL to connect to (typically /stream endpoint)</p> required <code>data</code> <code>Dict[str, Any]</code> <p>Data to send with the request containing: - thread_id: Unique conversation identifier - review_text: The text to process through the workflow - mode: Streaming mode (\"values\" or \"updates\")</p> required <p>Yields:</p> Type Description <p>Dict[str, Any]: Parsed JSON objects from the SSE stream representing workflow events</p> <p>Raises:</p> Type Description <code>RequestException</code> <p>For HTTP errors, connection issues, or timeouts</p> <code>JSONDecodeError</code> <p>For malformed JSON in the stream (handled gracefully)</p> Note <ul> <li>Uses POST request to handle large review text data in the request body</li> <li>Uses decode_unicode=False to get raw bytes for proper SSE parsing</li> <li>Handles malformed lines gracefully by catching exceptions and continuing</li> <li>Times out after 600 seconds to prevent hanging connections</li> <li>Maintains connection stability by ignoring bad lines rather than failing</li> <li>The generator pattern allows for memory-efficient streaming of large datasets</li> </ul> Source code in <code>src/metamorphosis/ui/streamlit_ui.py</code> <pre><code>def sse_events(url: str, data: Dict[str, Any]):\n    \"\"\"\n    Minimal Server-Sent Events (SSE) client using the requests library.\n\n    This function establishes an HTTP connection to the server and yields decoded JSON payloads\n    from lines that start with 'data:'. It's the core function that drives the server-side\n    LangGraph execution because the /stream endpoint calls graph.astream(...).\n\n    The function implements a robust SSE client that handles various edge cases and provides\n    real-time streaming of LangGraph workflow execution. It's designed to be resilient to\n    network issues and malformed data while maintaining a stable connection.\n\n    SSE Format: The server sends data in the format:\n        data: {\"message\": \"Hello\"}\n        data: {\"message\": \"World\"}\n\n    Args:\n        url (str): The SSE endpoint URL to connect to (typically /stream endpoint)\n        data (Dict[str, Any]): Data to send with the request containing:\n            - thread_id: Unique conversation identifier\n            - review_text: The text to process through the workflow\n            - mode: Streaming mode (\"values\" or \"updates\")\n\n    Yields:\n        Dict[str, Any]: Parsed JSON objects from the SSE stream representing workflow events\n\n    Raises:\n        requests.RequestException: For HTTP errors, connection issues, or timeouts\n        json.JSONDecodeError: For malformed JSON in the stream (handled gracefully)\n\n    Note:\n        - Uses POST request to handle large review text data in the request body\n        - Uses decode_unicode=False to get raw bytes for proper SSE parsing\n        - Handles malformed lines gracefully by catching exceptions and continuing\n        - Times out after 600 seconds to prevent hanging connections\n        - Maintains connection stability by ignoring bad lines rather than failing\n        - The generator pattern allows for memory-efficient streaming of large datasets\n    \"\"\"\n    # Establish streaming HTTP connection with timeout using POST\n    # POST is used instead of GET to handle large review text data in the request body\n    with requests.post(url, json=data, stream=True, timeout=600) as resp:\n        # Raise exception for HTTP error status codes (4xx, 5xx)\n        # This ensures we fail fast on server errors rather than processing error responses\n        resp.raise_for_status()\n\n        # Iterate through response lines as raw bytes (not decoded to unicode)\n        # This is crucial for proper SSE parsing as we need to handle the \"data:\" prefix\n        for raw in resp.iter_lines(decode_unicode=False):\n            # Skip empty lines and None values (SSE event boundaries)\n            # SSE uses blank lines to separate events, so we ignore them\n            if raw is None or not raw:\n                # SSE event boundary (blank line) \u2014 ignore\n                continue\n\n            # Check if line starts with \"data:\" prefix (standard SSE format)\n            # Only process lines that contain actual data payloads\n            if raw.startswith(b\"data:\"):\n                try:\n                    # Extract payload: remove \"data:\" prefix, strip whitespace, decode to UTF-8\n                    # The prefix is exactly 5 bytes (\"data:\"), so we slice from index 5\n                    payload = raw[len(b\"data:\") :].strip().decode(\"utf-8\")\n                    if payload:\n                        # Parse JSON and yield the resulting object\n                        # This converts the string payload into a Python dictionary\n                        yield json.loads(payload)\n                except Exception:\n                    # Ignore malformed lines; keep streaming to maintain connection\n                    # This prevents one bad line from breaking the entire stream\n                    # Common issues: invalid JSON, encoding problems, truncated data\n                    pass\n</code></pre>"},{"location":"metamorphosis/ui/#metamorphosis.ui.streamlit_ui.validate_review_text","title":"<code>validate_review_text(text)</code>","text":"<p>Simple validation for review text input.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The review text to validate</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[bool, str]</code> <p>(is_valid, error_message)</p> Source code in <code>src/metamorphosis/ui/streamlit_ui.py</code> <pre><code>def validate_review_text(text: str) -&gt; tuple[bool, str]:\n    \"\"\"\n    Simple validation for review text input.\n\n    Args:\n        text: The review text to validate\n\n    Returns:\n        tuple: (is_valid, error_message)\n    \"\"\"\n    if not text or not text.strip():\n        return False, \"Please enter some review text before starting.\"\n\n    if len(text.strip()) &lt; 10:\n        return False, \"Review text should be at least 10 characters long.\"\n\n    if len(text) &gt; 10000:\n        return False, \"Review text is too long (max 10,000 characters).\"\n\n    return True, \"\"\n</code></pre>"},{"location":"notebooks/supportvectors-common/","title":"Common Patterns","text":"<pre><code>%%html\n\n&lt;!-- Many of the styles here are inspired by: \n    https://towardsdatascience.com/10-practical-tips-you-need-to-know-to-personalize-jupyter-notebook-fbd202777e20 \n\n\n    On the author's local machine, these exist in the custom.css file. However, in order to keep uniform look and feel, \n    and at the request of participants, I have added it to this common import-file here.\n\n    --&gt;\n&lt;link href=\"https://fonts.googleapis.com/css?family=Lora:400,700|Montserrat:300\" rel=\"stylesheet\"/&gt;\n&lt;link href=\"https://fonts.googleapis.com/css2?family=Crimson+Pro&amp;amp;family=Literata&amp;amp;display=swap\" rel=\"stylesheet\"/&gt;\n&lt;style&gt;\n\n\n#ipython_notebook::before{\n content:\"Neural Architectures\";\n        color: white;\n        font-weight: bold;\n        text-transform: uppercase;\n        font-family: 'Lora',serif;\n        font-size:16pt;\n        margin-bottom:15px;\n        margin-top:15px;\n\n}\nbody &gt; #header {\n    #background: #D15555;\n    background: linear-gradient(to bottom, indianred 0%, #fff 100%);\n    opacity: 0.8;\n\n}\n\n\n.navbar-default .navbar-nav &gt; li &gt; a, #kernel_indicator {\n    color: white;\n    transition: all 0.25s;\n    font-size:10pt;\n    font-family: sans-serif;\n    font-weight:normal;\n}\n.navbar-default {\n    padding-left:100px;\n    background: none;\n    border: none;\n}\n\n\nbody &gt; menubar-container {\n    background-color: wheat;\n}\n#ipython_notebook img{                                                                                        \n    display:block; \n\n    background: url(\"https://www.supportvectors.com/wp-content/uploads/2016/03/logo-poster-smaller.png\") no-repeat;\n    background-size: contain;\n\n    padding-left: 600px;\n    padding-right: 100px;\n\n    -moz-box-sizing: border-box;\n    box-sizing: border-box;\n}\n\n\n\nbody {\n #font-family:  'Literata', serif;\n    font-family:'Lora', san-serif;\n    text-align: justify;\n    font-weight: 400;\n    font-size: 12pt;\n}\n\niframe{\n    width:100%;\n    min-height:600px;\n}\n\nh1, h2, h3, h4, h5, h6 {\n# font-family: 'Montserrat', sans-serif;\n font-family:'Lora', serif;\n font-weight: 200;\n text-transform: uppercase;\n color: #EC7063 ;\n}\n\nh2 {\n    color: #000080;\n}\n\n.checkpoint_status, .autosave_status {\n    color:wheat;\n}\n\n#notebook_name {\n    font-weight: 600;\n    font-size:20pt;\n    text-variant:uppercase;\n    color: wheat; \n    margin-right:20px;\n    margin-left:-500px;\n}\n#notebook_name:hover {\nbackground-color: salmon;\n}\n\n\n.dataframe { /* dataframe atau table */\n    background: white;\n    box-shadow: 0px 1px 2px #bbb;\n}\n.dataframe thead th, .dataframe tbody td {\n    text-align: center;\n    padding: 1em;\n}\n\n.checkpoint_status, .autosave_status {\n    color:wheat;\n}\n\n.output {\n    align-items: center;\n}\n\ndiv.cell {\n    transition: all 0.25s;\n    border: none;\n    position: relative;\n    top: 0;\n}\ndiv.cell.selected, div.cell.selected.jupyter-soft-selected {\n    border: none;\n    background: transparent;\n    box-shadow: 0 6px 18px #aaa;\n    z-index: 10;\n    top: -10px;\n}\n.CodeMirror pre, .CodeMirror-dialog, .CodeMirror-dialog .CodeMirror-search-field, .terminal-app .terminal {\n    font-family: 'Hack' , serif; \n    font-weight: 500;\n    font-size: 14pt;\n}\n\n\n\n&lt;/style&gt;    \n</code></pre> <pre><code># Starting with the standard imports\nimport numpy as np\nimport pandas as pd\n\n\n\n# Preprocessing data\nfrom sklearn.model_selection import train_test_split     # data-splitter\nfrom sklearn.preprocessing import StandardScaler         # data-normalization\nfrom sklearn.preprocessing import PolynomialFeatures     # for polynomials\nfrom sklearn.preprocessing import PowerTransformer       # for power-transformations\nfrom sklearn.pipeline import make_pipeline               # for pipelines\nnp.random.seed (42)                                      # for reproducible results\n\n#\n# Modeling and Metrics\n# \n# --For Regressor\nfrom sklearn.dummy import DummyRegressor                 # baseline regressor (null-hypothesis)\nfrom sklearn.linear_model import LinearRegression        # linear regression\nfrom sklearn.linear_model import ( Ridge, \n                                  Lasso, \n                                  ElasticNet,\n                                 RidgeCV, \n                                 LassoCV,\n                                 ElasticNetCV)           # regularized regressions with CV\nfrom sklearn.metrics import mean_squared_error, r2_score # model-metrics\nfrom sklearn.ensemble import RandomForestRegressor\n\n#\n# For Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.dummy import (DummyClassifier, DummyRegressor)\n\n#\n# For clusterers\nfrom scipy import stats, integrate\nimport sklearn.cluster as cluster\nfrom sklearn.cluster import (DBSCAN, KMeans)\nfrom sklearn import metrics\nfrom sklearn.datasets import make_blobs\n\n# Yellowbrick\n#from yellowbrick.features import FeatureImportances\n#from yellowbrick.classifier import ConfusionMatrix, ClassificationReport, ROCAUC\n\n\nfrom matplotlib import colors\nimport seaborn as sns\nlist_of_cmaps=['Blues','Greens','Reds','Purples']   # some colors to use\n\n# Now the Graphical libraries imports and settings\n%matplotlib inline\nimport matplotlib.pyplot as plt                          # for plotting\nimport seaborn as sns                                    # nicer looking plots\nimport altair as alt                                     # for interactive plots\nimport plotly.express as px                              # for plotly express\nfrom matplotlib import colors                            # for web-color specs\npd.set_option('plotting.backend', 'matplotlib')          # pandas_bokeh, plotly, etc       \nplt.rcParams[ 'figure.figsize' ] = '20,10'               # landscape format figures\nplt.rcParams[ 'legend.fontsize' ] = 13                   # legend font size\nplt.rcParams[ 'axes.labelsize' ] = 13                    # axis label font size\nplt.rcParams['figure.dpi'] = 144                         # high-dpi monitors support\nplt.style.use ('ggplot')                                 # emulate ggplot style\n\n# For latex-quality, i.e., publication quality legends and labels on graphs.\n# Warning: you must have installed LaTeX on your system.\nfrom matplotlib import rc\nrc ('font', family='serif')\nrc ('text', usetex=False) # Enable it selectively \nrc ('font', size=16)\n\n# For youtube video display\nfrom IPython.display import YouTubeVideo\n\nimport warnings\nwarnings.filterwarnings ('ignore')  # suppress warning\n</code></pre> <pre><code>%%capture\n# The automated EDA tools\n#import dataprep \n#import ydata_profiling # replacement for pandas_profiling\n#import pandas_profiling # to be deprecated\n#import autoviz \n#import dtale\n#import sweetviz\n</code></pre> <pre><code>def sv_table_styles():\n\n    th_props = [\n      ('font-size', '11pt'),\n        ('font-family', 'sans'),\n      ('text-align', 'center'),\n      ('font-weight', '300'),\n      ('color', 'cornsilk'),\n      ('background-color', 'salmon')\n      ]\n    # Set CSS properties for td elements in dataframe\n    td_props = [\n      ('font-size', '10px'),\n      #('color', 'cornsilk'),\n        ('font-weight', 'normal')\n      ]\n\n    # Currently, could not make this work!\n    first_col_props = [\n        ('background-color', 'cornsilk'),\n        ('color', 'black'),\n        ('font-weight', '300'),\n    ]\n\n    # Set table styles\n    styles = [\n      dict(selector=\"th\", props=th_props),\n      dict(selector=\"td\", props=td_props),\n      dict(selection=\"tr td:first-child()\", props=first_col_props)\n      ]\n\n    return styles\n</code></pre> <pre><code>#\n# Rotate Pandas dataframe column headers.\n# Taken from:\n# https://stackoverflow.com/questions/46715736/rotating-the-column-name-for-a-panda-dataframe\n#\ndef format_vertical_headers(df):\n    \"\"\"Display a dataframe with vertical column headers\"\"\"\n    styles = [\n        dict(selector=\"th\", props=[('width', '40px')]),\n              dict(selector=\"th.col_heading\",\n                   props=[(\"writing-mode\", \"vertical-rl\"),\n                          ('transform', 'rotateZ(180deg)'), \n                          ('height', '160px'),\n                          ('vertical-align', 'top')])]\n    return (df.fillna('').style.set_table_styles(styles))\n</code></pre> <pre><code>def use_default_plot_style(enable_tex:str = True) -&amp;gt; None:\n    default_plot_style = {\n                    \"text.usetex\": enable_tex,\n                    \"legend.fontsize\": 10,\n                    \"axes.labelsize\": 16,\n                    \"figure.dpi\": 144,\n                    'font.family': 'DejaVu Sans', # 'Lora,serif'\n                    'font.weight':'bold',\n            }\n    plt.rcParams.update(default_plot_style)\n    plt.style.use('ggplot')\n\nuse_default_plot_style(False) # set this to True if you have latex installed\n</code></pre> <pre><code>message = \"\"\"\n&lt;div style=\"color:#aaa;font-size:8pt\"&gt;\n&lt;hr/&gt;\n\u00a9 SupportVectors. All rights reserved. &lt;blockquote&gt;This notebook is the intellectual property of SupportVectors, and part of its training material. \nOnly the participants in SupportVectors workshops are allowed to study the notebooks for educational purposes currently, but is prohibited from copying or using it for any other purposes without written permission.\n\n&lt;b&gt; These notebooks are chapters and sections from Asif Qamar's textbook that he is writing on Data Science. So we request you to not circulate the material to others.&lt;/b&gt;\n&lt;/blockquote&gt;\n&lt;hr/&gt;\n&lt;/div&gt;\n\n\"\"\"\nfrom IPython.display import Markdown\n\ndef copyrights() -&amp;gt; None:\n     display (Markdown(message))\n</code></pre> <pre><code>copyrights()\n</code></pre>    \u00a9 SupportVectors. All rights reserved. This notebook is the intellectual property of SupportVectors, and part of its training material.  Only the participants in SupportVectors workshops are allowed to study the notebooks for educational purposes currently, but is prohibited from copying or using it for any other purposes without written permission.  <p> These notebooks are chapters and sections from Asif Qamar's textbook that he is writing on Data Science. So we request you to not circulate the material to others.</p> <pre><code>import os\nimport sys\nfrom pathlib import Path\n\ndef add_svlib():\n\n    current_dir = str(Path.cwd().resolve())\n\n    # Add parent directory to the python path.\n    parent_dir = str(Path.cwd().parent.parent.resolve())+\"/src\"\n\n    if parent_dir not in sys.path:\n        sys.path.append(parent_dir)\n</code></pre> <pre><code>add_svlib()\n</code></pre> <pre><code>from dotenv import load_dotenv\nload_dotenv(override=True)\n</code></pre>"},{"location":"notebooks/supportvectors-common/#importing-the-necessary-libraries","title":"IMPORTING THE NECESSARY LIBRARIES\u00b6","text":""},{"location":"project-guide/slides/","title":"Slides","text":""},{"location":"project-guide/slides/#slides-metamorphosis-learning-ai-agents-by-building-one","title":"Slides: Metamorphosis \u2014 Learning AI Agents by Building One","text":"<p>Lightweight slide deck in Markdown (works well with Reveal.js or as handout)</p>"},{"location":"project-guide/slides/#why-agents","title":"Why agents?","text":"<ul> <li>Small, clear steps</li> <li>Compose simple capabilities</li> <li>Learn by building</li> </ul>"},{"location":"project-guide/slides/#the-three-moving-parts","title":"The three moving parts","text":"<ul> <li>Workflow (LangGraph)</li> <li>Tools (MCP)</li> <li>State (small dict)</li> </ul> flowchart LR     UX[WorkflowExecutor] --&gt; GB[GraphBuilder]     GB --&gt; N[WorkflowNodes]     N --&gt; S[(GraphState)]     N --&gt; MCP[MCP Tools]"},{"location":"project-guide/slides/#the-workflow-sketch","title":"The workflow sketch","text":"flowchart LR     A[START] --&gt; B[Copy Editor]     B --&gt; C[Summarizer]     B --&gt; D[Word Cloud]     B --&gt; E[Achievements Extractor]     E --&gt;|if tools| F[ToolNode]     F --&gt; G[After Achievements Parser]     G --&gt; H[Review Text Evaluator]     H --&gt;|if tools| I[ToolNode]     I --&gt; J[After Evaluation Parser]     C --&gt; K[END]     D --&gt; K     J --&gt; K"},{"location":"project-guide/slides/#demo-steps","title":"Demo steps","text":"<ol> <li>Initialize <code>WorkflowExecutor</code></li> <li>Run workflow on sample text</li> <li>Watch events: copy edit \u2192 summary \u2192 word cloud</li> <li>Show achievements and evaluation branches</li> </ol>"},{"location":"project-guide/slides/#lessons","title":"Lessons","text":"<ul> <li>Keep nodes tiny and readable</li> <li>Validate inputs/outputs</li> <li>Prefer simple branches over clever logic</li> </ul>"},{"location":"project-guide/speaker-notes/","title":"Speaker Notes","text":""},{"location":"project-guide/speaker-notes/#speaker-notes-metamorphosis-talk","title":"Speaker Notes (Metamorphosis Talk)","text":"<p>These notes are designed for a 20\u201325 minute session.</p>"},{"location":"project-guide/speaker-notes/#timing-flow","title":"Timing &amp; Flow","text":"<ul> <li>0:00\u20132:00 \u2014 Welcome and goal: keep things simple, learn by building</li> <li>2:00\u20135:00 \u2014 What are agents? (workflow + tools + state)</li> <li>5:00\u20139:00 \u2014 Show the workflow diagram and explain branches</li> <li>9:00\u201317:00 \u2014 Live demo: run WorkflowExecutor and narrate events</li> <li>17:00\u201320:00 \u2014 Lessons learned + Q&amp;A buffer</li> </ul>"},{"location":"project-guide/speaker-notes/#demo-checklist","title":"Demo Checklist","text":"<ul> <li>Terminal ready with <code>OPENAI_API_KEY</code> set</li> <li>From project root, open a Python REPL or a short script to:</li> <li>Create <code>WorkflowExecutor</code></li> <li><code>await initialize()</code></li> <li><code>await run_workflow(sample_text, thread_id=\"demo-1\")</code></li> <li>Print or view <code>word_cloud_path</code></li> </ul>"},{"location":"project-guide/speaker-notes/#narration-tips","title":"Narration Tips","text":"<ul> <li>Emphasize the tiny pieces: copy edit \u2192 summarize \u2192 visualize</li> <li>Explain that tools are normal functions behind a friendly interface</li> <li>Show that branching is readable and relies on small decisions</li> </ul>"},{"location":"project-guide/speaker-notes/#recovery-plan","title":"Recovery Plan","text":"<ul> <li>If the LLM call is slow: pre-load a saved result and explain it</li> <li>If network is flaky: walk through the diagrams and code structure</li> </ul>"},{"location":"project-guide/speaker-notes/#links-to-show","title":"Links to show","text":"<ul> <li>Agents overview: <code>metamorphosis/agents/index.md</code></li> <li>Self Reviewer package: <code>metamorphosis/agents/self_reviewer/index.md</code></li> <li>TextModifiers class: <code>metamorphosis/mcp/TextModifiers.md</code></li> </ul>"},{"location":"project-guide/talk-outline/","title":"Outline","text":""},{"location":"project-guide/talk-outline/#talk-outline-learning-ai-agents-by-building-one","title":"Talk Outline: Learning AI Agents by Building One","text":"<p>This talk uses the Metamorphosis project as a gentle case study to learn the basics of AI agents, LangGraph, and MCP tools.</p>"},{"location":"project-guide/talk-outline/#1-why-agents-23-min","title":"1) Why agents? (2\u20133 min)","text":"<ul> <li>Goal: Clear steps, small pieces, easy to reason about</li> <li>Idea: Compose tiny capabilities (copy edit, summarize, visualize) into a simple workflow</li> <li>Takeaway: Agents are just small programs that cooperate</li> </ul>"},{"location":"project-guide/talk-outline/#2-the-three-moving-parts-34-min","title":"2) The three moving parts (3\u20134 min)","text":"<ul> <li>Workflow: LangGraph nodes and edges</li> <li>Tools: MCP tools for copy edit, summarize, and word cloud</li> <li>State: A small dictionary passed between steps</li> </ul> %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#e0f2ff', 'clusterBkg':'#f7fbff'}}}%% flowchart LR     UX[WorkflowExecutor] --&gt; GB[GraphBuilder]     GB --&gt; N[WorkflowNodes]     N --&gt; S[(GraphState)]     N --&gt; MCP[MCP Tools]"},{"location":"project-guide/talk-outline/#3-the-workflow-in-one-picture-34-min","title":"3) The workflow in one picture (3\u20134 min)","text":"%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#e0f2ff'}}}%% flowchart LR     A[START] --&gt; B[Copy Editor]     B --&gt; C[Summarizer]     B --&gt; D[Word Cloud]     B --&gt; E[Achievements Extractor]     E --&gt;|if tools| F[ToolNode: extract_achievements]     F --&gt; G[After Achievements Parser]     G --&gt; H[Review Text Evaluator]     H --&gt;|if tools| I[ToolNode: evaluate_review_text]     I --&gt; J[After Evaluation Parser]     C --&gt; K[END]     D --&gt; K     J --&gt; K"},{"location":"project-guide/talk-outline/#4-live-demo-plan-68-min","title":"4) Live demo plan (6\u20138 min)","text":"<ul> <li>Run <code>WorkflowExecutor</code> on a sample review</li> <li>Show streaming updates (copy edit \u2192 summary \u2192 word cloud)</li> <li>Show achievements and evaluation branches</li> <li>Open the generated word cloud</li> </ul>"},{"location":"project-guide/talk-outline/#5-lessons-learned-23-min","title":"5) Lessons learned (2\u20133 min)","text":"<ul> <li>Keep nodes small and predictable</li> <li>Use state carefully; validate inputs and outputs</li> <li>Prefer simple branching over clever logic</li> </ul>"},{"location":"project-guide/talk-outline/#6-qa-23-min","title":"6) Q&amp;A (2\u20133 min)","text":"<ul> <li>Share code links and diagrams in the docs</li> <li>Invite ideas for next steps (extra nodes, different tools)</li> </ul>"},{"location":"project-guide/tips-and-tricks/cuda-hell/","title":"Some tips and tricks to deal with the CUDA hell","text":"<p>These are some tips and tricks that may be helpful while dealing with CUDA.</p>"},{"location":"project-guide/tips-and-tricks/cuda-hell/#install-the-cuda-toolkit","title":"Install the cuda-toolkit","text":"<p>It is important to remember that one must install the cuda-toolkit, and set the CUDA_HOME environment variable, in order for some critical tools, such as <code>vLLM</code> to work. To do this, follow the instructions at: CUDA installation guide on Linux</p>"},{"location":"project-guide/tips-and-tricks/cuda-hell/#flush-cuda-memory-from-python-code","title":"Flush cuda memory from Python code","text":"<p>It is helpful to start all Python code that uses CUDA with the following mantra invocation:</p> <pre><code>import torch, gc\ngc.collect()\ntorch.cuda.empty_cache()\n</code></pre> <p>Despite this invocation, quite often it will not completely flush the cuda memory. In that case, a better invocation at the level of the Linux shell is:</p> <p><pre><code>nvidia-smi | grep 'python' | awk '{ print $5 }' | xargs -n1 kill -9\n</code></pre> This pearl of wisdom is gleaned from: How to flush GPU memory using CUDA </p> <p>If you're still hitting unexpected memory errors or similar problems then try:</p> <pre><code>sudo fuser -v /dev/nvidia* | cut -d' ' -f2- | sudo xargs -n1 kill -9\n</code></pre>"},{"location":"project-guide/tips-and-tricks/cuda-hell/#nvtop-is-your-friend","title":"<code>nvtop</code> is your friend!","text":"<p>A very useful and visual tool to see what is happening in CUDA is to use the tool <code>nvtop</code>. Install it with the mantra:</p> <p><pre><code>sudo dnf install nvtop\n</code></pre> (or its equivalent if you foolishly use anything other than redhat/centos/rocky-linux).</p>"}]}